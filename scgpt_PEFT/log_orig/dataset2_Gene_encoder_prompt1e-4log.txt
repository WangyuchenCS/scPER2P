nohup: ignoring input
Global seed set to 0
wandb: Currently logged in as: ywang2542-c (wang_wandb). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/wandb/run-20240423_032107-4ix12v39
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gallant-morning-348
wandb: ‚≠êÔ∏è View project at https://wandb.ai/wang_wandb/scGPT
wandb: üöÄ View run at https://wandb.ai/wang_wandb/scGPT/runs/4ix12v39
scPEFT_scGPT
Namespace(dataset='dataset2', lr=0.0001, use_prompt=True, prompt_type='Gene_encoder_prompt', data_name='ms', data_path='/home/wsl20/Projects/RNA2prot/scGPT/data/', space_conf=[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], mlp_conf=[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], epoch=100)
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]}
save to save/dataset2/Gene_encoder_prompt/0.0001
adata_gene (4330, 21005)
adata_protein (4330, 42)
celltype num_types:1
scGPT - INFO - match 19491/21005 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human/best_model.pt, the model args will override the config /home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human/args.json.
scGPT - INFO - Filtering genes by counts ...
scGPT - INFO - Filtering cells by counts ...
scGPT - INFO - Normalizing total counts ...
scGPT - INFO - Binning data ...
scGPT - INFO - train set number of samples: 3897, 
	 feature length: 2001
scGPT - INFO - valid set number of samples: 433, 
	 feature length: 2001
except!!!! only load params that are in the model and match the size!!!!!
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params decoder.fc.0.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params decoder.fc.0.bias with shape torch.Size([512])
scGPT - INFO - Loading params decoder.fc.2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params decoder.fc.2.bias with shape torch.Size([512])
Total Pre freeze Params 28343851
All para.requires_grad:  False, Freeze!
Adapter in name: transformer_encoder.layers.0.Space_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.0.Space_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.0.Space_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.0.Space_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.0.Space_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.0.Space_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.0.Space_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.0.Space_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.0.MLP_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.0.MLP_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.0.MLP_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.0.MLP_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.0.MLP_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.0.MLP_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.0.MLP_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.0.MLP_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.1.Space_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.1.Space_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.1.Space_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.1.Space_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.1.Space_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.1.Space_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.1.Space_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.1.Space_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.1.MLP_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.1.MLP_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.1.MLP_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.1.MLP_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.1.MLP_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.1.MLP_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.1.MLP_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.1.MLP_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.2.Space_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.2.Space_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.2.Space_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.2.Space_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.2.Space_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.2.Space_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.2.Space_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.2.Space_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.2.MLP_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.2.MLP_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.2.MLP_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.2.MLP_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.2.MLP_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.2.MLP_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.2.MLP_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.2.MLP_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.3.Space_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.3.Space_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.3.Space_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.3.Space_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.3.Space_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.3.Space_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.3.Space_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.3.Space_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.3.MLP_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.3.MLP_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.3.MLP_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.3.MLP_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.3.MLP_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.3.MLP_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.3.MLP_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.3.MLP_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.4.Space_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.4.Space_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.4.Space_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.4.Space_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.4.Space_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.4.Space_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.4.Space_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.4.Space_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.4.MLP_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.4.MLP_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.4.MLP_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.4.MLP_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.4.MLP_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.4.MLP_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.4.MLP_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.4.MLP_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.5.Space_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.5.Space_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.5.Space_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.5.Space_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.5.Space_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.5.Space_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.5.Space_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.5.Space_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.5.MLP_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.5.MLP_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.5.MLP_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.5.MLP_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.5.MLP_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.5.MLP_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.5.MLP_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.5.MLP_Adapter.D_fc2.bias
cls in name: cls_decoder._decoder.0.weight
scGPT - INFO - cls in name: cls_decoder._decoder.0.weight
cls in name: cls_decoder._decoder.0.bias
scGPT - INFO - cls in name: cls_decoder._decoder.0.bias
cls in name: cls_decoder._decoder.2.weight
scGPT - INFO - cls in name: cls_decoder._decoder.2.weight
cls in name: cls_decoder._decoder.2.bias
scGPT - INFO - cls in name: cls_decoder._decoder.2.bias
cls in name: cls_decoder._decoder.3.weight
scGPT - INFO - cls in name: cls_decoder._decoder.3.weight
cls in name: cls_decoder._decoder.3.bias
scGPT - INFO - cls in name: cls_decoder._decoder.3.bias
cls in name: cls_decoder._decoder.5.weight
scGPT - INFO - cls in name: cls_decoder._decoder.5.weight
cls in name: cls_decoder._decoder.5.bias
scGPT - INFO - cls in name: cls_decoder._decoder.5.bias
cls in name: cls_decoder.out_layer.weight
scGPT - INFO - cls in name: cls_decoder.out_layer.weight
cls in name: cls_decoder.out_layer.bias
scGPT - INFO - cls in name: cls_decoder.out_layer.bias
Total Post0 freeze Params 2655275
total:28343851
trainable:2655275
Total Post freeze Params 2655275
scGPT - INFO - Total Pre freeze Params 28343851
scGPT - INFO - Total Post freeze Params 2655275
Epoch:  1
Training model
scGPT - INFO - | epoch   1 | 100/1949 batches | lr 0.0001 | ms/batch 96.04 | loss 127.42 | mse 127.42 | mre  0.00 |
scGPT - INFO - | epoch   1 | 200/1949 batches | lr 0.0001 | ms/batch 90.31 | loss 65.22 | mse 65.22 | mre  0.00 |
scGPT - INFO - | epoch   1 | 300/1949 batches | lr 0.0001 | ms/batch 90.63 | loss 67.58 | mse 67.58 | mre  0.00 |
scGPT - INFO - | epoch   1 | 400/1949 batches | lr 0.0001 | ms/batch 90.67 | loss 64.38 | mse 64.38 | mre  0.00 |
scGPT - INFO - | epoch   1 | 500/1949 batches | lr 0.0001 | ms/batch 90.87 | loss 64.92 | mse 64.92 | mre  0.00 |
scGPT - INFO - | epoch   1 | 600/1949 batches | lr 0.0001 | ms/batch 90.86 | loss 63.15 | mse 63.15 | mre  0.00 |
scGPT - INFO - | epoch   1 | 700/1949 batches | lr 0.0001 | ms/batch 90.87 | loss 64.00 | mse 64.00 | mre  0.00 |
scGPT - INFO - | epoch   1 | 800/1949 batches | lr 0.0001 | ms/batch 90.83 | loss 65.79 | mse 65.79 | mre  0.00 |
scGPT - INFO - | epoch   1 | 900/1949 batches | lr 0.0001 | ms/batch 90.79 | loss 62.20 | mse 62.20 | mre  0.00 |
scGPT - INFO - | epoch   1 | 1000/1949 batches | lr 0.0001 | ms/batch 91.52 | loss 67.59 | mse 67.59 | mre  0.00 |
scGPT - INFO - | epoch   1 | 1100/1949 batches | lr 0.0001 | ms/batch 90.53 | loss 68.20 | mse 68.20 | mre  0.00 |
scGPT - INFO - | epoch   1 | 1200/1949 batches | lr 0.0001 | ms/batch 90.53 | loss 63.98 | mse 63.98 | mre  0.00 |
scGPT - INFO - | epoch   1 | 1300/1949 batches | lr 0.0001 | ms/batch 90.50 | loss 64.10 | mse 64.10 | mre  0.00 |
scGPT - INFO - | epoch   1 | 1400/1949 batches | lr 0.0001 | ms/batch 90.58 | loss 65.15 | mse 65.15 | mre  0.00 |
scGPT - INFO - | epoch   1 | 1500/1949 batches | lr 0.0001 | ms/batch 90.53 | loss 63.15 | mse 63.15 | mre  0.00 |
scGPT - INFO - | epoch   1 | 1600/1949 batches | lr 0.0001 | ms/batch 90.55 | loss 64.52 | mse 64.52 | mre  0.00 |
scGPT - INFO - | epoch   1 | 1700/1949 batches | lr 0.0001 | ms/batch 90.46 | loss 71.94 | mse 71.94 | mre  0.00 |
scGPT - INFO - | epoch   1 | 1800/1949 batches | lr 0.0001 | ms/batch 90.55 | loss 65.28 | mse 65.28 | mre  0.00 |
scGPT - INFO - | epoch   1 | 1900/1949 batches | lr 0.0001 | ms/batch 90.50 | loss 62.83 | mse 62.83 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 183.86s | valid loss/mse 62.8253 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 62.8253
epoch:  1
[[0.        0.        0.        ... 4.634729  0.        0.       ]
 [0.        0.        0.        ... 1.0986123 0.6931472 0.       ]
 [0.        0.        0.6931472 ... 0.6931472 0.        0.       ]
 ...
 [0.        3.218876  0.6931472 ... 1.9459102 0.        0.       ]
 [0.        3.3322046 1.609438  ... 2.0794415 0.        0.       ]
 [0.        4.7706847 3.1354942 ... 2.3025851 0.        0.       ]]
[[-0.03717   2.068     1.117    ...  2.72      0.0797    0.02904 ]
 [-0.03845   2.047     1.103    ...  2.703     0.0803    0.02977 ]
 [-0.03076   2.107     1.138    ...  2.752     0.077     0.02733 ]
 ...
 [-0.04272   2.018     1.081    ...  2.684     0.08386   0.02452 ]
 [-0.0357    2.078     1.12     ...  2.732     0.0818    0.02782 ]
 [-0.014175  2.2       1.195    ...  2.799     0.0698    0.02475 ]]
(433, 42) (433, 42)
By feature:  MSE:  0.7489¬±0.0 MAE:  0.5569¬±0.0 R2:  0.2732¬±0.0 PCC:  0.6328¬±0.0 Cosine Similarity:  0.7373¬±0.0
By sample:  MSE:  0.7489¬±0.0 MAE:  0.5569¬±0.0 R2:  -0.0305¬±0.0 PCC:  0.0798¬±0.0 Cosine Similarity:  0.4938¬±0.0
scGPT - INFO - By feature: MSE: 0.7488999962806702¬±0.0 MAE: 0.5569000244140625¬±0.0 R2: 0.2732¬±0.0 PCC: 0.6328¬±0.0 Cosine Similarity: 0.7373¬±0.0
scGPT - INFO - By sample: MSE: 0.7488999962806702¬±0.0 MAE: 0.5569000244140625¬±0.0 R2: -0.0305¬±0.0 PCC: 0.0798¬±0.0 Cosine Similarity: 0.4938¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  2
Training model
scGPT - INFO - | epoch   2 | 100/1949 batches | lr 0.0001 | ms/batch 92.39 | loss 62.16 | mse 62.16 | mre  0.00 |
scGPT - INFO - | epoch   2 | 200/1949 batches | lr 0.0001 | ms/batch 90.65 | loss 63.83 | mse 63.83 | mre  0.00 |
scGPT - INFO - | epoch   2 | 300/1949 batches | lr 0.0001 | ms/batch 90.69 | loss 65.52 | mse 65.52 | mre  0.00 |
scGPT - INFO - | epoch   2 | 400/1949 batches | lr 0.0001 | ms/batch 90.64 | loss 63.16 | mse 63.16 | mre  0.00 |
scGPT - INFO - | epoch   2 | 500/1949 batches | lr 0.0001 | ms/batch 90.71 | loss 63.16 | mse 63.16 | mre  0.00 |
scGPT - INFO - | epoch   2 | 600/1949 batches | lr 0.0001 | ms/batch 90.68 | loss 60.88 | mse 60.88 | mre  0.00 |
scGPT - INFO - | epoch   2 | 700/1949 batches | lr 0.0001 | ms/batch 90.70 | loss 62.34 | mse 62.34 | mre  0.00 |
scGPT - INFO - | epoch   2 | 800/1949 batches | lr 0.0001 | ms/batch 90.64 | loss 62.39 | mse 62.39 | mre  0.00 |
scGPT - INFO - | epoch   2 | 900/1949 batches | lr 0.0001 | ms/batch 90.70 | loss 56.44 | mse 56.44 | mre  0.00 |
scGPT - INFO - | epoch   2 | 1000/1949 batches | lr 0.0001 | ms/batch 90.86 | loss 59.56 | mse 59.56 | mre  0.00 |
scGPT - INFO - | epoch   2 | 1100/1949 batches | lr 0.0001 | ms/batch 91.17 | loss 58.60 | mse 58.60 | mre  0.00 |
scGPT - INFO - | epoch   2 | 1200/1949 batches | lr 0.0001 | ms/batch 90.59 | loss 57.97 | mse 57.97 | mre  0.00 |
scGPT - INFO - | epoch   2 | 1300/1949 batches | lr 0.0001 | ms/batch 90.62 | loss 58.56 | mse 58.56 | mre  0.00 |
scGPT - INFO - | epoch   2 | 1400/1949 batches | lr 0.0001 | ms/batch 90.64 | loss 58.85 | mse 58.85 | mre  0.00 |
scGPT - INFO - | epoch   2 | 1500/1949 batches | lr 0.0001 | ms/batch 90.69 | loss 56.52 | mse 56.52 | mre  0.00 |
scGPT - INFO - | epoch   2 | 1600/1949 batches | lr 0.0001 | ms/batch 90.64 | loss 55.74 | mse 55.74 | mre  0.00 |
scGPT - INFO - | epoch   2 | 1700/1949 batches | lr 0.0001 | ms/batch 90.72 | loss 62.92 | mse 62.92 | mre  0.00 |
scGPT - INFO - | epoch   2 | 1800/1949 batches | lr 0.0001 | ms/batch 90.62 | loss 54.82 | mse 54.82 | mre  0.00 |
scGPT - INFO - | epoch   2 | 1900/1949 batches | lr 0.0001 | ms/batch 90.60 | loss 53.58 | mse 53.58 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 183.53s | valid loss/mse 55.3730 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 55.3730
best_loss: 62.82531565606732, min_delta 0.0001, val_loss 55.3730247797096
Loss error: 7.452290876357715
epoch:  2
[[0.        0.        0.        ... 4.634729  0.        0.       ]
 [0.        0.        0.        ... 1.0986123 0.6931472 0.       ]
 [0.        0.        0.6931472 ... 0.6931472 0.        0.       ]
 ...
 [0.        3.218876  0.6931472 ... 1.9459102 0.        0.       ]
 [0.        3.3322046 1.609438  ... 2.0794415 0.        0.       ]
 [0.        4.7706847 3.1354942 ... 2.3025851 0.        0.       ]]
[[ 5.0720e-02  1.4453e+00  7.8662e-01 ...  2.7773e+00  1.2213e-01
   3.6865e-02]
 [ 6.0791e-02  9.2627e-01  3.7842e-01 ...  2.6504e+00  1.3232e-01
  -1.4099e-02]
 [ 6.7871e-02  9.8975e-01  4.3115e-01 ...  2.7324e+00  1.2549e-01
  -2.4414e-03]
 ...
 [ 8.3084e-03  2.6094e+00  1.4990e+00 ...  2.5547e+00  6.2805e-02
   3.7292e-02]
 [-5.9998e-02  3.6484e+00  2.0703e+00 ...  2.4336e+00  1.1444e-01
   1.1346e-01]
 [-6.2042e-02  3.6250e+00  2.0645e+00 ...  2.4668e+00  1.1438e-01
   1.0706e-01]]
(433, 42) (433, 42)
By feature:  MSE:  0.6596¬±0.0 MAE:  0.5038¬±0.0 R2:  0.3802¬±0.0 PCC:  0.6885¬±0.0 Cosine Similarity:  0.7744¬±0.0
By sample:  MSE:  0.6596¬±0.0 MAE:  0.5038¬±0.0 R2:  0.0313¬±0.0 PCC:  0.1735¬±0.0 Cosine Similarity:  0.5134¬±0.0
scGPT - INFO - By feature: MSE: 0.659600019454956¬±0.0 MAE: 0.5037999749183655¬±0.0 R2: 0.3802¬±0.0 PCC: 0.6885¬±0.0 Cosine Similarity: 0.7744¬±0.0
scGPT - INFO - By sample: MSE: 0.659600019454956¬±0.0 MAE: 0.5037999749183655¬±0.0 R2: 0.0313¬±0.0 PCC: 0.1735¬±0.0 Cosine Similarity: 0.5134¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  3
Training model
scGPT - INFO - | epoch   3 | 100/1949 batches | lr 0.0001 | ms/batch 91.85 | loss 53.17 | mse 53.17 | mre  0.00 |
scGPT - INFO - | epoch   3 | 200/1949 batches | lr 0.0001 | ms/batch 91.28 | loss 55.12 | mse 55.12 | mre  0.00 |
scGPT - INFO - | epoch   3 | 300/1949 batches | lr 0.0001 | ms/batch 90.61 | loss 57.29 | mse 57.29 | mre  0.00 |
scGPT - INFO - | epoch   3 | 400/1949 batches | lr 0.0001 | ms/batch 90.62 | loss 54.84 | mse 54.84 | mre  0.00 |
scGPT - INFO - | epoch   3 | 500/1949 batches | lr 0.0001 | ms/batch 90.65 | loss 53.85 | mse 53.85 | mre  0.00 |
scGPT - INFO - | epoch   3 | 600/1949 batches | lr 0.0001 | ms/batch 90.59 | loss 52.77 | mse 52.77 | mre  0.00 |
scGPT - INFO - | epoch   3 | 700/1949 batches | lr 0.0001 | ms/batch 90.65 | loss 53.34 | mse 53.34 | mre  0.00 |
scGPT - INFO - | epoch   3 | 800/1949 batches | lr 0.0001 | ms/batch 90.61 | loss 55.18 | mse 55.18 | mre  0.00 |
scGPT - INFO - | epoch   3 | 900/1949 batches | lr 0.0001 | ms/batch 90.63 | loss 51.25 | mse 51.25 | mre  0.00 |
scGPT - INFO - | epoch   3 | 1000/1949 batches | lr 0.0001 | ms/batch 90.62 | loss 54.97 | mse 54.97 | mre  0.00 |
scGPT - INFO - | epoch   3 | 1100/1949 batches | lr 0.0001 | ms/batch 90.77 | loss 55.48 | mse 55.48 | mre  0.00 |
scGPT - INFO - | epoch   3 | 1200/1949 batches | lr 0.0001 | ms/batch 91.20 | loss 53.94 | mse 53.94 | mre  0.00 |
scGPT - INFO - | epoch   3 | 1300/1949 batches | lr 0.0001 | ms/batch 90.60 | loss 54.72 | mse 54.72 | mre  0.00 |
scGPT - INFO - | epoch   3 | 1400/1949 batches | lr 0.0001 | ms/batch 90.60 | loss 53.68 | mse 53.68 | mre  0.00 |
scGPT - INFO - | epoch   3 | 1500/1949 batches | lr 0.0001 | ms/batch 90.61 | loss 54.19 | mse 54.19 | mre  0.00 |
scGPT - INFO - | epoch   3 | 1600/1949 batches | lr 0.0001 | ms/batch 90.61 | loss 51.62 | mse 51.62 | mre  0.00 |
scGPT - INFO - | epoch   3 | 1700/1949 batches | lr 0.0001 | ms/batch 90.75 | loss 57.84 | mse 57.84 | mre  0.00 |
scGPT - INFO - | epoch   3 | 1800/1949 batches | lr 0.0001 | ms/batch 90.63 | loss 50.80 | mse 50.80 | mre  0.00 |
scGPT - INFO - | epoch   3 | 1900/1949 batches | lr 0.0001 | ms/batch 90.72 | loss 49.64 | mse 49.64 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 183.51s | valid loss/mse 52.4000 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 52.4000
best_loss: 55.3730247797096, min_delta 0.0001, val_loss 52.3999617887149
Loss error: 2.9730629909947055
epoch:  3
[[0.        0.        0.        ... 4.634729  0.        0.       ]
 [0.        0.        0.        ... 1.0986123 0.6931472 0.       ]
 [0.        0.        0.6931472 ... 0.6931472 0.        0.       ]
 ...
 [0.        3.218876  0.6931472 ... 1.9459102 0.        0.       ]
 [0.        3.3322046 1.609438  ... 2.0794415 0.        0.       ]
 [0.        4.7706847 3.1354942 ... 2.3025851 0.        0.       ]]
[[ 0.07043  0.9185   0.4194  ...  2.836    0.12024 -0.00408]
 [ 0.0621   0.894    0.3923  ...  2.84     0.1309  -0.03424]
 [ 0.0675   0.9126   0.406   ...  2.887    0.1268  -0.0246 ]
 ...
 [-0.02509  3.346    1.882   ...  2.477    0.10144  0.076  ]
 [-0.02744  3.398    1.8955  ...  2.512    0.11816  0.08026]
 [-0.04825  3.602    2.008   ...  2.504    0.1193   0.0628 ]]
(433, 42) (433, 42)
By feature:  MSE:  0.6242¬±0.0 MAE:  0.4911¬±0.0 R2:  0.3984¬±0.0 PCC:  0.7077¬±0.0 Cosine Similarity:  0.787¬±0.0
By sample:  MSE:  0.6242¬±0.0 MAE:  0.4911¬±0.0 R2:  0.053¬±0.0 PCC:  0.1974¬±0.0 Cosine Similarity:  0.5183¬±0.0
scGPT - INFO - By feature: MSE: 0.6241999864578247¬±0.0 MAE: 0.491100013256073¬±0.0 R2: 0.3984¬±0.0 PCC: 0.7077¬±0.0 Cosine Similarity: 0.787¬±0.0
scGPT - INFO - By sample: MSE: 0.6241999864578247¬±0.0 MAE: 0.491100013256073¬±0.0 R2: 0.053¬±0.0 PCC: 0.1974¬±0.0 Cosine Similarity: 0.5183¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  4
Training model
scGPT - INFO - | epoch   4 | 100/1949 batches | lr 0.0001 | ms/batch 91.91 | loss 50.89 | mse 50.89 | mre  0.00 |
scGPT - INFO - | epoch   4 | 200/1949 batches | lr 0.0001 | ms/batch 91.20 | loss 50.94 | mse 50.94 | mre  0.00 |
scGPT - INFO - | epoch   4 | 300/1949 batches | lr 0.0001 | ms/batch 90.67 | loss 53.50 | mse 53.50 | mre  0.00 |
scGPT - INFO - | epoch   4 | 400/1949 batches | lr 0.0001 | ms/batch 90.70 | loss 51.31 | mse 51.31 | mre  0.00 |
scGPT - INFO - | epoch   4 | 500/1949 batches | lr 0.0001 | ms/batch 90.67 | loss 51.56 | mse 51.56 | mre  0.00 |
scGPT - INFO - | epoch   4 | 600/1949 batches | lr 0.0001 | ms/batch 90.66 | loss 50.99 | mse 50.99 | mre  0.00 |
scGPT - INFO - | epoch   4 | 700/1949 batches | lr 0.0001 | ms/batch 90.66 | loss 49.17 | mse 49.17 | mre  0.00 |
scGPT - INFO - | epoch   4 | 800/1949 batches | lr 0.0001 | ms/batch 90.90 | loss 50.94 | mse 50.94 | mre  0.00 |
scGPT - INFO - | epoch   4 | 900/1949 batches | lr 0.0001 | ms/batch 90.68 | loss 48.83 | mse 48.83 | mre  0.00 |
scGPT - INFO - | epoch   4 | 1000/1949 batches | lr 0.0001 | ms/batch 90.69 | loss 52.71 | mse 52.71 | mre  0.00 |
scGPT - INFO - | epoch   4 | 1100/1949 batches | lr 0.0001 | ms/batch 90.68 | loss 52.52 | mse 52.52 | mre  0.00 |
scGPT - INFO - | epoch   4 | 1200/1949 batches | lr 0.0001 | ms/batch 91.22 | loss 48.24 | mse 48.24 | mre  0.00 |
scGPT - INFO - | epoch   4 | 1300/1949 batches | lr 0.0001 | ms/batch 90.72 | loss 50.24 | mse 50.24 | mre  0.00 |
scGPT - INFO - | epoch   4 | 1400/1949 batches | lr 0.0001 | ms/batch 90.71 | loss 50.60 | mse 50.60 | mre  0.00 |
scGPT - INFO - | epoch   4 | 1500/1949 batches | lr 0.0001 | ms/batch 90.66 | loss 51.53 | mse 51.53 | mre  0.00 |
scGPT - INFO - | epoch   4 | 1600/1949 batches | lr 0.0001 | ms/batch 90.64 | loss 47.97 | mse 47.97 | mre  0.00 |
scGPT - INFO - | epoch   4 | 1700/1949 batches | lr 0.0001 | ms/batch 90.62 | loss 54.32 | mse 54.32 | mre  0.00 |
scGPT - INFO - | epoch   4 | 1800/1949 batches | lr 0.0001 | ms/batch 90.65 | loss 48.74 | mse 48.74 | mre  0.00 |
scGPT - INFO - | epoch   4 | 1900/1949 batches | lr 0.0001 | ms/batch 90.65 | loss 48.38 | mse 48.38 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 183.57s | valid loss/mse 50.0907 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 50.0907
best_loss: 52.3999617887149, min_delta 0.0001, val_loss 50.0907335986166
Loss error: 2.3092281900982954
epoch:  4
[[0.        0.        0.        ... 4.634729  0.        0.       ]
 [0.        0.        0.        ... 1.0986123 0.6931472 0.       ]
 [0.        0.        0.6931472 ... 0.6931472 0.        0.       ]
 ...
 [0.        3.218876  0.6931472 ... 1.9459102 0.        0.       ]
 [0.        3.3322046 1.609438  ... 2.0794415 0.        0.       ]
 [0.        4.7706847 3.1354942 ... 2.3025851 0.        0.       ]]
[[ 0.07135   0.9536    0.4436   ...  2.66      0.0701    0.04672 ]
 [ 0.06946   0.881     0.3884   ...  2.617     0.08844   0.0392  ]
 [ 0.0781    0.9224    0.4182   ...  2.686     0.0832    0.0497  ]
 ...
 [ 0.008255  3.342     1.846    ...  2.373     0.0759    0.0719  ]
 [-0.0374    3.357     1.825    ...  2.312     0.1034    0.0925  ]
 [-0.03      3.809     2.082    ...  2.432     0.0904    0.09827 ]]
(433, 42) (433, 42)
By feature:  MSE:  0.5967¬±0.0 MAE:  0.4756¬±0.0 R2:  0.4303¬±0.0 PCC:  0.7213¬±0.0 Cosine Similarity:  0.7964¬±0.0
By sample:  MSE:  0.5967¬±0.0 MAE:  0.4756¬±0.0 R2:  0.0766¬±0.0 PCC:  0.2403¬±0.0 Cosine Similarity:  0.5338¬±0.0
scGPT - INFO - By feature: MSE: 0.5967000126838684¬±0.0 MAE: 0.475600004196167¬±0.0 R2: 0.4303¬±0.0 PCC: 0.7213¬±0.0 Cosine Similarity: 0.7964¬±0.0
scGPT - INFO - By sample: MSE: 0.5967000126838684¬±0.0 MAE: 0.475600004196167¬±0.0 R2: 0.0766¬±0.0 PCC: 0.2403¬±0.0 Cosine Similarity: 0.5338¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  5
Training model
scGPT - INFO - | epoch   5 | 100/1949 batches | lr 0.0001 | ms/batch 91.85 | loss 48.27 | mse 48.27 | mre  0.00 |
scGPT - INFO - | epoch   5 | 200/1949 batches | lr 0.0001 | ms/batch 90.61 | loss 49.70 | mse 49.70 | mre  0.00 |
scGPT - INFO - | epoch   5 | 300/1949 batches | lr 0.0001 | ms/batch 91.21 | loss 51.75 | mse 51.75 | mre  0.00 |
scGPT - INFO - | epoch   5 | 400/1949 batches | lr 0.0001 | ms/batch 90.65 | loss 49.13 | mse 49.13 | mre  0.00 |
scGPT - INFO - | epoch   5 | 500/1949 batches | lr 0.0001 | ms/batch 90.66 | loss 50.63 | mse 50.63 | mre  0.00 |
scGPT - INFO - | epoch   5 | 600/1949 batches | lr 0.0001 | ms/batch 90.67 | loss 49.46 | mse 49.46 | mre  0.00 |
scGPT - INFO - | epoch   5 | 700/1949 batches | lr 0.0001 | ms/batch 90.68 | loss 47.76 | mse 47.76 | mre  0.00 |
scGPT - INFO - | epoch   5 | 800/1949 batches | lr 0.0001 | ms/batch 90.69 | loss 49.64 | mse 49.64 | mre  0.00 |
scGPT - INFO - | epoch   5 | 900/1949 batches | lr 0.0001 | ms/batch 90.81 | loss 47.77 | mse 47.77 | mre  0.00 |
scGPT - INFO - | epoch   5 | 1000/1949 batches | lr 0.0001 | ms/batch 90.73 | loss 51.14 | mse 51.14 | mre  0.00 |
scGPT - INFO - | epoch   5 | 1100/1949 batches | lr 0.0001 | ms/batch 90.74 | loss 49.10 | mse 49.10 | mre  0.00 |
scGPT - INFO - | epoch   5 | 1200/1949 batches | lr 0.0001 | ms/batch 91.03 | loss 46.49 | mse 46.49 | mre  0.00 |
scGPT - INFO - | epoch   5 | 1300/1949 batches | lr 0.0001 | ms/batch 91.31 | loss 47.80 | mse 47.80 | mre  0.00 |
scGPT - INFO - | epoch   5 | 1400/1949 batches | lr 0.0001 | ms/batch 90.70 | loss 48.79 | mse 48.79 | mre  0.00 |
scGPT - INFO - | epoch   5 | 1500/1949 batches | lr 0.0001 | ms/batch 90.66 | loss 49.89 | mse 49.89 | mre  0.00 |
scGPT - INFO - | epoch   5 | 1600/1949 batches | lr 0.0001 | ms/batch 90.66 | loss 47.18 | mse 47.18 | mre  0.00 |
scGPT - INFO - | epoch   5 | 1700/1949 batches | lr 0.0001 | ms/batch 90.80 | loss 52.93 | mse 52.93 | mre  0.00 |
scGPT - INFO - | epoch   5 | 1800/1949 batches | lr 0.0001 | ms/batch 90.76 | loss 46.87 | mse 46.87 | mre  0.00 |
scGPT - INFO - | epoch   5 | 1900/1949 batches | lr 0.0001 | ms/batch 90.72 | loss 45.88 | mse 45.88 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 183.63s | valid loss/mse 49.0280 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 49.0280
best_loss: 50.0907335986166, min_delta 0.0001, val_loss 49.028024303445086
Loss error: 1.0627092951715156
epoch:  5
[[0.        0.        0.        ... 4.634729  0.        0.       ]
 [0.        0.        0.        ... 1.0986123 0.6931472 0.       ]
 [0.        0.        0.6931472 ... 0.6931472 0.        0.       ]
 ...
 [0.        3.218876  0.6931472 ... 1.9459102 0.        0.       ]
 [0.        3.3322046 1.609438  ... 2.0794415 0.        0.       ]
 [0.        4.7706847 3.1354942 ... 2.3025851 0.        0.       ]]
[[0.07306 0.946   0.4385  ... 2.613   0.0924  0.0629 ]
 [0.06415 0.875   0.3735  ... 2.482   0.1041  0.03998]
 [0.06366 0.8823  0.378   ... 2.549   0.1011  0.05194]
 ...
 [0.03065 3.05    1.663   ... 2.465   0.08765 0.0632 ]
 [0.01106 3.209   1.707   ... 2.404   0.12256 0.0627 ]
 [0.01296 3.84    2.09    ... 2.502   0.1063  0.069  ]]
(433, 42) (433, 42)
By feature:  MSE:  0.584¬±0.0 MAE:  0.4678¬±0.0 R2:  0.4393¬±0.0 PCC:  0.7293¬±0.0 Cosine Similarity:  0.8017¬±0.0
By sample:  MSE:  0.584¬±0.0 MAE:  0.4678¬±0.0 R2:  0.093¬±0.0 PCC:  0.2527¬±0.0 Cosine Similarity:  0.5403¬±0.0
scGPT - INFO - By feature: MSE: 0.5839999914169312¬±0.0 MAE: 0.46779999136924744¬±0.0 R2: 0.4393¬±0.0 PCC: 0.7293¬±0.0 Cosine Similarity: 0.8017¬±0.0
scGPT - INFO - By sample: MSE: 0.5839999914169312¬±0.0 MAE: 0.46779999136924744¬±0.0 R2: 0.093¬±0.0 PCC: 0.2527¬±0.0 Cosine Similarity: 0.5403¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  6
Training model
scGPT - INFO - | epoch   6 | 100/1949 batches | lr 0.0001 | ms/batch 91.84 | loss 46.87 | mse 46.87 | mre  0.00 |
scGPT - INFO - | epoch   6 | 200/1949 batches | lr 0.0001 | ms/batch 90.74 | loss 48.33 | mse 48.33 | mre  0.00 |
scGPT - INFO - | epoch   6 | 300/1949 batches | lr 0.0001 | ms/batch 91.23 | loss 49.62 | mse 49.62 | mre  0.00 |
scGPT - INFO - | epoch   6 | 400/1949 batches | lr 0.0001 | ms/batch 90.69 | loss 47.61 | mse 47.61 | mre  0.00 |
scGPT - INFO - | epoch   6 | 500/1949 batches | lr 0.0001 | ms/batch 90.70 | loss 49.20 | mse 49.20 | mre  0.00 |
scGPT - INFO - | epoch   6 | 600/1949 batches | lr 0.0001 | ms/batch 90.71 | loss 48.67 | mse 48.67 | mre  0.00 |
scGPT - INFO - | epoch   6 | 700/1949 batches | lr 0.0001 | ms/batch 90.72 | loss 48.05 | mse 48.05 | mre  0.00 |
scGPT - INFO - | epoch   6 | 800/1949 batches | lr 0.0001 | ms/batch 90.71 | loss 47.67 | mse 47.67 | mre  0.00 |
scGPT - INFO - | epoch   6 | 900/1949 batches | lr 0.0001 | ms/batch 90.67 | loss 46.78 | mse 46.78 | mre  0.00 |
scGPT - INFO - | epoch   6 | 1000/1949 batches | lr 0.0001 | ms/batch 90.74 | loss 49.53 | mse 49.53 | mre  0.00 |
scGPT - INFO - | epoch   6 | 1100/1949 batches | lr 0.0001 | ms/batch 90.69 | loss 47.55 | mse 47.55 | mre  0.00 |
scGPT - INFO - | epoch   6 | 1200/1949 batches | lr 0.0001 | ms/batch 90.72 | loss 45.84 | mse 45.84 | mre  0.00 |
scGPT - INFO - | epoch   6 | 1300/1949 batches | lr 0.0001 | ms/batch 91.25 | loss 46.96 | mse 46.96 | mre  0.00 |
scGPT - INFO - | epoch   6 | 1400/1949 batches | lr 0.0001 | ms/batch 90.74 | loss 47.61 | mse 47.61 | mre  0.00 |
scGPT - INFO - | epoch   6 | 1500/1949 batches | lr 0.0001 | ms/batch 90.65 | loss 49.99 | mse 49.99 | mre  0.00 |
scGPT - INFO - | epoch   6 | 1600/1949 batches | lr 0.0001 | ms/batch 90.71 | loss 45.31 | mse 45.31 | mre  0.00 |
scGPT - INFO - | epoch   6 | 1700/1949 batches | lr 0.0001 | ms/batch 90.72 | loss 51.54 | mse 51.54 | mre  0.00 |
scGPT - INFO - | epoch   6 | 1800/1949 batches | lr 0.0001 | ms/batch 90.64 | loss 45.67 | mse 45.67 | mre  0.00 |
scGPT - INFO - | epoch   6 | 1900/1949 batches | lr 0.0001 | ms/batch 90.64 | loss 45.51 | mse 45.51 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 183.61s | valid loss/mse 47.9017 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 47.9017
best_loss: 49.028024303445086, min_delta 0.0001, val_loss 47.90170079290729
Loss error: 1.1263235105377944
epoch:  6
[[0.        0.        0.        ... 4.634729  0.        0.       ]
 [0.        0.        0.        ... 1.0986123 0.6931472 0.       ]
 [0.        0.        0.6931472 ... 0.6931472 0.        0.       ]
 ...
 [0.        3.218876  0.6931472 ... 1.9459102 0.        0.       ]
 [0.        3.3322046 1.609438  ... 2.0794415 0.        0.       ]
 [0.        4.7706847 3.1354942 ... 2.3025851 0.        0.       ]]
[[0.0627  0.8193  0.357   ... 2.6     0.0932  0.04727]
 [0.06366 0.797   0.3516  ... 2.518   0.093   0.03946]
 [0.06464 0.821   0.3613  ... 2.615   0.0931  0.04605]
 ...
 [0.02357 3.1     1.638   ... 2.396   0.0904  0.07666]
 [0.01485 3.037   1.542   ... 2.38    0.1343  0.05997]
 [0.01306 3.81    2.053   ... 2.482   0.10406 0.0752 ]]
(433, 42) (433, 42)
By feature:  MSE:  0.5706¬±0.0 MAE:  0.4624¬±0.0 R2:  0.4524¬±0.0 PCC:  0.7352¬±0.0 Cosine Similarity:  0.8057¬±0.0
By sample:  MSE:  0.5706¬±0.0 MAE:  0.4624¬±0.0 R2:  0.106¬±0.0 PCC:  0.2669¬±0.0 Cosine Similarity:  0.5463¬±0.0
scGPT - INFO - By feature: MSE: 0.5705999732017517¬±0.0 MAE: 0.46239998936653137¬±0.0 R2: 0.4524¬±0.0 PCC: 0.7352¬±0.0 Cosine Similarity: 0.8057¬±0.0
scGPT - INFO - By sample: MSE: 0.5705999732017517¬±0.0 MAE: 0.46239998936653137¬±0.0 R2: 0.106¬±0.0 PCC: 0.2669¬±0.0 Cosine Similarity: 0.5463¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  7
Training model
scGPT - INFO - | epoch   7 | 100/1949 batches | lr 0.0001 | ms/batch 91.92 | loss 46.16 | mse 46.16 | mre  0.00 |
scGPT - INFO - | epoch   7 | 200/1949 batches | lr 0.0001 | ms/batch 90.75 | loss 46.81 | mse 46.81 | mre  0.00 |
scGPT - INFO - | epoch   7 | 300/1949 batches | lr 0.0001 | ms/batch 90.76 | loss 49.07 | mse 49.07 | mre  0.00 |
scGPT - INFO - | epoch   7 | 400/1949 batches | lr 0.0001 | ms/batch 91.28 | loss 46.44 | mse 46.44 | mre  0.00 |
scGPT - INFO - | epoch   7 | 500/1949 batches | lr 0.0001 | ms/batch 90.76 | loss 48.31 | mse 48.31 | mre  0.00 |
scGPT - INFO - | epoch   7 | 600/1949 batches | lr 0.0001 | ms/batch 90.73 | loss 48.38 | mse 48.38 | mre  0.00 |
scGPT - INFO - | epoch   7 | 700/1949 batches | lr 0.0001 | ms/batch 90.75 | loss 47.40 | mse 47.40 | mre  0.00 |
scGPT - INFO - | epoch   7 | 800/1949 batches | lr 0.0001 | ms/batch 90.70 | loss 46.68 | mse 46.68 | mre  0.00 |
scGPT - INFO - | epoch   7 | 900/1949 batches | lr 0.0001 | ms/batch 90.74 | loss 46.27 | mse 46.27 | mre  0.00 |
scGPT - INFO - | epoch   7 | 1000/1949 batches | lr 0.0001 | ms/batch 90.73 | loss 49.00 | mse 49.00 | mre  0.00 |
scGPT - INFO - | epoch   7 | 1100/1949 batches | lr 0.0001 | ms/batch 90.70 | loss 46.77 | mse 46.77 | mre  0.00 |
scGPT - INFO - | epoch   7 | 1200/1949 batches | lr 0.0001 | ms/batch 90.80 | loss 45.10 | mse 45.10 | mre  0.00 |
scGPT - INFO - | epoch   7 | 1300/1949 batches | lr 0.0001 | ms/batch 90.72 | loss 45.92 | mse 45.92 | mre  0.00 |
scGPT - INFO - | epoch   7 | 1400/1949 batches | lr 0.0001 | ms/batch 91.25 | loss 47.57 | mse 47.57 | mre  0.00 |
scGPT - INFO - | epoch   7 | 1500/1949 batches | lr 0.0001 | ms/batch 90.78 | loss 47.86 | mse 47.86 | mre  0.00 |
scGPT - INFO - | epoch   7 | 1600/1949 batches | lr 0.0001 | ms/batch 90.71 | loss 45.46 | mse 45.46 | mre  0.00 |
scGPT - INFO - | epoch   7 | 1700/1949 batches | lr 0.0001 | ms/batch 90.75 | loss 50.71 | mse 50.71 | mre  0.00 |
scGPT - INFO - | epoch   7 | 1800/1949 batches | lr 0.0001 | ms/batch 90.96 | loss 45.37 | mse 45.37 | mre  0.00 |
scGPT - INFO - | epoch   7 | 1900/1949 batches | lr 0.0001 | ms/batch 90.73 | loss 44.84 | mse 44.84 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 183.72s | valid loss/mse 47.9302 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 47.90170079290729, min_delta 0.0001, val_loss 47.930178816268956
Loss error: -0.028478023361664384
scGPT - INFO - INFO: Early stopping counter 1 of 10
epoch:  7
[[0.        0.        0.        ... 4.634729  0.        0.       ]
 [0.        0.        0.        ... 1.0986123 0.6931472 0.       ]
 [0.        0.        0.6931472 ... 0.6931472 0.        0.       ]
 ...
 [0.        3.218876  0.6931472 ... 1.9459102 0.        0.       ]
 [0.        3.3322046 1.609438  ... 2.0794415 0.        0.       ]
 [0.        4.7706847 3.1354942 ... 2.3025851 0.        0.       ]]
[[0.07446  0.888    0.3953   ... 2.719    0.0861   0.0496  ]
 [0.06604  0.8223   0.3477   ... 2.604    0.08484  0.02734 ]
 [0.06537  0.855    0.361    ... 2.719    0.08655  0.03848 ]
 ...
 [0.02908  3.258    1.683    ... 2.441    0.08093  0.074   ]
 [0.009865 3.484    1.73     ... 2.45     0.106    0.0619  ]
 [0.0224   3.975    2.127    ... 2.533    0.0979   0.0732  ]]
(433, 42) (433, 42)
By feature:  MSE:  0.5709¬±0.0 MAE:  0.4645¬±0.0 R2:  0.4391¬±0.0 PCC:  0.736¬±0.0 Cosine Similarity:  0.8062¬±0.0
By sample:  MSE:  0.5709¬±0.0 MAE:  0.4645¬±0.0 R2:  0.1055¬±0.0 PCC:  0.2685¬±0.0 Cosine Similarity:  0.5465¬±0.0
scGPT - INFO - By feature: MSE: 0.570900022983551¬±0.0 MAE: 0.4645000100135803¬±0.0 R2: 0.4391¬±0.0 PCC: 0.736¬±0.0 Cosine Similarity: 0.8062¬±0.0
scGPT - INFO - By sample: MSE: 0.570900022983551¬±0.0 MAE: 0.4645000100135803¬±0.0 R2: 0.1055¬±0.0 PCC: 0.2685¬±0.0 Cosine Similarity: 0.5465¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  8
Training model
scGPT - INFO - | epoch   8 | 100/1949 batches | lr 0.0000 | ms/batch 91.82 | loss 45.84 | mse 45.84 | mre  0.00 |
scGPT - INFO - | epoch   8 | 200/1949 batches | lr 0.0000 | ms/batch 90.71 | loss 46.44 | mse 46.44 | mre  0.00 |
scGPT - INFO - | epoch   8 | 300/1949 batches | lr 0.0000 | ms/batch 90.72 | loss 47.89 | mse 47.89 | mre  0.00 |
scGPT - INFO - | epoch   8 | 400/1949 batches | lr 0.0000 | ms/batch 91.22 | loss 46.06 | mse 46.06 | mre  0.00 |
scGPT - INFO - | epoch   8 | 500/1949 batches | lr 0.0000 | ms/batch 90.65 | loss 47.26 | mse 47.26 | mre  0.00 |
scGPT - INFO - | epoch   8 | 600/1949 batches | lr 0.0000 | ms/batch 90.58 | loss 47.73 | mse 47.73 | mre  0.00 |
scGPT - INFO - | epoch   8 | 700/1949 batches | lr 0.0000 | ms/batch 90.60 | loss 46.76 | mse 46.76 | mre  0.00 |
scGPT - INFO - | epoch   8 | 800/1949 batches | lr 0.0000 | ms/batch 90.60 | loss 46.92 | mse 46.92 | mre  0.00 |
scGPT - INFO - | epoch   8 | 900/1949 batches | lr 0.0000 | ms/batch 90.63 | loss 45.40 | mse 45.40 | mre  0.00 |
scGPT - INFO - | epoch   8 | 1000/1949 batches | lr 0.0000 | ms/batch 90.59 | loss 48.74 | mse 48.74 | mre  0.00 |
scGPT - INFO - | epoch   8 | 1100/1949 batches | lr 0.0000 | ms/batch 90.63 | loss 47.95 | mse 47.95 | mre  0.00 |
scGPT - INFO - | epoch   8 | 1200/1949 batches | lr 0.0000 | ms/batch 90.79 | loss 44.76 | mse 44.76 | mre  0.00 |
scGPT - INFO - | epoch   8 | 1300/1949 batches | lr 0.0000 | ms/batch 90.73 | loss 45.38 | mse 45.38 | mre  0.00 |
scGPT - INFO - | epoch   8 | 1400/1949 batches | lr 0.0000 | ms/batch 91.32 | loss 46.10 | mse 46.10 | mre  0.00 |
scGPT - INFO - | epoch   8 | 1500/1949 batches | lr 0.0000 | ms/batch 90.81 | loss 47.43 | mse 47.43 | mre  0.00 |
scGPT - INFO - | epoch   8 | 1600/1949 batches | lr 0.0000 | ms/batch 90.73 | loss 44.11 | mse 44.11 | mre  0.00 |
scGPT - INFO - | epoch   8 | 1700/1949 batches | lr 0.0000 | ms/batch 90.72 | loss 49.53 | mse 49.53 | mre  0.00 |
scGPT - INFO - | epoch   8 | 1800/1949 batches | lr 0.0000 | ms/batch 90.72 | loss 44.48 | mse 44.48 | mre  0.00 |
scGPT - INFO - | epoch   8 | 1900/1949 batches | lr 0.0000 | ms/batch 90.73 | loss 43.99 | mse 43.99 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   8 | time: 183.59s | valid loss/mse 47.6215 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 47.6215
best_loss: 47.90170079290729, min_delta 0.0001, val_loss 47.621519044695496
Loss error: 0.2801817482117954
epoch:  8
[[0.        0.        0.        ... 4.634729  0.        0.       ]
 [0.        0.        0.        ... 1.0986123 0.6931472 0.       ]
 [0.        0.        0.6931472 ... 0.6931472 0.        0.       ]
 ...
 [0.        3.218876  0.6931472 ... 1.9459102 0.        0.       ]
 [0.        3.3322046 1.609438  ... 2.0794415 0.        0.       ]
 [0.        4.7706847 3.1354942 ... 2.3025851 0.        0.       ]]
[[0.0687  0.889   0.4004  ... 2.83    0.10077 0.0656 ]
 [0.04306 0.7383  0.2627  ... 2.572   0.0974  0.02972]
 [0.04572 0.7627  0.2708  ... 2.717   0.1071  0.0449 ]
 ...
 [0.04248 3.496   1.827   ... 2.49    0.0853  0.07776]
 [0.02087 3.29    1.559   ... 2.496   0.1128  0.0691 ]
 [0.02812 3.814   2.021   ... 2.607   0.082   0.0661 ]]
(433, 42) (433, 42)
By feature:  MSE:  0.5673¬±0.0 MAE:  0.4642¬±0.0 R2:  0.4444¬±0.0 PCC:  0.7381¬±0.0 Cosine Similarity:  0.8076¬±0.0
By sample:  MSE:  0.5673¬±0.0 MAE:  0.4642¬±0.0 R2:  0.1083¬±0.0 PCC:  0.2715¬±0.0 Cosine Similarity:  0.548¬±0.0
scGPT - INFO - By feature: MSE: 0.567300021648407¬±0.0 MAE: 0.4641999900341034¬±0.0 R2: 0.4444¬±0.0 PCC: 0.7381¬±0.0 Cosine Similarity: 0.8076¬±0.0
scGPT - INFO - By sample: MSE: 0.567300021648407¬±0.0 MAE: 0.4641999900341034¬±0.0 R2: 0.1083¬±0.0 PCC: 0.2715¬±0.0 Cosine Similarity: 0.548¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  9
Training model
scGPT - INFO - | epoch   9 | 100/1949 batches | lr 0.0000 | ms/batch 91.88 | loss 46.10 | mse 46.10 | mre  0.00 |
scGPT - INFO - | epoch   9 | 200/1949 batches | lr 0.0000 | ms/batch 90.68 | loss 44.89 | mse 44.89 | mre  0.00 |
scGPT - INFO - | epoch   9 | 300/1949 batches | lr 0.0000 | ms/batch 90.73 | loss 48.13 | mse 48.13 | mre  0.00 |
scGPT - INFO - | epoch   9 | 400/1949 batches | lr 0.0000 | ms/batch 90.71 | loss 45.37 | mse 45.37 | mre  0.00 |
scGPT - INFO - | epoch   9 | 500/1949 batches | lr 0.0000 | ms/batch 91.18 | loss 46.35 | mse 46.35 | mre  0.00 |
scGPT - INFO - | epoch   9 | 600/1949 batches | lr 0.0000 | ms/batch 90.88 | loss 45.19 | mse 45.19 | mre  0.00 |
scGPT - INFO - | epoch   9 | 700/1949 batches | lr 0.0000 | ms/batch 90.73 | loss 46.33 | mse 46.33 | mre  0.00 |
scGPT - INFO - | epoch   9 | 800/1949 batches | lr 0.0000 | ms/batch 90.74 | loss 45.93 | mse 45.93 | mre  0.00 |
scGPT - INFO - | epoch   9 | 900/1949 batches | lr 0.0000 | ms/batch 90.77 | loss 43.95 | mse 43.95 | mre  0.00 |
scGPT - INFO - | epoch   9 | 1000/1949 batches | lr 0.0000 | ms/batch 90.67 | loss 47.76 | mse 47.76 | mre  0.00 |
scGPT - INFO - | epoch   9 | 1100/1949 batches | lr 0.0000 | ms/batch 90.68 | loss 46.55 | mse 46.55 | mre  0.00 |
scGPT - INFO - | epoch   9 | 1200/1949 batches | lr 0.0000 | ms/batch 90.65 | loss 44.67 | mse 44.67 | mre  0.00 |
scGPT - INFO - | epoch   9 | 1300/1949 batches | lr 0.0000 | ms/batch 90.67 | loss 43.99 | mse 43.99 | mre  0.00 |
scGPT - INFO - | epoch   9 | 1400/1949 batches | lr 0.0000 | ms/batch 90.71 | loss 45.66 | mse 45.66 | mre  0.00 |
scGPT - INFO - | epoch   9 | 1500/1949 batches | lr 0.0000 | ms/batch 91.22 | loss 46.51 | mse 46.51 | mre  0.00 |
scGPT - INFO - | epoch   9 | 1600/1949 batches | lr 0.0000 | ms/batch 90.69 | loss 43.56 | mse 43.56 | mre  0.00 |
scGPT - INFO - | epoch   9 | 1700/1949 batches | lr 0.0000 | ms/batch 90.69 | loss 49.44 | mse 49.44 | mre  0.00 |
scGPT - INFO - | epoch   9 | 1800/1949 batches | lr 0.0000 | ms/batch 90.68 | loss 43.92 | mse 43.92 | mre  0.00 |
scGPT - INFO - | epoch   9 | 1900/1949 batches | lr 0.0000 | ms/batch 90.64 | loss 43.83 | mse 43.83 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   9 | time: 183.64s | valid loss/mse 47.4896 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 47.4896
best_loss: 47.621519044695496, min_delta 0.0001, val_loss 47.489570741014724
Loss error: 0.13194830368077248
epoch:  9
[[0.        0.        0.        ... 4.634729  0.        0.       ]
 [0.        0.        0.        ... 1.0986123 0.6931472 0.       ]
 [0.        0.        0.6931472 ... 0.6931472 0.        0.       ]
 ...
 [0.        3.218876  0.6931472 ... 1.9459102 0.        0.       ]
 [0.        3.3322046 1.609438  ... 2.0794415 0.        0.       ]
 [0.        4.7706847 3.1354942 ... 2.3025851 0.        0.       ]]
[[0.07056 0.7944  0.3325  ... 2.71    0.07556 0.06726]
 [0.0827  0.756   0.2942  ... 2.49    0.07526 0.06033]
 [0.0782  0.786   0.2998  ... 2.656   0.0744  0.0646 ]
 ...
 [0.05408 3.516   1.814   ... 2.441   0.08905 0.0784 ]
 [0.03314 3.166   1.419   ... 2.447   0.11975 0.07135]
 [0.0399  4.066   2.246   ... 2.543   0.0959  0.0719 ]]
(433, 42) (433, 42)
By feature:  MSE:  0.5656¬±0.0 MAE:  0.4628¬±0.0 R2:  0.4455¬±0.0 PCC:  0.739¬±0.0 Cosine Similarity:  0.8082¬±0.0
By sample:  MSE:  0.5656¬±0.0 MAE:  0.4628¬±0.0 R2:  0.1094¬±0.0 PCC:  0.2752¬±0.0 Cosine Similarity:  0.548¬±0.0
scGPT - INFO - By feature: MSE: 0.5655999779701233¬±0.0 MAE: 0.462799996137619¬±0.0 R2: 0.4455¬±0.0 PCC: 0.739¬±0.0 Cosine Similarity: 0.8082¬±0.0
scGPT - INFO - By sample: MSE: 0.5655999779701233¬±0.0 MAE: 0.462799996137619¬±0.0 R2: 0.1094¬±0.0 PCC: 0.2752¬±0.0 Cosine Similarity: 0.548¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  10
Training model
scGPT - INFO - | epoch  10 | 100/1949 batches | lr 0.0000 | ms/batch 91.83 | loss 44.38 | mse 44.38 | mre  0.00 |
scGPT - INFO - | epoch  10 | 200/1949 batches | lr 0.0000 | ms/batch 90.64 | loss 44.43 | mse 44.43 | mre  0.00 |
scGPT - INFO - | epoch  10 | 300/1949 batches | lr 0.0000 | ms/batch 90.66 | loss 46.36 | mse 46.36 | mre  0.00 |
scGPT - INFO - | epoch  10 | 400/1949 batches | lr 0.0000 | ms/batch 90.65 | loss 44.93 | mse 44.93 | mre  0.00 |
scGPT - INFO - | epoch  10 | 500/1949 batches | lr 0.0000 | ms/batch 91.19 | loss 46.69 | mse 46.69 | mre  0.00 |
scGPT - INFO - | epoch  10 | 600/1949 batches | lr 0.0000 | ms/batch 90.63 | loss 44.84 | mse 44.84 | mre  0.00 |
scGPT - INFO - | epoch  10 | 700/1949 batches | lr 0.0000 | ms/batch 90.62 | loss 46.26 | mse 46.26 | mre  0.00 |
scGPT - INFO - | epoch  10 | 800/1949 batches | lr 0.0000 | ms/batch 90.66 | loss 45.88 | mse 45.88 | mre  0.00 |
scGPT - INFO - | epoch  10 | 900/1949 batches | lr 0.0000 | ms/batch 90.62 | loss 44.10 | mse 44.10 | mre  0.00 |
scGPT - INFO - | epoch  10 | 1000/1949 batches | lr 0.0000 | ms/batch 90.61 | loss 47.13 | mse 47.13 | mre  0.00 |
scGPT - INFO - | epoch  10 | 1100/1949 batches | lr 0.0000 | ms/batch 90.61 | loss 46.58 | mse 46.58 | mre  0.00 |
scGPT - INFO - | epoch  10 | 1200/1949 batches | lr 0.0000 | ms/batch 90.65 | loss 43.12 | mse 43.12 | mre  0.00 |
scGPT - INFO - | epoch  10 | 1300/1949 batches | lr 0.0000 | ms/batch 90.60 | loss 44.24 | mse 44.24 | mre  0.00 |
scGPT - INFO - | epoch  10 | 1400/1949 batches | lr 0.0000 | ms/batch 90.70 | loss 44.85 | mse 44.85 | mre  0.00 |
scGPT - INFO - | epoch  10 | 1500/1949 batches | lr 0.0000 | ms/batch 91.18 | loss 45.33 | mse 45.33 | mre  0.00 |
scGPT - INFO - | epoch  10 | 1600/1949 batches | lr 0.0000 | ms/batch 90.66 | loss 42.64 | mse 42.64 | mre  0.00 |
scGPT - INFO - | epoch  10 | 1700/1949 batches | lr 0.0000 | ms/batch 90.65 | loss 48.94 | mse 48.94 | mre  0.00 |
scGPT - INFO - | epoch  10 | 1800/1949 batches | lr 0.0000 | ms/batch 90.69 | loss 42.82 | mse 42.82 | mre  0.00 |
scGPT - INFO - | epoch  10 | 1900/1949 batches | lr 0.0000 | ms/batch 90.65 | loss 43.12 | mse 43.12 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  10 | time: 183.49s | valid loss/mse 47.6814 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 47.489570741014724, min_delta 0.0001, val_loss 47.68135613051628
Loss error: -0.191785389501554
scGPT - INFO - INFO: Early stopping counter 1 of 10
epoch:  10
[[0.        0.        0.        ... 4.634729  0.        0.       ]
 [0.        0.        0.        ... 1.0986123 0.6931472 0.       ]
 [0.        0.        0.6931472 ... 0.6931472 0.        0.       ]
 ...
 [0.        3.218876  0.6931472 ... 1.9459102 0.        0.       ]
 [0.        3.3322046 1.609438  ... 2.0794415 0.        0.       ]
 [0.        4.7706847 3.1354942 ... 2.3025851 0.        0.       ]]
[[0.0659  0.7954  0.3086  ... 2.662   0.10254 0.0617 ]
 [0.0596  0.745   0.2625  ... 2.443   0.0918  0.05145]
 [0.06506 0.7876  0.291   ... 2.693   0.0945  0.06964]
 ...
 [0.0668  3.652   1.901   ... 2.447   0.0824  0.07654]
 [0.0209  3.418   1.527   ... 2.39    0.11035 0.0575 ]
 [0.04382 4.176   2.371   ... 2.514   0.08875 0.08826]]
(433, 42) (433, 42)
By feature:  MSE:  0.5679¬±0.0 MAE:  0.4637¬±0.0 R2:  0.4391¬±0.0 PCC:  0.7393¬±0.0 Cosine Similarity:  0.8084¬±0.0
By sample:  MSE:  0.5679¬±0.0 MAE:  0.4637¬±0.0 R2:  0.1062¬±0.0 PCC:  0.2789¬±0.0 Cosine Similarity:  0.5479¬±0.0
scGPT - INFO - By feature: MSE: 0.5679000020027161¬±0.0 MAE: 0.46369999647140503¬±0.0 R2: 0.4391¬±0.0 PCC: 0.7393¬±0.0 Cosine Similarity: 0.8084¬±0.0
scGPT - INFO - By sample: MSE: 0.5679000020027161¬±0.0 MAE: 0.46369999647140503¬±0.0 R2: 0.1062¬±0.0 PCC: 0.2789¬±0.0 Cosine Similarity: 0.5479¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  11
Training model
scGPT - INFO - | epoch  11 | 100/1949 batches | lr 0.0000 | ms/batch 91.93 | loss 44.28 | mse 44.28 | mre  0.00 |
scGPT - INFO - | epoch  11 | 200/1949 batches | lr 0.0000 | ms/batch 90.63 | loss 44.12 | mse 44.12 | mre  0.00 |
scGPT - INFO - | epoch  11 | 300/1949 batches | lr 0.0000 | ms/batch 90.61 | loss 46.33 | mse 46.33 | mre  0.00 |
scGPT - INFO - | epoch  11 | 400/1949 batches | lr 0.0000 | ms/batch 90.63 | loss 43.93 | mse 43.93 | mre  0.00 |
scGPT - INFO - | epoch  11 | 500/1949 batches | lr 0.0000 | ms/batch 90.69 | loss 46.13 | mse 46.13 | mre  0.00 |
scGPT - INFO - | epoch  11 | 600/1949 batches | lr 0.0000 | ms/batch 91.19 | loss 43.83 | mse 43.83 | mre  0.00 |
scGPT - INFO - | epoch  11 | 700/1949 batches | lr 0.0000 | ms/batch 90.68 | loss 46.36 | mse 46.36 | mre  0.00 |
scGPT - INFO - | epoch  11 | 800/1949 batches | lr 0.0000 | ms/batch 90.63 | loss 45.02 | mse 45.02 | mre  0.00 |
scGPT - INFO - | epoch  11 | 900/1949 batches | lr 0.0000 | ms/batch 90.65 | loss 43.11 | mse 43.11 | mre  0.00 |
scGPT - INFO - | epoch  11 | 1000/1949 batches | lr 0.0000 | ms/batch 90.68 | loss 46.73 | mse 46.73 | mre  0.00 |
scGPT - INFO - | epoch  11 | 1100/1949 batches | lr 0.0000 | ms/batch 90.71 | loss 45.56 | mse 45.56 | mre  0.00 |
scGPT - INFO - | epoch  11 | 1200/1949 batches | lr 0.0000 | ms/batch 90.65 | loss 42.30 | mse 42.30 | mre  0.00 |
scGPT - INFO - | epoch  11 | 1300/1949 batches | lr 0.0000 | ms/batch 90.67 | loss 43.79 | mse 43.79 | mre  0.00 |
scGPT - INFO - | epoch  11 | 1400/1949 batches | lr 0.0000 | ms/batch 90.70 | loss 44.61 | mse 44.61 | mre  0.00 |
scGPT - INFO - | epoch  11 | 1500/1949 batches | lr 0.0000 | ms/batch 90.66 | loss 44.74 | mse 44.74 | mre  0.00 |
scGPT - INFO - | epoch  11 | 1600/1949 batches | lr 0.0000 | ms/batch 91.18 | loss 42.13 | mse 42.13 | mre  0.00 |
scGPT - INFO - | epoch  11 | 1700/1949 batches | lr 0.0000 | ms/batch 90.66 | loss 48.69 | mse 48.69 | mre  0.00 |
scGPT - INFO - | epoch  11 | 1800/1949 batches | lr 0.0000 | ms/batch 90.64 | loss 42.34 | mse 42.34 | mre  0.00 |
scGPT - INFO - | epoch  11 | 1900/1949 batches | lr 0.0000 | ms/batch 90.67 | loss 41.65 | mse 41.65 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  11 | time: 183.53s | valid loss/mse 47.3639 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 47.3639
best_loss: 47.489570741014724, min_delta 0.0001, val_loss 47.363856253943325
Loss error: 0.12571448707139865
epoch:  11
[[0.        0.        0.        ... 4.634729  0.        0.       ]
 [0.        0.        0.        ... 1.0986123 0.6931472 0.       ]
 [0.        0.        0.6931472 ... 0.6931472 0.        0.       ]
 ...
 [0.        3.218876  0.6931472 ... 1.9459102 0.        0.       ]
 [0.        3.3322046 1.609438  ... 2.0794415 0.        0.       ]
 [0.        4.7706847 3.1354942 ... 2.3025851 0.        0.       ]]
[[0.06915 0.762   0.3013  ... 2.535   0.09326 0.05695]
 [0.04987 0.7373  0.2625  ... 2.318   0.0818  0.0627 ]
 [0.0475  0.751   0.2588  ... 2.451   0.08704 0.0729 ]
 ...
 [0.06    3.857   2.176   ... 2.438   0.0852  0.07947]
 [0.02802 3.527   1.591   ... 2.34    0.1134  0.05157]
 [0.05103 4.223   2.422   ... 2.516   0.10077 0.08075]]
(433, 42) (433, 42)
By feature:  MSE:  0.5641¬±0.0 MAE:  0.46¬±0.0 R2:  0.4459¬±0.0 PCC:  0.7404¬±0.0 Cosine Similarity:  0.8091¬±0.0
By sample:  MSE:  0.5641¬±0.0 MAE:  0.46¬±0.0 R2:  0.1109¬±0.0 PCC:  0.2836¬±0.0 Cosine Similarity:  0.5491¬±0.0
scGPT - INFO - By feature: MSE: 0.5641000270843506¬±0.0 MAE: 0.46000000834465027¬±0.0 R2: 0.4459¬±0.0 PCC: 0.7404¬±0.0 Cosine Similarity: 0.8091¬±0.0
scGPT - INFO - By sample: MSE: 0.5641000270843506¬±0.0 MAE: 0.46000000834465027¬±0.0 R2: 0.1109¬±0.0 PCC: 0.2836¬±0.0 Cosine Similarity: 0.5491¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  12
Training model
scGPT - INFO - | epoch  12 | 100/1949 batches | lr 0.0000 | ms/batch 91.91 | loss 43.02 | mse 43.02 | mre  0.00 |
scGPT - INFO - | epoch  12 | 200/1949 batches | lr 0.0000 | ms/batch 90.66 | loss 43.94 | mse 43.94 | mre  0.00 |
scGPT - INFO - | epoch  12 | 300/1949 batches | lr 0.0000 | ms/batch 90.65 | loss 46.02 | mse 46.02 | mre  0.00 |
scGPT - INFO - | epoch  12 | 400/1949 batches | lr 0.0000 | ms/batch 90.62 | loss 44.33 | mse 44.33 | mre  0.00 |
scGPT - INFO - | epoch  12 | 500/1949 batches | lr 0.0000 | ms/batch 90.68 | loss 45.16 | mse 45.16 | mre  0.00 |
scGPT - INFO - | epoch  12 | 600/1949 batches | lr 0.0000 | ms/batch 91.75 | loss 43.74 | mse 43.74 | mre  0.00 |
scGPT - INFO - | epoch  12 | 700/1949 batches | lr 0.0000 | ms/batch 90.80 | loss 45.42 | mse 45.42 | mre  0.00 |
scGPT - INFO - | epoch  12 | 800/1949 batches | lr 0.0000 | ms/batch 90.66 | loss 43.61 | mse 43.61 | mre  0.00 |
scGPT - INFO - | epoch  12 | 900/1949 batches | lr 0.0000 | ms/batch 90.68 | loss 42.41 | mse 42.41 | mre  0.00 |
scGPT - INFO - | epoch  12 | 1000/1949 batches | lr 0.0000 | ms/batch 90.69 | loss 46.32 | mse 46.32 | mre  0.00 |
scGPT - INFO - | epoch  12 | 1100/1949 batches | lr 0.0000 | ms/batch 90.71 | loss 45.61 | mse 45.61 | mre  0.00 |
scGPT - INFO - | epoch  12 | 1200/1949 batches | lr 0.0000 | ms/batch 90.65 | loss 41.85 | mse 41.85 | mre  0.00 |
scGPT - INFO - | epoch  12 | 1300/1949 batches | lr 0.0000 | ms/batch 90.70 | loss 42.23 | mse 42.23 | mre  0.00 |
scGPT - INFO - | epoch  12 | 1400/1949 batches | lr 0.0000 | ms/batch 90.64 | loss 43.42 | mse 43.42 | mre  0.00 |
scGPT - INFO - | epoch  12 | 1500/1949 batches | lr 0.0000 | ms/batch 90.70 | loss 44.68 | mse 44.68 | mre  0.00 |
scGPT - INFO - | epoch  12 | 1600/1949 batches | lr 0.0000 | ms/batch 91.24 | loss 41.51 | mse 41.51 | mre  0.00 |
scGPT - INFO - | epoch  12 | 1700/1949 batches | lr 0.0000 | ms/batch 90.66 | loss 48.03 | mse 48.03 | mre  0.00 |
scGPT - INFO - | epoch  12 | 1800/1949 batches | lr 0.0000 | ms/batch 90.67 | loss 41.16 | mse 41.16 | mre  0.00 |
scGPT - INFO - | epoch  12 | 1900/1949 batches | lr 0.0000 | ms/batch 90.68 | loss 40.91 | mse 40.91 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  12 | time: 183.64s | valid loss/mse 47.7336 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 47.363856253943325, min_delta 0.0001, val_loss 47.733597295113576
Loss error: -0.36974104117025064
scGPT - INFO - INFO: Early stopping counter 1 of 10
epoch:  12
[[0.        0.        0.        ... 4.634729  0.        0.       ]
 [0.        0.        0.        ... 1.0986123 0.6931472 0.       ]
 [0.        0.        0.6931472 ... 0.6931472 0.        0.       ]
 ...
 [0.        3.218876  0.6931472 ... 1.9459102 0.        0.       ]
 [0.        3.3322046 1.609438  ... 2.0794415 0.        0.       ]
 [0.        4.7706847 3.1354942 ... 2.3025851 0.        0.       ]]
[[0.0548  0.7466  0.2725  ... 2.348   0.0913  0.0645 ]
 [0.0421  0.7734  0.2764  ... 2.232   0.07214 0.0679 ]
 [0.03815 0.7866  0.2747  ... 2.404   0.08203 0.0816 ]
 ...
 [0.03766 3.828   2.207   ... 2.39    0.0907  0.0868 ]
 [0.02036 3.693   1.706   ... 2.334   0.11975 0.06113]
 [0.02942 4.188   2.457   ... 2.494   0.1     0.095  ]]
(433, 42) (433, 42)
By feature:  MSE:  0.5685¬±0.0 MAE:  0.4562¬±0.0 R2:  0.4489¬±0.0 PCC:  0.7371¬±0.0 Cosine Similarity:  0.8066¬±0.0
By sample:  MSE:  0.5685¬±0.0 MAE:  0.4562¬±0.0 R2:  0.1103¬±0.0 PCC:  0.2842¬±0.0 Cosine Similarity:  0.5491¬±0.0
scGPT - INFO - By feature: MSE: 0.5684999823570251¬±0.0 MAE: 0.4562000036239624¬±0.0 R2: 0.4489¬±0.0 PCC: 0.7371¬±0.0 Cosine Similarity: 0.8066¬±0.0
scGPT - INFO - By sample: MSE: 0.5684999823570251¬±0.0 MAE: 0.4562000036239624¬±0.0 R2: 0.1103¬±0.0 PCC: 0.2842¬±0.0 Cosine Similarity: 0.5491¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  13
Training model
scGPT - INFO - | epoch  13 | 100/1949 batches | lr 0.0000 | ms/batch 91.90 | loss 42.40 | mse 42.40 | mre  0.00 |
scGPT - INFO - | epoch  13 | 200/1949 batches | lr 0.0000 | ms/batch 90.66 | loss 42.66 | mse 42.66 | mre  0.00 |
scGPT - INFO - | epoch  13 | 300/1949 batches | lr 0.0000 | ms/batch 90.70 | loss 45.59 | mse 45.59 | mre  0.00 |
scGPT - INFO - | epoch  13 | 400/1949 batches | lr 0.0000 | ms/batch 90.73 | loss 43.29 | mse 43.29 | mre  0.00 |
scGPT - INFO - | epoch  13 | 500/1949 batches | lr 0.0000 | ms/batch 90.70 | loss 45.14 | mse 45.14 | mre  0.00 |
scGPT - INFO - | epoch  13 | 600/1949 batches | lr 0.0000 | ms/batch 90.75 | loss 42.48 | mse 42.48 | mre  0.00 |
scGPT - INFO - | epoch  13 | 700/1949 batches | lr 0.0000 | ms/batch 91.31 | loss 45.05 | mse 45.05 | mre  0.00 |
scGPT - INFO - | epoch  13 | 800/1949 batches | lr 0.0000 | ms/batch 90.80 | loss 42.72 | mse 42.72 | mre  0.00 |
scGPT - INFO - | epoch  13 | 900/1949 batches | lr 0.0000 | ms/batch 90.80 | loss 41.55 | mse 41.55 | mre  0.00 |
scGPT - INFO - | epoch  13 | 1000/1949 batches | lr 0.0000 | ms/batch 90.79 | loss 46.52 | mse 46.52 | mre  0.00 |
scGPT - INFO - | epoch  13 | 1100/1949 batches | lr 0.0000 | ms/batch 90.77 | loss 44.93 | mse 44.93 | mre  0.00 |
scGPT - INFO - | epoch  13 | 1200/1949 batches | lr 0.0000 | ms/batch 90.80 | loss 41.28 | mse 41.28 | mre  0.00 |
scGPT - INFO - | epoch  13 | 1300/1949 batches | lr 0.0000 | ms/batch 90.72 | loss 42.24 | mse 42.24 | mre  0.00 |
scGPT - INFO - | epoch  13 | 1400/1949 batches | lr 0.0000 | ms/batch 90.73 | loss 42.82 | mse 42.82 | mre  0.00 |
scGPT - INFO - | epoch  13 | 1500/1949 batches | lr 0.0000 | ms/batch 90.68 | loss 43.76 | mse 43.76 | mre  0.00 |
scGPT - INFO - | epoch  13 | 1600/1949 batches | lr 0.0000 | ms/batch 90.74 | loss 40.84 | mse 40.84 | mre  0.00 |
scGPT - INFO - | epoch  13 | 1700/1949 batches | lr 0.0000 | ms/batch 91.32 | loss 47.20 | mse 47.20 | mre  0.00 |
scGPT - INFO - | epoch  13 | 1800/1949 batches | lr 0.0000 | ms/batch 90.80 | loss 41.21 | mse 41.21 | mre  0.00 |
scGPT - INFO - | epoch  13 | 1900/1949 batches | lr 0.0000 | ms/batch 90.81 | loss 40.82 | mse 40.82 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  13 | time: 183.71s | valid loss/mse 47.2602 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 47.2602
best_loss: 47.363856253943325, min_delta 0.0001, val_loss 47.2601818058287
Loss error: 0.10367444811462434
epoch:  13
[[0.        0.        0.        ... 4.634729  0.        0.       ]
 [0.        0.        0.        ... 1.0986123 0.6931472 0.       ]
 [0.        0.        0.6931472 ... 0.6931472 0.        0.       ]
 ...
 [0.        3.218876  0.6931472 ... 1.9459102 0.        0.       ]
 [0.        3.3322046 1.609438  ... 2.0794415 0.        0.       ]
 [0.        4.7706847 3.1354942 ... 2.3025851 0.        0.       ]]
[[0.0527  0.7466  0.281   ... 2.293   0.09906 0.07996]
 [0.03049 0.794   0.2893  ... 2.2     0.0751  0.0987 ]
 [0.03528 0.8022  0.27    ... 2.309   0.08185 0.10034]
 ...
 [0.04556 3.846   2.195   ... 2.426   0.08636 0.09375]
 [0.01426 3.607   1.616   ... 2.258   0.11993 0.04904]
 [0.04047 4.24    2.53    ... 2.527   0.0821  0.1099 ]]
(433, 42) (433, 42)
By feature:  MSE:  0.5628¬±0.0 MAE:  0.454¬±0.0 R2:  0.4539¬±0.0 PCC:  0.7396¬±0.0 Cosine Similarity:  0.8084¬±0.0
By sample:  MSE:  0.5628¬±0.0 MAE:  0.454¬±0.0 R2:  0.1139¬±0.0 PCC:  0.2889¬±0.0 Cosine Similarity:  0.5508¬±0.0
scGPT - INFO - By feature: MSE: 0.5627999901771545¬±0.0 MAE: 0.45399999618530273¬±0.0 R2: 0.4539¬±0.0 PCC: 0.7396¬±0.0 Cosine Similarity: 0.8084¬±0.0
scGPT - INFO - By sample: MSE: 0.5627999901771545¬±0.0 MAE: 0.45399999618530273¬±0.0 R2: 0.1139¬±0.0 PCC: 0.2889¬±0.0 Cosine Similarity: 0.5508¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  14
Training model
scGPT - INFO - | epoch  14 | 100/1949 batches | lr 0.0000 | ms/batch 91.85 | loss 41.48 | mse 41.48 | mre  0.00 |
scGPT - INFO - | epoch  14 | 200/1949 batches | lr 0.0000 | ms/batch 90.66 | loss 41.42 | mse 41.42 | mre  0.00 |
scGPT - INFO - | epoch  14 | 300/1949 batches | lr 0.0000 | ms/batch 90.61 | loss 44.45 | mse 44.45 | mre  0.00 |
scGPT - INFO - | epoch  14 | 400/1949 batches | lr 0.0000 | ms/batch 90.90 | loss 42.80 | mse 42.80 | mre  0.00 |
scGPT - INFO - | epoch  14 | 500/1949 batches | lr 0.0000 | ms/batch 90.66 | loss 44.89 | mse 44.89 | mre  0.00 |
scGPT - INFO - | epoch  14 | 600/1949 batches | lr 0.0000 | ms/batch 90.67 | loss 42.59 | mse 42.59 | mre  0.00 |
scGPT - INFO - | epoch  14 | 700/1949 batches | lr 0.0000 | ms/batch 91.17 | loss 45.04 | mse 45.04 | mre  0.00 |
scGPT - INFO - | epoch  14 | 800/1949 batches | lr 0.0000 | ms/batch 90.69 | loss 43.49 | mse 43.49 | mre  0.00 |
scGPT - INFO - | epoch  14 | 900/1949 batches | lr 0.0000 | ms/batch 90.68 | loss 41.70 | mse 41.70 | mre  0.00 |
scGPT - INFO - | epoch  14 | 1000/1949 batches | lr 0.0000 | ms/batch 90.69 | loss 45.90 | mse 45.90 | mre  0.00 |
scGPT - INFO - | epoch  14 | 1100/1949 batches | lr 0.0000 | ms/batch 90.79 | loss 43.71 | mse 43.71 | mre  0.00 |
scGPT - INFO - | epoch  14 | 1200/1949 batches | lr 0.0000 | ms/batch 90.74 | loss 39.99 | mse 39.99 | mre  0.00 |
scGPT - INFO - | epoch  14 | 1300/1949 batches | lr 0.0000 | ms/batch 90.74 | loss 41.60 | mse 41.60 | mre  0.00 |
scGPT - INFO - | epoch  14 | 1400/1949 batches | lr 0.0000 | ms/batch 90.77 | loss 41.90 | mse 41.90 | mre  0.00 |
scGPT - INFO - | epoch  14 | 1500/1949 batches | lr 0.0000 | ms/batch 90.79 | loss 43.54 | mse 43.54 | mre  0.00 |
scGPT - INFO - | epoch  14 | 1600/1949 batches | lr 0.0000 | ms/batch 90.80 | loss 40.53 | mse 40.53 | mre  0.00 |
scGPT - INFO - | epoch  14 | 1700/1949 batches | lr 0.0000 | ms/batch 91.24 | loss 46.80 | mse 46.80 | mre  0.00 |
scGPT - INFO - | epoch  14 | 1800/1949 batches | lr 0.0000 | ms/batch 90.68 | loss 39.72 | mse 39.72 | mre  0.00 |
scGPT - INFO - | epoch  14 | 1900/1949 batches | lr 0.0000 | ms/batch 90.75 | loss 39.16 | mse 39.16 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  14 | time: 183.63s | valid loss/mse 48.2122 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 47.2601818058287, min_delta 0.0001, val_loss 48.21216219952696
Loss error: -0.9519803936982569
scGPT - INFO - INFO: Early stopping counter 1 of 10
epoch:  14
[[0.        0.        0.        ... 4.634729  0.        0.       ]
 [0.        0.        0.        ... 1.0986123 0.6931472 0.       ]
 [0.        0.        0.6931472 ... 0.6931472 0.        0.       ]
 ...
 [0.        3.218876  0.6931472 ... 1.9459102 0.        0.       ]
 [0.        3.3322046 1.609438  ... 2.0794415 0.        0.       ]
 [0.        4.7706847 3.1354942 ... 2.3025851 0.        0.       ]]
[[0.04462 0.7725  0.2583  ... 2.266   0.08636 0.0814 ]
 [0.02861 0.8184  0.2656  ... 2.125   0.06107 0.08875]
 [0.03394 0.835   0.254   ... 2.244   0.07404 0.09357]
 ...
 [0.0638  3.807   2.127   ... 2.445   0.0782  0.0775 ]
 [0.03174 3.623   1.579   ... 2.281   0.113   0.04834]
 [0.05032 4.293   2.564   ... 2.54    0.079   0.09875]]
(433, 42) (433, 42)
By feature:  MSE:  0.5742¬±0.0 MAE:  0.4572¬±0.0 R2:  0.4388¬±0.0 PCC:  0.7354¬±0.0 Cosine Similarity:  0.8055¬±0.0
By sample:  MSE:  0.5742¬±0.0 MAE:  0.4572¬±0.0 R2:  0.1088¬±0.0 PCC:  0.286¬±0.0 Cosine Similarity:  0.5497¬±0.0
scGPT - INFO - By feature: MSE: 0.5741999745368958¬±0.0 MAE: 0.45719999074935913¬±0.0 R2: 0.4388¬±0.0 PCC: 0.7354¬±0.0 Cosine Similarity: 0.8055¬±0.0
scGPT - INFO - By sample: MSE: 0.5741999745368958¬±0.0 MAE: 0.45719999074935913¬±0.0 R2: 0.1088¬±0.0 PCC: 0.286¬±0.0 Cosine Similarity: 0.5497¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  15
Training model
scGPT - INFO - | epoch  15 | 100/1949 batches | lr 0.0000 | ms/batch 91.89 | loss 40.72 | mse 40.72 | mre  0.00 |
scGPT - INFO - | epoch  15 | 200/1949 batches | lr 0.0000 | ms/batch 90.69 | loss 40.58 | mse 40.58 | mre  0.00 |
scGPT - INFO - | epoch  15 | 300/1949 batches | lr 0.0000 | ms/batch 90.72 | loss 43.83 | mse 43.83 | mre  0.00 |
scGPT - INFO - | epoch  15 | 400/1949 batches | lr 0.0000 | ms/batch 90.69 | loss 43.01 | mse 43.01 | mre  0.00 |
scGPT - INFO - | epoch  15 | 500/1949 batches | lr 0.0000 | ms/batch 90.70 | loss 44.19 | mse 44.19 | mre  0.00 |
scGPT - INFO - | epoch  15 | 600/1949 batches | lr 0.0000 | ms/batch 90.87 | loss 40.77 | mse 40.77 | mre  0.00 |
scGPT - INFO - | epoch  15 | 700/1949 batches | lr 0.0000 | ms/batch 90.81 | loss 45.17 | mse 45.17 | mre  0.00 |
scGPT - INFO - | epoch  15 | 800/1949 batches | lr 0.0000 | ms/batch 91.42 | loss 42.15 | mse 42.15 | mre  0.00 |
scGPT - INFO - | epoch  15 | 900/1949 batches | lr 0.0000 | ms/batch 90.81 | loss 39.99 | mse 39.99 | mre  0.00 |
scGPT - INFO - | epoch  15 | 1000/1949 batches | lr 0.0000 | ms/batch 90.79 | loss 45.04 | mse 45.04 | mre  0.00 |
scGPT - INFO - | epoch  15 | 1100/1949 batches | lr 0.0000 | ms/batch 90.75 | loss 44.26 | mse 44.26 | mre  0.00 |
scGPT - INFO - | epoch  15 | 1200/1949 batches | lr 0.0000 | ms/batch 90.88 | loss 39.78 | mse 39.78 | mre  0.00 |
scGPT - INFO - | epoch  15 | 1300/1949 batches | lr 0.0000 | ms/batch 90.84 | loss 40.27 | mse 40.27 | mre  0.00 |
scGPT - INFO - | epoch  15 | 1400/1949 batches | lr 0.0000 | ms/batch 90.87 | loss 41.05 | mse 41.05 | mre  0.00 |
scGPT - INFO - | epoch  15 | 1500/1949 batches | lr 0.0000 | ms/batch 90.89 | loss 42.96 | mse 42.96 | mre  0.00 |
scGPT - INFO - | epoch  15 | 1600/1949 batches | lr 0.0000 | ms/batch 90.86 | loss 40.00 | mse 40.00 | mre  0.00 |
scGPT - INFO - | epoch  15 | 1700/1949 batches | lr 0.0000 | ms/batch 90.87 | loss 46.99 | mse 46.99 | mre  0.00 |
scGPT - INFO - | epoch  15 | 1800/1949 batches | lr 0.0000 | ms/batch 91.41 | loss 38.82 | mse 38.82 | mre  0.00 |
scGPT - INFO - | epoch  15 | 1900/1949 batches | lr 0.0000 | ms/batch 90.86 | loss 39.37 | mse 39.37 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  15 | time: 183.81s | valid loss/mse 48.4607 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 47.2601818058287, min_delta 0.0001, val_loss 48.460658801345296
Loss error: -1.2004769955165955
scGPT - INFO - INFO: Early stopping counter 2 of 10
epoch:  15
[[0.        0.        0.        ... 4.634729  0.        0.       ]
 [0.        0.        0.        ... 1.0986123 0.6931472 0.       ]
 [0.        0.        0.6931472 ... 0.6931472 0.        0.       ]
 ...
 [0.        3.218876  0.6931472 ... 1.9459102 0.        0.       ]
 [0.        3.3322046 1.609438  ... 2.0794415 0.        0.       ]
 [0.        4.7706847 3.1354942 ... 2.3025851 0.        0.       ]]
[[0.05002 0.736   0.2512  ... 2.176   0.11444 0.06073]
 [0.03125 0.794   0.2676  ... 1.984   0.0627  0.08435]
 [0.0364  0.83    0.2656  ... 2.172   0.09796 0.0896 ]
 ...
 [0.0515  3.72    2.105   ... 2.379   0.0858  0.1026 ]
 [0.00599 3.332   1.395   ... 2.156   0.1154  0.04944]
 [0.04797 4.28    2.6     ... 2.55    0.0936  0.1211 ]]
(433, 42) (433, 42)
By feature:  MSE:  0.5771¬±0.0 MAE:  0.456¬±0.0 R2:  0.4333¬±0.0 PCC:  0.7305¬±0.0 Cosine Similarity:  0.8016¬±0.0
By sample:  MSE:  0.5771¬±0.0 MAE:  0.456¬±0.0 R2:  0.1079¬±0.0 PCC:  0.2819¬±0.0 Cosine Similarity:  0.5481¬±0.0
scGPT - INFO - By feature: MSE: 0.5770999789237976¬±0.0 MAE: 0.4560000002384186¬±0.0 R2: 0.4333¬±0.0 PCC: 0.7305¬±0.0 Cosine Similarity: 0.8016¬±0.0
scGPT - INFO - By sample: MSE: 0.5770999789237976¬±0.0 MAE: 0.4560000002384186¬±0.0 R2: 0.1079¬±0.0 PCC: 0.2819¬±0.0 Cosine Similarity: 0.5481¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  16
Training model
scGPT - INFO - | epoch  16 | 100/1949 batches | lr 0.0000 | ms/batch 92.06 | loss 41.73 | mse 41.73 | mre  0.00 |
scGPT - INFO - | epoch  16 | 200/1949 batches | lr 0.0000 | ms/batch 90.83 | loss 39.60 | mse 39.60 | mre  0.00 |
scGPT - INFO - | epoch  16 | 300/1949 batches | lr 0.0000 | ms/batch 90.84 | loss 43.84 | mse 43.84 | mre  0.00 |
scGPT - INFO - | epoch  16 | 400/1949 batches | lr 0.0000 | ms/batch 90.85 | loss 42.77 | mse 42.77 | mre  0.00 |
scGPT - INFO - | epoch  16 | 500/1949 batches | lr 0.0000 | ms/batch 90.88 | loss 44.21 | mse 44.21 | mre  0.00 |
scGPT - INFO - | epoch  16 | 600/1949 batches | lr 0.0000 | ms/batch 90.80 | loss 40.84 | mse 40.84 | mre  0.00 |
scGPT - INFO - | epoch  16 | 700/1949 batches | lr 0.0000 | ms/batch 90.82 | loss 43.68 | mse 43.68 | mre  0.00 |
scGPT - INFO - | epoch  16 | 800/1949 batches | lr 0.0000 | ms/batch 91.39 | loss 41.91 | mse 41.91 | mre  0.00 |
scGPT - INFO - | epoch  16 | 900/1949 batches | lr 0.0000 | ms/batch 90.83 | loss 39.57 | mse 39.57 | mre  0.00 |
scGPT - INFO - | epoch  16 | 1000/1949 batches | lr 0.0000 | ms/batch 90.82 | loss 44.98 | mse 44.98 | mre  0.00 |
scGPT - INFO - | epoch  16 | 1100/1949 batches | lr 0.0000 | ms/batch 90.81 | loss 43.54 | mse 43.54 | mre  0.00 |
scGPT - INFO - | epoch  16 | 1200/1949 batches | lr 0.0000 | ms/batch 90.79 | loss 38.87 | mse 38.87 | mre  0.00 |
scGPT - INFO - | epoch  16 | 1300/1949 batches | lr 0.0000 | ms/batch 90.81 | loss 39.93 | mse 39.93 | mre  0.00 |
scGPT - INFO - | epoch  16 | 1400/1949 batches | lr 0.0000 | ms/batch 90.83 | loss 42.16 | mse 42.16 | mre  0.00 |
scGPT - INFO - | epoch  16 | 1500/1949 batches | lr 0.0000 | ms/batch 90.85 | loss 42.07 | mse 42.07 | mre  0.00 |
scGPT - INFO - | epoch  16 | 1600/1949 batches | lr 0.0000 | ms/batch 90.83 | loss 38.94 | mse 38.94 | mre  0.00 |
scGPT - INFO - | epoch  16 | 1700/1949 batches | lr 0.0000 | ms/batch 90.84 | loss 46.99 | mse 46.99 | mre  0.00 |
scGPT - INFO - | epoch  16 | 1800/1949 batches | lr 0.0000 | ms/batch 91.54 | loss 38.21 | mse 38.21 | mre  0.00 |
scGPT - INFO - | epoch  16 | 1900/1949 batches | lr 0.0000 | ms/batch 90.83 | loss 37.30 | mse 37.30 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  16 | time: 183.92s | valid loss/mse 48.6304 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 47.2601818058287, min_delta 0.0001, val_loss 48.630431454947036
Loss error: -1.3702496491183354
scGPT - INFO - INFO: Early stopping counter 3 of 10
epoch:  16
[[0.        0.        0.        ... 4.634729  0.        0.       ]
 [0.        0.        0.        ... 1.0986123 0.6931472 0.       ]
 [0.        0.        0.6931472 ... 0.6931472 0.        0.       ]
 ...
 [0.        3.218876  0.6931472 ... 1.9459102 0.        0.       ]
 [0.        3.3322046 1.609438  ... 2.0794415 0.        0.       ]
 [0.        4.7706847 3.1354942 ... 2.3025851 0.        0.       ]]
[[0.03079  0.741    0.2495   ... 2.1      0.0979   0.0632  ]
 [0.008766 0.7944   0.2795   ... 1.968    0.0457   0.0755  ]
 [0.01392  0.8193   0.2585   ... 2.115    0.08167  0.0793  ]
 ...
 [0.05072  3.744    2.125    ... 2.408    0.08246  0.10394 ]
 [0.01191  3.305    1.373    ... 2.115    0.10583  0.0476  ]
 [0.04312  4.336    2.617    ... 2.578    0.0898   0.10834 ]]
(433, 42) (433, 42)
By feature:  MSE:  0.5792¬±0.0 MAE:  0.4552¬±0.0 R2:  0.4285¬±0.0 PCC:  0.7328¬±0.0 Cosine Similarity:  0.8031¬±0.0
By sample:  MSE:  0.5792¬±0.0 MAE:  0.4552¬±0.0 R2:  0.1101¬±0.0 PCC:  0.2887¬±0.0 Cosine Similarity:  0.551¬±0.0
scGPT - INFO - By feature: MSE: 0.579200029373169¬±0.0 MAE: 0.4551999866962433¬±0.0 R2: 0.4285¬±0.0 PCC: 0.7328¬±0.0 Cosine Similarity: 0.8031¬±0.0
scGPT - INFO - By sample: MSE: 0.579200029373169¬±0.0 MAE: 0.4551999866962433¬±0.0 R2: 0.1101¬±0.0 PCC: 0.2887¬±0.0 Cosine Similarity: 0.551¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  17
Training model
scGPT - INFO - | epoch  17 | 100/1949 batches | lr 0.0000 | ms/batch 92.01 | loss 40.45 | mse 40.45 | mre  0.00 |
scGPT - INFO - | epoch  17 | 200/1949 batches | lr 0.0000 | ms/batch 90.84 | loss 40.27 | mse 40.27 | mre  0.00 |
scGPT - INFO - | epoch  17 | 300/1949 batches | lr 0.0000 | ms/batch 90.79 | loss 42.28 | mse 42.28 | mre  0.00 |
scGPT - INFO - | epoch  17 | 400/1949 batches | lr 0.0000 | ms/batch 90.83 | loss 42.49 | mse 42.49 | mre  0.00 |
scGPT - INFO - | epoch  17 | 500/1949 batches | lr 0.0000 | ms/batch 90.84 | loss 42.87 | mse 42.87 | mre  0.00 |
scGPT - INFO - | epoch  17 | 600/1949 batches | lr 0.0000 | ms/batch 90.72 | loss 38.79 | mse 38.79 | mre  0.00 |
scGPT - INFO - | epoch  17 | 700/1949 batches | lr 0.0000 | ms/batch 90.81 | loss 44.09 | mse 44.09 | mre  0.00 |
scGPT - INFO - | epoch  17 | 800/1949 batches | lr 0.0000 | ms/batch 90.81 | loss 41.63 | mse 41.63 | mre  0.00 |
scGPT - INFO - | epoch  17 | 900/1949 batches | lr 0.0000 | ms/batch 91.36 | loss 39.41 | mse 39.41 | mre  0.00 |
scGPT - INFO - | epoch  17 | 1000/1949 batches | lr 0.0000 | ms/batch 90.72 | loss 44.57 | mse 44.57 | mre  0.00 |
scGPT - INFO - | epoch  17 | 1100/1949 batches | lr 0.0000 | ms/batch 90.88 | loss 43.40 | mse 43.40 | mre  0.00 |
scGPT - INFO - | epoch  17 | 1200/1949 batches | lr 0.0000 | ms/batch 90.73 | loss 39.06 | mse 39.06 | mre  0.00 |
scGPT - INFO - | epoch  17 | 1300/1949 batches | lr 0.0000 | ms/batch 90.70 | loss 39.93 | mse 39.93 | mre  0.00 |
scGPT - INFO - | epoch  17 | 1400/1949 batches | lr 0.0000 | ms/batch 90.67 | loss 40.79 | mse 40.79 | mre  0.00 |
scGPT - INFO - | epoch  17 | 1500/1949 batches | lr 0.0000 | ms/batch 90.72 | loss 41.61 | mse 41.61 | mre  0.00 |
scGPT - INFO - | epoch  17 | 1600/1949 batches | lr 0.0000 | ms/batch 90.70 | loss 38.69 | mse 38.69 | mre  0.00 |
scGPT - INFO - | epoch  17 | 1700/1949 batches | lr 0.0000 | ms/batch 90.70 | loss 46.12 | mse 46.12 | mre  0.00 |
scGPT - INFO - | epoch  17 | 1800/1949 batches | lr 0.0000 | ms/batch 90.71 | loss 36.98 | mse 36.98 | mre  0.00 |
scGPT - INFO - | epoch  17 | 1900/1949 batches | lr 0.0000 | ms/batch 91.27 | loss 37.66 | mse 37.66 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  17 | time: 183.72s | valid loss/mse 47.9201 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 47.2601818058287, min_delta 0.0001, val_loss 47.92009145062735
Loss error: -0.6599096447986526
scGPT - INFO - INFO: Early stopping counter 4 of 10
epoch:  17
[[0.        0.        0.        ... 4.634729  0.        0.       ]
 [0.        0.        0.        ... 1.0986123 0.6931472 0.       ]
 [0.        0.        0.6931472 ... 0.6931472 0.        0.       ]
 ...
 [0.        3.218876  0.6931472 ... 1.9459102 0.        0.       ]
 [0.        3.3322046 1.609438  ... 2.0794415 0.        0.       ]
 [0.        4.7706847 3.1354942 ... 2.3025851 0.        0.       ]]
[[0.03018 0.777   0.2615  ... 2.102   0.0831  0.0777 ]
 [0.03177 0.7935  0.2915  ... 1.991   0.05414 0.08417]
 [0.03055 0.809   0.2744  ... 2.102   0.07806 0.0837 ]
 ...
 [0.037   3.877   2.324   ... 2.37    0.0866  0.1104 ]
 [0.01971 3.5     1.533   ... 2.117   0.1055  0.07465]
 [0.03613 4.33    2.668   ... 2.518   0.0904  0.1112 ]]
(433, 42) (433, 42)
By feature:  MSE:  0.5707¬±0.0 MAE:  0.4486¬±0.0 R2:  0.4465¬±0.0 PCC:  0.7381¬±0.0 Cosine Similarity:  0.8068¬±0.0
By sample:  MSE:  0.5707¬±0.0 MAE:  0.4486¬±0.0 R2:  0.1175¬±0.0 PCC:  0.2968¬±0.0 Cosine Similarity:  0.554¬±0.0
scGPT - INFO - By feature: MSE: 0.5706999897956848¬±0.0 MAE: 0.44859999418258667¬±0.0 R2: 0.4465¬±0.0 PCC: 0.7381¬±0.0 Cosine Similarity: 0.8068¬±0.0
scGPT - INFO - By sample: MSE: 0.5706999897956848¬±0.0 MAE: 0.44859999418258667¬±0.0 R2: 0.1175¬±0.0 PCC: 0.2968¬±0.0 Cosine Similarity: 0.554¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  18
Training model
scGPT - INFO - | epoch  18 | 100/1949 batches | lr 0.0000 | ms/batch 91.93 | loss 40.50 | mse 40.50 | mre  0.00 |
scGPT - INFO - | epoch  18 | 200/1949 batches | lr 0.0000 | ms/batch 90.71 | loss 38.93 | mse 38.93 | mre  0.00 |
scGPT - INFO - | epoch  18 | 300/1949 batches | lr 0.0000 | ms/batch 90.71 | loss 42.94 | mse 42.94 | mre  0.00 |
scGPT - INFO - | epoch  18 | 400/1949 batches | lr 0.0000 | ms/batch 90.86 | loss 40.56 | mse 40.56 | mre  0.00 |
scGPT - INFO - | epoch  18 | 500/1949 batches | lr 0.0000 | ms/batch 90.74 | loss 43.99 | mse 43.99 | mre  0.00 |
scGPT - INFO - | epoch  18 | 600/1949 batches | lr 0.0000 | ms/batch 90.79 | loss 38.56 | mse 38.56 | mre  0.00 |
scGPT - INFO - | epoch  18 | 700/1949 batches | lr 0.0000 | ms/batch 90.77 | loss 43.47 | mse 43.47 | mre  0.00 |
scGPT - INFO - | epoch  18 | 800/1949 batches | lr 0.0000 | ms/batch 90.75 | loss 42.09 | mse 42.09 | mre  0.00 |
scGPT - INFO - | epoch  18 | 900/1949 batches | lr 0.0000 | ms/batch 91.38 | loss 37.93 | mse 37.93 | mre  0.00 |
scGPT - INFO - | epoch  18 | 1000/1949 batches | lr 0.0000 | ms/batch 90.79 | loss 44.66 | mse 44.66 | mre  0.00 |
scGPT - INFO - | epoch  18 | 1100/1949 batches | lr 0.0000 | ms/batch 91.04 | loss 43.66 | mse 43.66 | mre  0.00 |
scGPT - INFO - | epoch  18 | 1200/1949 batches | lr 0.0000 | ms/batch 90.81 | loss 39.29 | mse 39.29 | mre  0.00 |
scGPT - INFO - | epoch  18 | 1300/1949 batches | lr 0.0000 | ms/batch 90.77 | loss 38.80 | mse 38.80 | mre  0.00 |
scGPT - INFO - | epoch  18 | 1400/1949 batches | lr 0.0000 | ms/batch 90.78 | loss 40.43 | mse 40.43 | mre  0.00 |
scGPT - INFO - | epoch  18 | 1500/1949 batches | lr 0.0000 | ms/batch 90.62 | loss 40.26 | mse 40.26 | mre  0.00 |
scGPT - INFO - | epoch  18 | 1600/1949 batches | lr 0.0000 | ms/batch 90.62 | loss 38.70 | mse 38.70 | mre  0.00 |
scGPT - INFO - | epoch  18 | 1700/1949 batches | lr 0.0000 | ms/batch 90.67 | loss 45.92 | mse 45.92 | mre  0.00 |
scGPT - INFO - | epoch  18 | 1800/1949 batches | lr 0.0000 | ms/batch 90.68 | loss 35.66 | mse 35.66 | mre  0.00 |
scGPT - INFO - | epoch  18 | 1900/1949 batches | lr 0.0000 | ms/batch 91.15 | loss 36.59 | mse 36.59 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  18 | time: 183.69s | valid loss/mse 48.6384 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 47.2601818058287, min_delta 0.0001, val_loss 48.63841398428549
Loss error: -1.3782321784567912
scGPT - INFO - INFO: Early stopping counter 5 of 10
epoch:  18
[[0.        0.        0.        ... 4.634729  0.        0.       ]
 [0.        0.        0.        ... 1.0986123 0.6931472 0.       ]
 [0.        0.        0.6931472 ... 0.6931472 0.        0.       ]
 ...
 [0.        3.218876  0.6931472 ... 1.9459102 0.        0.       ]
 [0.        3.3322046 1.609438  ... 2.0794415 0.        0.       ]
 [0.        4.7706847 3.1354942 ... 2.3025851 0.        0.       ]]
[[0.033   0.745   0.2507  ... 2.111   0.0996  0.0837 ]
 [0.02493 0.7656  0.27    ... 1.866   0.03418 0.0983 ]
 [0.0257  0.801   0.2634  ... 2.037   0.0649  0.1066 ]
 ...
 [0.0629  3.844   2.281   ... 2.39    0.0856  0.1115 ]
 [0.03897 3.566   1.59    ... 2.121   0.1074  0.0747 ]
 [0.054   4.426   2.775   ... 2.594   0.0878  0.1191 ]]
(433, 42) (433, 42)
By feature:  MSE:  0.5792¬±0.0 MAE:  0.4493¬±0.0 R2:  0.4363¬±0.0 PCC:  0.7352¬±0.0 Cosine Similarity:  0.8042¬±0.0
By sample:  MSE:  0.5792¬±0.0 MAE:  0.4493¬±0.0 R2:  0.1105¬±0.0 PCC:  0.2965¬±0.0 Cosine Similarity:  0.5518¬±0.0
scGPT - INFO - By feature: MSE: 0.579200029373169¬±0.0 MAE: 0.44929999113082886¬±0.0 R2: 0.4363¬±0.0 PCC: 0.7352¬±0.0 Cosine Similarity: 0.8042¬±0.0
scGPT - INFO - By sample: MSE: 0.579200029373169¬±0.0 MAE: 0.44929999113082886¬±0.0 R2: 0.1105¬±0.0 PCC: 0.2965¬±0.0 Cosine Similarity: 0.5518¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  19
Training model
scGPT - INFO - | epoch  19 | 100/1949 batches | lr 0.0000 | ms/batch 92.16 | loss 40.03 | mse 40.03 | mre  0.00 |
scGPT - INFO - | epoch  19 | 200/1949 batches | lr 0.0000 | ms/batch 90.68 | loss 37.57 | mse 37.57 | mre  0.00 |
scGPT - INFO - | epoch  19 | 300/1949 batches | lr 0.0000 | ms/batch 90.72 | loss 41.90 | mse 41.90 | mre  0.00 |
scGPT - INFO - | epoch  19 | 400/1949 batches | lr 0.0000 | ms/batch 90.72 | loss 41.23 | mse 41.23 | mre  0.00 |
scGPT - INFO - | epoch  19 | 500/1949 batches | lr 0.0000 | ms/batch 90.67 | loss 43.57 | mse 43.57 | mre  0.00 |
scGPT - INFO - | epoch  19 | 600/1949 batches | lr 0.0000 | ms/batch 90.69 | loss 37.58 | mse 37.58 | mre  0.00 |
scGPT - INFO - | epoch  19 | 700/1949 batches | lr 0.0000 | ms/batch 90.69 | loss 43.54 | mse 43.54 | mre  0.00 |
scGPT - INFO - | epoch  19 | 800/1949 batches | lr 0.0000 | ms/batch 90.73 | loss 40.90 | mse 40.90 | mre  0.00 |
scGPT - INFO - | epoch  19 | 900/1949 batches | lr 0.0000 | ms/batch 90.65 | loss 37.27 | mse 37.27 | mre  0.00 |
scGPT - INFO - | epoch  19 | 1000/1949 batches | lr 0.0000 | ms/batch 91.71 | loss 44.21 | mse 44.21 | mre  0.00 |
scGPT - INFO - | epoch  19 | 1100/1949 batches | lr 0.0000 | ms/batch 90.82 | loss 42.53 | mse 42.53 | mre  0.00 |
scGPT - INFO - | epoch  19 | 1200/1949 batches | lr 0.0000 | ms/batch 90.75 | loss 37.64 | mse 37.64 | mre  0.00 |
scGPT - INFO - | epoch  19 | 1300/1949 batches | lr 0.0000 | ms/batch 90.67 | loss 40.10 | mse 40.10 | mre  0.00 |
scGPT - INFO - | epoch  19 | 1400/1949 batches | lr 0.0000 | ms/batch 90.70 | loss 40.67 | mse 40.67 | mre  0.00 |
scGPT - INFO - | epoch  19 | 1500/1949 batches | lr 0.0000 | ms/batch 90.70 | loss 40.28 | mse 40.28 | mre  0.00 |
scGPT - INFO - | epoch  19 | 1600/1949 batches | lr 0.0000 | ms/batch 90.67 | loss 38.94 | mse 38.94 | mre  0.00 |
scGPT - INFO - | epoch  19 | 1700/1949 batches | lr 0.0000 | ms/batch 90.71 | loss 44.49 | mse 44.49 | mre  0.00 |
scGPT - INFO - | epoch  19 | 1800/1949 batches | lr 0.0000 | ms/batch 90.68 | loss 36.27 | mse 36.27 | mre  0.00 |
scGPT - INFO - | epoch  19 | 1900/1949 batches | lr 0.0000 | ms/batch 90.64 | loss 36.40 | mse 36.40 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  19 | time: 183.68s | valid loss/mse 48.2302 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 47.2601818058287, min_delta 0.0001, val_loss 48.23017262751731
Loss error: -0.9699908216886115
scGPT - INFO - INFO: Early stopping counter 6 of 10
epoch:  19
[[0.        0.        0.        ... 4.634729  0.        0.       ]
 [0.        0.        0.        ... 1.0986123 0.6931472 0.       ]
 [0.        0.        0.6931472 ... 0.6931472 0.        0.       ]
 ...
 [0.        3.218876  0.6931472 ... 1.9459102 0.        0.       ]
 [0.        3.3322046 1.609438  ... 2.0794415 0.        0.       ]
 [0.        4.7706847 3.1354942 ... 2.3025851 0.        0.       ]]
[[0.0691  0.6636  0.2524  ... 2.506   0.0962  0.07556]
 [0.03094 0.7744  0.3027  ... 1.908   0.04724 0.07434]
 [0.02823 0.797   0.2927  ... 2.04    0.0643  0.0821 ]
 ...
 [0.05408 3.777   2.23    ... 2.375   0.08636 0.11304]
 [0.04413 3.498   1.55    ... 2.16    0.0993  0.0736 ]
 [0.0445  4.43    2.793   ... 2.658   0.08856 0.12317]]
(433, 42) (433, 42)
By feature:  MSE:  0.5744¬±0.0 MAE:  0.4469¬±0.0 R2:  0.4464¬±0.0 PCC:  0.7374¬±0.0 Cosine Similarity:  0.8059¬±0.0
By sample:  MSE:  0.5744¬±0.0 MAE:  0.4469¬±0.0 R2:  0.1177¬±0.0 PCC:  0.301¬±0.0 Cosine Similarity:  0.5546¬±0.0
scGPT - INFO - By feature: MSE: 0.574400007724762¬±0.0 MAE: 0.44690001010894775¬±0.0 R2: 0.4464¬±0.0 PCC: 0.7374¬±0.0 Cosine Similarity: 0.8059¬±0.0
scGPT - INFO - By sample: MSE: 0.574400007724762¬±0.0 MAE: 0.44690001010894775¬±0.0 R2: 0.1177¬±0.0 PCC: 0.301¬±0.0 Cosine Similarity: 0.5546¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  20
Training model
scGPT - INFO - | epoch  20 | 100/1949 batches | lr 0.0000 | ms/batch 91.85 | loss 38.87 | mse 38.87 | mre  0.00 |
scGPT - INFO - | epoch  20 | 200/1949 batches | lr 0.0000 | ms/batch 90.80 | loss 37.09 | mse 37.09 | mre  0.00 |
scGPT - INFO - | epoch  20 | 300/1949 batches | lr 0.0000 | ms/batch 90.91 | loss 40.94 | mse 40.94 | mre  0.00 |
scGPT - INFO - | epoch  20 | 400/1949 batches | lr 0.0000 | ms/batch 90.78 | loss 40.14 | mse 40.14 | mre  0.00 |
scGPT - INFO - | epoch  20 | 500/1949 batches | lr 0.0000 | ms/batch 90.85 | loss 42.07 | mse 42.07 | mre  0.00 |
scGPT - INFO - | epoch  20 | 600/1949 batches | lr 0.0000 | ms/batch 90.84 | loss 38.13 | mse 38.13 | mre  0.00 |
scGPT - INFO - | epoch  20 | 700/1949 batches | lr 0.0000 | ms/batch 90.93 | loss 42.62 | mse 42.62 | mre  0.00 |
scGPT - INFO - | epoch  20 | 800/1949 batches | lr 0.0000 | ms/batch 90.80 | loss 40.47 | mse 40.47 | mre  0.00 |
scGPT - INFO - | epoch  20 | 900/1949 batches | lr 0.0000 | ms/batch 90.82 | loss 37.36 | mse 37.36 | mre  0.00 |
scGPT - INFO - | epoch  20 | 1000/1949 batches | lr 0.0000 | ms/batch 91.44 | loss 43.72 | mse 43.72 | mre  0.00 |
scGPT - INFO - | epoch  20 | 1100/1949 batches | lr 0.0000 | ms/batch 90.84 | loss 42.30 | mse 42.30 | mre  0.00 |
scGPT - INFO - | epoch  20 | 1200/1949 batches | lr 0.0000 | ms/batch 90.73 | loss 37.81 | mse 37.81 | mre  0.00 |
scGPT - INFO - | epoch  20 | 1300/1949 batches | lr 0.0000 | ms/batch 90.83 | loss 38.83 | mse 38.83 | mre  0.00 |
scGPT - INFO - | epoch  20 | 1400/1949 batches | lr 0.0000 | ms/batch 90.78 | loss 40.24 | mse 40.24 | mre  0.00 |
scGPT - INFO - | epoch  20 | 1500/1949 batches | lr 0.0000 | ms/batch 90.74 | loss 39.34 | mse 39.34 | mre  0.00 |
scGPT - INFO - | epoch  20 | 1600/1949 batches | lr 0.0000 | ms/batch 90.77 | loss 38.00 | mse 38.00 | mre  0.00 |
scGPT - INFO - | epoch  20 | 1700/1949 batches | lr 0.0000 | ms/batch 90.69 | loss 44.56 | mse 44.56 | mre  0.00 |
scGPT - INFO - | epoch  20 | 1800/1949 batches | lr 0.0000 | ms/batch 90.67 | loss 36.43 | mse 36.43 | mre  0.00 |
scGPT - INFO - | epoch  20 | 1900/1949 batches | lr 0.0000 | ms/batch 90.70 | loss 35.76 | mse 35.76 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  20 | time: 183.72s | valid loss/mse 48.4573 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 47.2601818058287, min_delta 0.0001, val_loss 48.457295130361864
Loss error: -1.1971133245331629
scGPT - INFO - INFO: Early stopping counter 7 of 10
epoch:  20
[[0.        0.        0.        ... 4.634729  0.        0.       ]
 [0.        0.        0.        ... 1.0986123 0.6931472 0.       ]
 [0.        0.        0.6931472 ... 0.6931472 0.        0.       ]
 ...
 [0.        3.218876  0.6931472 ... 1.9459102 0.        0.       ]
 [0.        3.3322046 1.609438  ... 2.0794415 0.        0.       ]
 [0.        4.7706847 3.1354942 ... 2.3025851 0.        0.       ]]
[[0.02982 0.7593  0.272   ... 2.04    0.0869  0.0638 ]
 [0.02243 0.783   0.2961  ... 1.85    0.06885 0.05957]
 [0.02673 0.811   0.29    ... 2.033   0.089   0.07263]
 ...
 [0.05698 3.787   2.207   ... 2.363   0.0969  0.1103 ]
 [0.03824 3.36    1.419   ... 2.045   0.1162  0.06183]
 [0.04413 4.465   2.79    ... 2.668   0.1008  0.1216 ]]
(433, 42) (433, 42)
By feature:  MSE:  0.5771¬±0.0 MAE:  0.4468¬±0.0 R2:  0.4478¬±0.0 PCC:  0.7377¬±0.0 Cosine Similarity:  0.8061¬±0.0
By sample:  MSE:  0.5771¬±0.0 MAE:  0.4468¬±0.0 R2:  0.1151¬±0.0 PCC:  0.299¬±0.0 Cosine Similarity:  0.5537¬±0.0
scGPT - INFO - By feature: MSE: 0.5770999789237976¬±0.0 MAE: 0.44679999351501465¬±0.0 R2: 0.4478¬±0.0 PCC: 0.7377¬±0.0 Cosine Similarity: 0.8061¬±0.0
scGPT - INFO - By sample: MSE: 0.5770999789237976¬±0.0 MAE: 0.44679999351501465¬±0.0 R2: 0.1151¬±0.0 PCC: 0.299¬±0.0 Cosine Similarity: 0.5537¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  21
Training model
scGPT - INFO - | epoch  21 | 100/1949 batches | lr 0.0000 | ms/batch 92.48 | loss 38.14 | mse 38.14 | mre  0.00 |
scGPT - INFO - | epoch  21 | 200/1949 batches | lr 0.0000 | ms/batch 90.77 | loss 37.66 | mse 37.66 | mre  0.00 |
scGPT - INFO - | epoch  21 | 300/1949 batches | lr 0.0000 | ms/batch 90.74 | loss 41.29 | mse 41.29 | mre  0.00 |
scGPT - INFO - | epoch  21 | 400/1949 batches | lr 0.0000 | ms/batch 90.72 | loss 39.69 | mse 39.69 | mre  0.00 |
scGPT - INFO - | epoch  21 | 500/1949 batches | lr 0.0000 | ms/batch 90.67 | loss 41.67 | mse 41.67 | mre  0.00 |
scGPT - INFO - | epoch  21 | 600/1949 batches | lr 0.0000 | ms/batch 90.77 | loss 38.43 | mse 38.43 | mre  0.00 |
scGPT - INFO - | epoch  21 | 700/1949 batches | lr 0.0000 | ms/batch 90.73 | loss 43.19 | mse 43.19 | mre  0.00 |
scGPT - INFO - | epoch  21 | 800/1949 batches | lr 0.0000 | ms/batch 90.69 | loss 40.86 | mse 40.86 | mre  0.00 |
scGPT - INFO - | epoch  21 | 900/1949 batches | lr 0.0000 | ms/batch 90.73 | loss 36.15 | mse 36.15 | mre  0.00 |
scGPT - INFO - | epoch  21 | 1000/1949 batches | lr 0.0000 | ms/batch 90.67 | loss 44.03 | mse 44.03 | mre  0.00 |
scGPT - INFO - | epoch  21 | 1100/1949 batches | lr 0.0000 | ms/batch 91.26 | loss 42.37 | mse 42.37 | mre  0.00 |
scGPT - INFO - | epoch  21 | 1200/1949 batches | lr 0.0000 | ms/batch 90.74 | loss 37.28 | mse 37.28 | mre  0.00 |
scGPT - INFO - | epoch  21 | 1300/1949 batches | lr 0.0000 | ms/batch 90.68 | loss 38.52 | mse 38.52 | mre  0.00 |
scGPT - INFO - | epoch  21 | 1400/1949 batches | lr 0.0000 | ms/batch 90.71 | loss 40.06 | mse 40.06 | mre  0.00 |
scGPT - INFO - | epoch  21 | 1500/1949 batches | lr 0.0000 | ms/batch 90.73 | loss 39.30 | mse 39.30 | mre  0.00 |
scGPT - INFO - | epoch  21 | 1600/1949 batches | lr 0.0000 | ms/batch 90.71 | loss 37.47 | mse 37.47 | mre  0.00 |
scGPT - INFO - | epoch  21 | 1700/1949 batches | lr 0.0000 | ms/batch 90.89 | loss 44.30 | mse 44.30 | mre  0.00 |
scGPT - INFO - | epoch  21 | 1800/1949 batches | lr 0.0000 | ms/batch 90.73 | loss 36.84 | mse 36.84 | mre  0.00 |
scGPT - INFO - | epoch  21 | 1900/1949 batches | lr 0.0000 | ms/batch 90.77 | loss 35.80 | mse 35.80 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  21 | time: 183.68s | valid loss/mse 48.7770 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 47.2601818058287, min_delta 0.0001, val_loss 48.77703548413777
Loss error: -1.5168536783090687
scGPT - INFO - INFO: Early stopping counter 8 of 10
epoch:  21
[[0.        0.        0.        ... 4.634729  0.        0.       ]
 [0.        0.        0.        ... 1.0986123 0.6931472 0.       ]
 [0.        0.        0.6931472 ... 0.6931472 0.        0.       ]
 ...
 [0.        3.218876  0.6931472 ... 1.9459102 0.        0.       ]
 [0.        3.3322046 1.609438  ... 2.0794415 0.        0.       ]
 [0.        4.7706847 3.1354942 ... 2.3025851 0.        0.       ]]
[[0.0896  0.572   0.2449  ... 3.184   0.08746 0.07465]
 [0.02243 0.7676  0.279   ... 1.807   0.05127 0.08716]
 [0.02512 0.7964  0.2637  ... 1.983   0.0782  0.09406]
 ...
 [0.05487 3.725   2.17    ... 2.305   0.0793  0.1128 ]
 [0.0453  3.441   1.4375  ... 2.076   0.094   0.07153]
 [0.0476  4.45    2.781   ... 2.67    0.0822  0.1315 ]]
(433, 42) (433, 42)
By feature:  MSE:  0.5809¬±0.0 MAE:  0.4469¬±0.0 R2:  0.4323¬±0.0 PCC:  0.7358¬±0.0 Cosine Similarity:  0.8042¬±0.0
By sample:  MSE:  0.5809¬±0.0 MAE:  0.4469¬±0.0 R2:  0.1112¬±0.0 PCC:  0.2979¬±0.0 Cosine Similarity:  0.5524¬±0.0
scGPT - INFO - By feature: MSE: 0.5809000134468079¬±0.0 MAE: 0.44690001010894775¬±0.0 R2: 0.4323¬±0.0 PCC: 0.7358¬±0.0 Cosine Similarity: 0.8042¬±0.0
scGPT - INFO - By sample: MSE: 0.5809000134468079¬±0.0 MAE: 0.44690001010894775¬±0.0 R2: 0.1112¬±0.0 PCC: 0.2979¬±0.0 Cosine Similarity: 0.5524¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  22
Training model
scGPT - INFO - | epoch  22 | 100/1949 batches | lr 0.0000 | ms/batch 92.55 | loss 38.55 | mse 38.55 | mre  0.00 |
scGPT - INFO - | epoch  22 | 200/1949 batches | lr 0.0000 | ms/batch 91.06 | loss 37.10 | mse 37.10 | mre  0.00 |
scGPT - INFO - | epoch  22 | 300/1949 batches | lr 0.0000 | ms/batch 91.04 | loss 40.74 | mse 40.74 | mre  0.00 |
scGPT - INFO - | epoch  22 | 400/1949 batches | lr 0.0000 | ms/batch 90.80 | loss 39.82 | mse 39.82 | mre  0.00 |
scGPT - INFO - | epoch  22 | 500/1949 batches | lr 0.0000 | ms/batch 90.79 | loss 41.62 | mse 41.62 | mre  0.00 |
scGPT - INFO - | epoch  22 | 600/1949 batches | lr 0.0000 | ms/batch 90.92 | loss 37.25 | mse 37.25 | mre  0.00 |
scGPT - INFO - | epoch  22 | 700/1949 batches | lr 0.0000 | ms/batch 90.81 | loss 42.11 | mse 42.11 | mre  0.00 |
scGPT - INFO - | epoch  22 | 800/1949 batches | lr 0.0000 | ms/batch 90.81 | loss 38.73 | mse 38.73 | mre  0.00 |
scGPT - INFO - | epoch  22 | 900/1949 batches | lr 0.0000 | ms/batch 91.21 | loss 37.61 | mse 37.61 | mre  0.00 |
scGPT - INFO - | epoch  22 | 1000/1949 batches | lr 0.0000 | ms/batch 90.75 | loss 43.25 | mse 43.25 | mre  0.00 |
scGPT - INFO - | epoch  22 | 1100/1949 batches | lr 0.0000 | ms/batch 91.22 | loss 42.90 | mse 42.90 | mre  0.00 |
scGPT - INFO - | epoch  22 | 1200/1949 batches | lr 0.0000 | ms/batch 90.88 | loss 37.70 | mse 37.70 | mre  0.00 |
scGPT - INFO - | epoch  22 | 1300/1949 batches | lr 0.0000 | ms/batch 90.73 | loss 38.50 | mse 38.50 | mre  0.00 |
scGPT - INFO - | epoch  22 | 1400/1949 batches | lr 0.0000 | ms/batch 90.69 | loss 40.11 | mse 40.11 | mre  0.00 |
scGPT - INFO - | epoch  22 | 1500/1949 batches | lr 0.0000 | ms/batch 90.75 | loss 39.38 | mse 39.38 | mre  0.00 |
scGPT - INFO - | epoch  22 | 1600/1949 batches | lr 0.0000 | ms/batch 90.86 | loss 37.19 | mse 37.19 | mre  0.00 |
scGPT - INFO - | epoch  22 | 1700/1949 batches | lr 0.0000 | ms/batch 90.78 | loss 44.11 | mse 44.11 | mre  0.00 |
scGPT - INFO - | epoch  22 | 1800/1949 batches | lr 0.0000 | ms/batch 91.49 | loss 35.50 | mse 35.50 | mre  0.00 |
scGPT - INFO - | epoch  22 | 1900/1949 batches | lr 0.0000 | ms/batch 90.78 | loss 35.80 | mse 35.80 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  22 | time: 183.94s | valid loss/mse 48.9924 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 47.2601818058287, min_delta 0.0001, val_loss 48.99239962756221
Loss error: -1.7322178217335065
scGPT - INFO - INFO: Early stopping counter 9 of 10
epoch:  22
[[0.        0.        0.        ... 4.634729  0.        0.       ]
 [0.        0.        0.        ... 1.0986123 0.6931472 0.       ]
 [0.        0.        0.6931472 ... 0.6931472 0.        0.       ]
 ...
 [0.        3.218876  0.6931472 ... 1.9459102 0.        0.       ]
 [0.        3.3322046 1.609438  ... 2.0794415 0.        0.       ]
 [0.        4.7706847 3.1354942 ... 2.3025851 0.        0.       ]]
[[0.0795  0.5996  0.2491  ... 2.922   0.08356 0.0746 ]
 [0.0174  0.7466  0.2688  ... 1.752   0.03998 0.0765 ]
 [0.01999 0.772   0.2522  ... 1.909   0.07416 0.0797 ]
 ...
 [0.04782 3.734   2.127   ... 2.326   0.09076 0.1098 ]
 [0.0389  3.473   1.429   ... 2.066   0.10754 0.0688 ]
 [0.03473 4.508   2.81    ... 2.682   0.0943  0.1306 ]]
(433, 42) (433, 42)
By feature:  MSE:  0.5834¬±0.0 MAE:  0.4469¬±0.0 R2:  0.4353¬±0.0 PCC:  0.7345¬±0.0 Cosine Similarity:  0.8036¬±0.0
By sample:  MSE:  0.5834¬±0.0 MAE:  0.4469¬±0.0 R2:  0.1114¬±0.0 PCC:  0.2985¬±0.0 Cosine Similarity:  0.552¬±0.0
scGPT - INFO - By feature: MSE: 0.5834000110626221¬±0.0 MAE: 0.44690001010894775¬±0.0 R2: 0.4353¬±0.0 PCC: 0.7345¬±0.0 Cosine Similarity: 0.8036¬±0.0
scGPT - INFO - By sample: MSE: 0.5834000110626221¬±0.0 MAE: 0.44690001010894775¬±0.0 R2: 0.1114¬±0.0 PCC: 0.2985¬±0.0 Cosine Similarity: 0.552¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  23
Training model
scGPT - INFO - | epoch  23 | 100/1949 batches | lr 0.0000 | ms/batch 91.85 | loss 37.83 | mse 37.83 | mre  0.00 |
scGPT - INFO - | epoch  23 | 200/1949 batches | lr 0.0000 | ms/batch 91.28 | loss 36.34 | mse 36.34 | mre  0.00 |
scGPT - INFO - | epoch  23 | 300/1949 batches | lr 0.0000 | ms/batch 90.67 | loss 39.77 | mse 39.77 | mre  0.00 |
scGPT - INFO - | epoch  23 | 400/1949 batches | lr 0.0000 | ms/batch 90.64 | loss 40.15 | mse 40.15 | mre  0.00 |
scGPT - INFO - | epoch  23 | 500/1949 batches | lr 0.0000 | ms/batch 90.61 | loss 40.95 | mse 40.95 | mre  0.00 |
scGPT - INFO - | epoch  23 | 600/1949 batches | lr 0.0000 | ms/batch 90.66 | loss 36.58 | mse 36.58 | mre  0.00 |
scGPT - INFO - | epoch  23 | 700/1949 batches | lr 0.0000 | ms/batch 90.66 | loss 41.47 | mse 41.47 | mre  0.00 |
scGPT - INFO - | epoch  23 | 800/1949 batches | lr 0.0000 | ms/batch 90.67 | loss 39.53 | mse 39.53 | mre  0.00 |
scGPT - INFO - | epoch  23 | 900/1949 batches | lr 0.0000 | ms/batch 90.65 | loss 36.18 | mse 36.18 | mre  0.00 |
scGPT - INFO - | epoch  23 | 1000/1949 batches | lr 0.0000 | ms/batch 90.75 | loss 43.03 | mse 43.03 | mre  0.00 |
scGPT - INFO - | epoch  23 | 1100/1949 batches | lr 0.0000 | ms/batch 90.62 | loss 40.65 | mse 40.65 | mre  0.00 |
scGPT - INFO - | epoch  23 | 1200/1949 batches | lr 0.0000 | ms/batch 91.16 | loss 37.16 | mse 37.16 | mre  0.00 |
scGPT - INFO - | epoch  23 | 1300/1949 batches | lr 0.0000 | ms/batch 90.65 | loss 37.98 | mse 37.98 | mre  0.00 |
scGPT - INFO - | epoch  23 | 1400/1949 batches | lr 0.0000 | ms/batch 90.63 | loss 39.49 | mse 39.49 | mre  0.00 |
scGPT - INFO - | epoch  23 | 1500/1949 batches | lr 0.0000 | ms/batch 90.73 | loss 39.16 | mse 39.16 | mre  0.00 |
scGPT - INFO - | epoch  23 | 1600/1949 batches | lr 0.0000 | ms/batch 90.64 | loss 35.55 | mse 35.55 | mre  0.00 |
scGPT - INFO - | epoch  23 | 1700/1949 batches | lr 0.0000 | ms/batch 90.64 | loss 42.95 | mse 42.95 | mre  0.00 |
scGPT - INFO - | epoch  23 | 1800/1949 batches | lr 0.0000 | ms/batch 90.70 | loss 34.25 | mse 34.25 | mre  0.00 |
scGPT - INFO - | epoch  23 | 1900/1949 batches | lr 0.0000 | ms/batch 90.90 | loss 35.40 | mse 35.40 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  23 | time: 183.56s | valid loss/mse 49.2981 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 47.2601818058287, min_delta 0.0001, val_loss 49.298126873738774
Loss error: -2.037945067910073
scGPT - INFO - INFO: Early stopping counter 10 of 10
scGPT - INFO - INFO: Early stopping
scGPT - INFO - Best model saved successfully!
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | End test time: 183.66s | 
scGPT - INFO - -----------------------------------------------------------------------------------------
wandb: - 0.003 MB of 0.006 MB uploadedwandb: \ 0.207 MB of 0.210 MB uploadedwandb: | 0.210 MB of 0.210 MB uploadedwandb: / 0.210 MB of 0.210 MB uploadedwandb: 
wandb: Run history:
wandb:                        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb: info/post_freeze_param_count ‚ñÅ
wandb:  info/pre_freeze_param_count ‚ñÅ
wandb:                    train/mse ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ
wandb:                    valid/dab ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                    valid/err ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                    valid/mse ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:            valid/sum_mse_dab ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb:                        epoch 23
wandb: info/post_freeze_param_count 2655275
wandb:  info/pre_freeze_param_count 28343851
wandb:                    train/mse 10.95489
wandb:                    valid/err 0.0
wandb: 
wandb: üöÄ View run gallant-morning-348 at: https://wandb.ai/wang_wandb/scGPT/runs/4ix12v39
wandb: Ô∏è‚ö° View job at https://wandb.ai/wang_wandb/scGPT/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE1NTQ0MTQ2Nw==/version_details/v32
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240423_032107-4ix12v39/logs
