nohup: ignoring input
Global seed set to 0
wandb: Currently logged in as: ywang2542-c (wang_wandb). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/wandb/run-20240423_163207-s6f1yc9c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-pyramid-350
wandb: ‚≠êÔ∏è View project at https://wandb.ai/wang_wandb/scGPT
wandb: üöÄ View run at https://wandb.ai/wang_wandb/scGPT/runs/s6f1yc9c
scPEFT_scGPT
Namespace(dataset='dataset3', lr=0.0001, use_prompt=True, prompt_type='Gene_encoder_prompt', data_name='ms', data_path='/home/wsl20/Projects/RNA2prot/scGPT/data/', space_conf=[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], mlp_conf=[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], epoch=100)
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]}
save to save/dataset3/Gene_encoder_prompt/0.0001
adata_gene (8005, 16508)
adata_protein (8005, 11)
celltype num_types:1
scGPT - INFO - match 15367/16508 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human/best_model.pt, the model args will override the config /home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human/args.json.
scGPT - INFO - Filtering genes by counts ...
scGPT - INFO - Filtering cells by counts ...
scGPT - INFO - Normalizing total counts ...
scGPT - INFO - Binning data ...
scGPT - INFO - train set number of samples: 7204, 
	 feature length: 2001
scGPT - INFO - valid set number of samples: 801, 
	 feature length: 2001
except!!!! only load params that are in the model and match the size!!!!!
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params decoder.fc.0.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params decoder.fc.0.bias with shape torch.Size([512])
scGPT - INFO - Loading params decoder.fc.2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params decoder.fc.2.bias with shape torch.Size([512])
Total Pre freeze Params 29650444
All para.requires_grad:  False, Freeze!
Adapter in name: transformer_encoder.layers.0.Space_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.0.Space_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.0.Space_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.0.Space_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.0.Space_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.0.Space_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.0.Space_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.0.Space_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.0.MLP_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.0.MLP_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.0.MLP_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.0.MLP_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.0.MLP_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.0.MLP_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.0.MLP_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.0.MLP_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.1.Space_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.1.Space_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.1.Space_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.1.Space_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.1.Space_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.1.Space_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.1.Space_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.1.Space_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.1.MLP_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.1.MLP_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.1.MLP_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.1.MLP_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.1.MLP_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.1.MLP_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.1.MLP_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.1.MLP_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.2.Space_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.2.Space_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.2.Space_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.2.Space_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.2.Space_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.2.Space_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.2.Space_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.2.Space_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.2.MLP_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.2.MLP_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.2.MLP_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.2.MLP_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.2.MLP_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.2.MLP_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.2.MLP_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.2.MLP_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.3.Space_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.3.Space_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.3.Space_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.3.Space_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.3.Space_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.3.Space_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.3.Space_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.3.Space_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.3.MLP_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.3.MLP_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.3.MLP_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.3.MLP_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.3.MLP_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.3.MLP_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.3.MLP_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.3.MLP_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.4.Space_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.4.Space_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.4.Space_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.4.Space_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.4.Space_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.4.Space_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.4.Space_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.4.Space_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.4.MLP_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.4.MLP_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.4.MLP_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.4.MLP_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.4.MLP_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.4.MLP_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.4.MLP_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.4.MLP_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.5.Space_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.5.Space_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.5.Space_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.5.Space_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.5.Space_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.5.Space_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.5.Space_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.5.Space_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.5.MLP_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.5.MLP_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.5.MLP_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.5.MLP_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.5.MLP_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.5.MLP_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.5.MLP_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.5.MLP_Adapter.D_fc2.bias
cls in name: cls_decoder._decoder.0.weight
scGPT - INFO - cls in name: cls_decoder._decoder.0.weight
cls in name: cls_decoder._decoder.0.bias
scGPT - INFO - cls in name: cls_decoder._decoder.0.bias
cls in name: cls_decoder._decoder.2.weight
scGPT - INFO - cls in name: cls_decoder._decoder.2.weight
cls in name: cls_decoder._decoder.2.bias
scGPT - INFO - cls in name: cls_decoder._decoder.2.bias
cls in name: cls_decoder._decoder.3.weight
scGPT - INFO - cls in name: cls_decoder._decoder.3.weight
cls in name: cls_decoder._decoder.3.bias
scGPT - INFO - cls in name: cls_decoder._decoder.3.bias
cls in name: cls_decoder._decoder.5.weight
scGPT - INFO - cls in name: cls_decoder._decoder.5.weight
cls in name: cls_decoder._decoder.5.bias
scGPT - INFO - cls in name: cls_decoder._decoder.5.bias
cls in name: cls_decoder.out_layer.weight
scGPT - INFO - cls in name: cls_decoder.out_layer.weight
cls in name: cls_decoder.out_layer.bias
scGPT - INFO - cls in name: cls_decoder.out_layer.bias
Total Post0 freeze Params 2639372
total:29650444
trainable:2639372
Total Post freeze Params 2639372
scGPT - INFO - Total Pre freeze Params 29650444
scGPT - INFO - Total Post freeze Params 2639372
Epoch:  1
Training model
scGPT - INFO - | epoch   1 | 100/3602 batches | lr 0.0001 | ms/batch 97.82 | loss 114.31 | mse 114.31 | mre  0.00 |
scGPT - INFO - | epoch   1 | 200/3602 batches | lr 0.0001 | ms/batch 89.62 | loss 22.87 | mse 22.87 | mre  0.00 |
scGPT - INFO - | epoch   1 | 300/3602 batches | lr 0.0001 | ms/batch 89.82 | loss 19.91 | mse 19.91 | mre  0.00 |
scGPT - INFO - | epoch   1 | 400/3602 batches | lr 0.0001 | ms/batch 90.09 | loss 21.04 | mse 21.04 | mre  0.00 |
scGPT - INFO - | epoch   1 | 500/3602 batches | lr 0.0001 | ms/batch 90.11 | loss 20.03 | mse 20.03 | mre  0.00 |
scGPT - INFO - | epoch   1 | 600/3602 batches | lr 0.0001 | ms/batch 90.31 | loss 20.72 | mse 20.72 | mre  0.00 |
scGPT - INFO - | epoch   1 | 700/3602 batches | lr 0.0001 | ms/batch 90.40 | loss 22.19 | mse 22.19 | mre  0.00 |
wandb: Network error (ReadTimeout), entering retry loop.
scGPT - INFO - | epoch   1 | 800/3602 batches | lr 0.0001 | ms/batch 90.40 | loss 22.05 | mse 22.05 | mre  0.00 |
scGPT - INFO - | epoch   1 | 900/3602 batches | lr 0.0001 | ms/batch 90.41 | loss 21.60 | mse 21.60 | mre  0.00 |
scGPT - INFO - | epoch   1 | 1000/3602 batches | lr 0.0001 | ms/batch 91.53 | loss 21.45 | mse 21.45 | mre  0.00 |
scGPT - INFO - | epoch   1 | 1100/3602 batches | lr 0.0001 | ms/batch 90.51 | loss 22.10 | mse 22.10 | mre  0.00 |
scGPT - INFO - | epoch   1 | 1200/3602 batches | lr 0.0001 | ms/batch 90.80 | loss 21.68 | mse 21.68 | mre  0.00 |
scGPT - INFO - | epoch   1 | 1300/3602 batches | lr 0.0001 | ms/batch 90.60 | loss 21.73 | mse 21.73 | mre  0.00 |
scGPT - INFO - | epoch   1 | 1400/3602 batches | lr 0.0001 | ms/batch 90.64 | loss 23.97 | mse 23.97 | mre  0.00 |
scGPT - INFO - | epoch   1 | 1500/3602 batches | lr 0.0001 | ms/batch 90.68 | loss 21.34 | mse 21.34 | mre  0.00 |
scGPT - INFO - | epoch   1 | 1600/3602 batches | lr 0.0001 | ms/batch 90.89 | loss 19.90 | mse 19.90 | mre  0.00 |
scGPT - INFO - | epoch   1 | 1700/3602 batches | lr 0.0001 | ms/batch 91.19 | loss 22.78 | mse 22.78 | mre  0.00 |
scGPT - INFO - | epoch   1 | 1800/3602 batches | lr 0.0001 | ms/batch 91.32 | loss 19.75 | mse 19.75 | mre  0.00 |
scGPT - INFO - | epoch   1 | 1900/3602 batches | lr 0.0001 | ms/batch 91.35 | loss 23.57 | mse 23.57 | mre  0.00 |
scGPT - INFO - | epoch   1 | 2000/3602 batches | lr 0.0001 | ms/batch 91.63 | loss 21.54 | mse 21.54 | mre  0.00 |
scGPT - INFO - | epoch   1 | 2100/3602 batches | lr 0.0001 | ms/batch 90.98 | loss 22.11 | mse 22.11 | mre  0.00 |
scGPT - INFO - | epoch   1 | 2200/3602 batches | lr 0.0001 | ms/batch 91.14 | loss 21.76 | mse 21.76 | mre  0.00 |
scGPT - INFO - | epoch   1 | 2300/3602 batches | lr 0.0001 | ms/batch 90.97 | loss 21.02 | mse 21.02 | mre  0.00 |
scGPT - INFO - | epoch   1 | 2400/3602 batches | lr 0.0001 | ms/batch 90.89 | loss 21.13 | mse 21.13 | mre  0.00 |
scGPT - INFO - | epoch   1 | 2500/3602 batches | lr 0.0001 | ms/batch 90.88 | loss 20.88 | mse 20.88 | mre  0.00 |
scGPT - INFO - | epoch   1 | 2600/3602 batches | lr 0.0001 | ms/batch 90.85 | loss 22.14 | mse 22.14 | mre  0.00 |
scGPT - INFO - | epoch   1 | 2700/3602 batches | lr 0.0001 | ms/batch 90.76 | loss 21.82 | mse 21.82 | mre  0.00 |
scGPT - INFO - | epoch   1 | 2800/3602 batches | lr 0.0001 | ms/batch 90.80 | loss 20.82 | mse 20.82 | mre  0.00 |
scGPT - INFO - | epoch   1 | 2900/3602 batches | lr 0.0001 | ms/batch 90.86 | loss 20.75 | mse 20.75 | mre  0.00 |
scGPT - INFO - | epoch   1 | 3000/3602 batches | lr 0.0001 | ms/batch 91.44 | loss 23.11 | mse 23.11 | mre  0.00 |
scGPT - INFO - | epoch   1 | 3100/3602 batches | lr 0.0001 | ms/batch 90.63 | loss 20.58 | mse 20.58 | mre  0.00 |
scGPT - INFO - | epoch   1 | 3200/3602 batches | lr 0.0001 | ms/batch 90.47 | loss 21.25 | mse 21.25 | mre  0.00 |
scGPT - INFO - | epoch   1 | 3300/3602 batches | lr 0.0001 | ms/batch 90.51 | loss 20.53 | mse 20.53 | mre  0.00 |
scGPT - INFO - | epoch   1 | 3400/3602 batches | lr 0.0001 | ms/batch 90.53 | loss 20.99 | mse 20.99 | mre  0.00 |
scGPT - INFO - | epoch   1 | 3500/3602 batches | lr 0.0001 | ms/batch 90.54 | loss 18.76 | mse 18.76 | mre  0.00 |
scGPT - INFO - | epoch   1 | 3600/3602 batches | lr 0.0001 | ms/batch 90.50 | loss 24.33 | mse 24.33 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 339.75s | valid loss/mse 20.5204 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 20.5204
epoch:  1
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[5.6   3.861 3.785 ... 3.703 3.94  3.844]
 [5.6   3.863 3.787 ... 3.703 3.94  3.844]
 [5.6   3.861 3.785 ... 3.703 3.94  3.844]
 ...
 [5.6   3.861 3.785 ... 3.701 3.941 3.842]
 [5.6   3.863 3.787 ... 3.703 3.94  3.846]
 [5.6   3.863 3.787 ... 3.703 3.941 3.846]]
(801, 11) (801, 11)
By feature:  MSE:  0.9329¬±0.0 MAE:  0.7074¬±0.0 R2:  -0.1892¬±0.0 PCC:  0.5694¬±0.0 Cosine Similarity:  0.9808¬±0.0
By sample:  MSE:  0.9329¬±0.0 MAE:  0.7074¬±0.0 R2:  -0.0303¬±0.0 PCC:  0.0568¬±0.0 Cosine Similarity:  0.9775¬±0.0
scGPT - INFO - By feature: MSE: 0.9329000115394592¬±0.0 MAE: 0.7074000239372253¬±0.0 R2: -0.1892¬±0.0 PCC: 0.5694¬±0.0 Cosine Similarity: 0.9808¬±0.0
scGPT - INFO - By sample: MSE: 0.9329000115394592¬±0.0 MAE: 0.7074000239372253¬±0.0 R2: -0.0303¬±0.0 PCC: 0.0568¬±0.0 Cosine Similarity: 0.9775¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  2
Training model
scGPT - INFO - | epoch   2 | 100/3602 batches | lr 0.0001 | ms/batch 91.73 | loss 22.11 | mse 22.11 | mre  0.00 |
scGPT - INFO - | epoch   2 | 200/3602 batches | lr 0.0001 | ms/batch 90.48 | loss 22.29 | mse 22.29 | mre  0.00 |
scGPT - INFO - | epoch   2 | 300/3602 batches | lr 0.0001 | ms/batch 90.50 | loss 19.26 | mse 19.26 | mre  0.00 |
scGPT - INFO - | epoch   2 | 400/3602 batches | lr 0.0001 | ms/batch 91.02 | loss 20.49 | mse 20.49 | mre  0.00 |
scGPT - INFO - | epoch   2 | 500/3602 batches | lr 0.0001 | ms/batch 90.45 | loss 19.38 | mse 19.38 | mre  0.00 |
scGPT - INFO - | epoch   2 | 600/3602 batches | lr 0.0001 | ms/batch 90.50 | loss 19.90 | mse 19.90 | mre  0.00 |
scGPT - INFO - | epoch   2 | 700/3602 batches | lr 0.0001 | ms/batch 90.50 | loss 21.79 | mse 21.79 | mre  0.00 |
scGPT - INFO - | epoch   2 | 800/3602 batches | lr 0.0001 | ms/batch 90.64 | loss 21.63 | mse 21.63 | mre  0.00 |
scGPT - INFO - | epoch   2 | 900/3602 batches | lr 0.0001 | ms/batch 90.51 | loss 21.06 | mse 21.06 | mre  0.00 |
scGPT - INFO - | epoch   2 | 1000/3602 batches | lr 0.0001 | ms/batch 90.61 | loss 20.98 | mse 20.98 | mre  0.00 |
scGPT - INFO - | epoch   2 | 1100/3602 batches | lr 0.0001 | ms/batch 90.57 | loss 21.54 | mse 21.54 | mre  0.00 |
scGPT - INFO - | epoch   2 | 1200/3602 batches | lr 0.0001 | ms/batch 90.62 | loss 21.41 | mse 21.41 | mre  0.00 |
scGPT - INFO - | epoch   2 | 1300/3602 batches | lr 0.0001 | ms/batch 90.64 | loss 21.42 | mse 21.42 | mre  0.00 |
scGPT - INFO - | epoch   2 | 1400/3602 batches | lr 0.0001 | ms/batch 91.15 | loss 24.00 | mse 24.00 | mre  0.00 |
scGPT - INFO - | epoch   2 | 1500/3602 batches | lr 0.0001 | ms/batch 90.72 | loss 21.04 | mse 21.04 | mre  0.00 |
scGPT - INFO - | epoch   2 | 1600/3602 batches | lr 0.0001 | ms/batch 90.55 | loss 19.49 | mse 19.49 | mre  0.00 |
scGPT - INFO - | epoch   2 | 1700/3602 batches | lr 0.0001 | ms/batch 90.53 | loss 22.10 | mse 22.10 | mre  0.00 |
scGPT - INFO - | epoch   2 | 1800/3602 batches | lr 0.0001 | ms/batch 90.59 | loss 19.50 | mse 19.50 | mre  0.00 |
scGPT - INFO - | epoch   2 | 1900/3602 batches | lr 0.0001 | ms/batch 90.59 | loss 23.25 | mse 23.25 | mre  0.00 |
scGPT - INFO - | epoch   2 | 2000/3602 batches | lr 0.0001 | ms/batch 90.71 | loss 21.13 | mse 21.13 | mre  0.00 |
scGPT - INFO - | epoch   2 | 2100/3602 batches | lr 0.0001 | ms/batch 90.64 | loss 21.79 | mse 21.79 | mre  0.00 |
scGPT - INFO - | epoch   2 | 2200/3602 batches | lr 0.0001 | ms/batch 91.45 | loss 21.39 | mse 21.39 | mre  0.00 |
scGPT - INFO - | epoch   2 | 2300/3602 batches | lr 0.0001 | ms/batch 92.84 | loss 20.59 | mse 20.59 | mre  0.00 |
scGPT - INFO - | epoch   2 | 2400/3602 batches | lr 0.0001 | ms/batch 92.86 | loss 20.79 | mse 20.79 | mre  0.00 |
scGPT - INFO - | epoch   2 | 2500/3602 batches | lr 0.0001 | ms/batch 91.41 | loss 20.35 | mse 20.35 | mre  0.00 |
scGPT - INFO - | epoch   2 | 2600/3602 batches | lr 0.0001 | ms/batch 90.91 | loss 22.04 | mse 22.04 | mre  0.00 |
scGPT - INFO - | epoch   2 | 2700/3602 batches | lr 0.0001 | ms/batch 90.73 | loss 21.57 | mse 21.57 | mre  0.00 |
scGPT - INFO - | epoch   2 | 2800/3602 batches | lr 0.0001 | ms/batch 90.94 | loss 20.50 | mse 20.50 | mre  0.00 |
scGPT - INFO - | epoch   2 | 2900/3602 batches | lr 0.0001 | ms/batch 90.74 | loss 20.36 | mse 20.36 | mre  0.00 |
scGPT - INFO - | epoch   2 | 3000/3602 batches | lr 0.0001 | ms/batch 91.65 | loss 22.86 | mse 22.86 | mre  0.00 |
scGPT - INFO - | epoch   2 | 3100/3602 batches | lr 0.0001 | ms/batch 91.97 | loss 20.29 | mse 20.29 | mre  0.00 |
scGPT - INFO - | epoch   2 | 3200/3602 batches | lr 0.0001 | ms/batch 91.81 | loss 21.01 | mse 21.01 | mre  0.00 |
scGPT - INFO - | epoch   2 | 3300/3602 batches | lr 0.0001 | ms/batch 90.65 | loss 20.33 | mse 20.33 | mre  0.00 |
scGPT - INFO - | epoch   2 | 3400/3602 batches | lr 0.0001 | ms/batch 91.20 | loss 20.67 | mse 20.67 | mre  0.00 |
scGPT - INFO - | epoch   2 | 3500/3602 batches | lr 0.0001 | ms/batch 90.52 | loss 18.48 | mse 18.48 | mre  0.00 |
scGPT - INFO - | epoch   2 | 3600/3602 batches | lr 0.0001 | ms/batch 90.65 | loss 23.82 | mse 23.82 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 339.94s | valid loss/mse 20.4049 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 20.4049
best_loss: 20.520355883906696, min_delta 0.0001, val_loss 20.404878672886728
Loss error: 0.115477211019968
epoch:  2
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[5.555 3.818 3.736 ... 3.713 3.883 3.746]
 [5.56  3.82  3.738 ... 3.715 3.885 3.75 ]
 [5.555 3.816 3.736 ... 3.71  3.88  3.746]
 ...
 [5.56  3.818 3.74  ... 3.713 3.885 3.748]
 [5.56  3.82  3.738 ... 3.713 3.885 3.748]
 [5.566 3.826 3.744 ... 3.719 3.89  3.754]]
(801, 11) (801, 11)
By feature:  MSE:  0.9277¬±0.0 MAE:  0.6948¬±0.0 R2:  -0.1864¬±0.0 PCC:  0.5766¬±0.0 Cosine Similarity:  0.9809¬±0.0
By sample:  MSE:  0.9277¬±0.0 MAE:  0.6948¬±0.0 R2:  -0.0144¬±0.0 PCC:  0.0667¬±0.0 Cosine Similarity:  0.9775¬±0.0
scGPT - INFO - By feature: MSE: 0.9276999831199646¬±0.0 MAE: 0.6948000192642212¬±0.0 R2: -0.1864¬±0.0 PCC: 0.5766¬±0.0 Cosine Similarity: 0.9809¬±0.0
scGPT - INFO - By sample: MSE: 0.9276999831199646¬±0.0 MAE: 0.6948000192642212¬±0.0 R2: -0.0144¬±0.0 PCC: 0.0667¬±0.0 Cosine Similarity: 0.9775¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  3
Training model
scGPT - INFO - | epoch   3 | 100/3602 batches | lr 0.0001 | ms/batch 91.79 | loss 21.92 | mse 21.92 | mre  0.00 |
scGPT - INFO - | epoch   3 | 200/3602 batches | lr 0.0001 | ms/batch 90.65 | loss 21.91 | mse 21.91 | mre  0.00 |
scGPT - INFO - | epoch   3 | 300/3602 batches | lr 0.0001 | ms/batch 90.59 | loss 19.05 | mse 19.05 | mre  0.00 |
scGPT - INFO - | epoch   3 | 400/3602 batches | lr 0.0001 | ms/batch 90.61 | loss 20.26 | mse 20.26 | mre  0.00 |
scGPT - INFO - | epoch   3 | 500/3602 batches | lr 0.0001 | ms/batch 90.62 | loss 19.14 | mse 19.14 | mre  0.00 |
scGPT - INFO - | epoch   3 | 600/3602 batches | lr 0.0001 | ms/batch 90.68 | loss 19.47 | mse 19.47 | mre  0.00 |
scGPT - INFO - | epoch   3 | 700/3602 batches | lr 0.0001 | ms/batch 90.51 | loss 21.45 | mse 21.45 | mre  0.00 |
scGPT - INFO - | epoch   3 | 800/3602 batches | lr 0.0001 | ms/batch 91.08 | loss 21.50 | mse 21.50 | mre  0.00 |
scGPT - INFO - | epoch   3 | 900/3602 batches | lr 0.0001 | ms/batch 90.52 | loss 20.87 | mse 20.87 | mre  0.00 |
scGPT - INFO - | epoch   3 | 1000/3602 batches | lr 0.0001 | ms/batch 90.51 | loss 20.87 | mse 20.87 | mre  0.00 |
scGPT - INFO - | epoch   3 | 1100/3602 batches | lr 0.0001 | ms/batch 90.51 | loss 21.35 | mse 21.35 | mre  0.00 |
scGPT - INFO - | epoch   3 | 1200/3602 batches | lr 0.0001 | ms/batch 90.56 | loss 21.15 | mse 21.15 | mre  0.00 |
scGPT - INFO - | epoch   3 | 1300/3602 batches | lr 0.0001 | ms/batch 90.55 | loss 21.27 | mse 21.27 | mre  0.00 |
scGPT - INFO - | epoch   3 | 1400/3602 batches | lr 0.0001 | ms/batch 90.52 | loss 23.70 | mse 23.70 | mre  0.00 |
scGPT - INFO - | epoch   3 | 1500/3602 batches | lr 0.0001 | ms/batch 90.58 | loss 20.78 | mse 20.78 | mre  0.00 |
scGPT - INFO - | epoch   3 | 1600/3602 batches | lr 0.0001 | ms/batch 90.55 | loss 19.39 | mse 19.39 | mre  0.00 |
scGPT - INFO - | epoch   3 | 1700/3602 batches | lr 0.0001 | ms/batch 90.51 | loss 21.94 | mse 21.94 | mre  0.00 |
scGPT - INFO - | epoch   3 | 1800/3602 batches | lr 0.0001 | ms/batch 91.43 | loss 19.24 | mse 19.24 | mre  0.00 |
scGPT - INFO - | epoch   3 | 1900/3602 batches | lr 0.0001 | ms/batch 90.85 | loss 23.04 | mse 23.04 | mre  0.00 |
scGPT - INFO - | epoch   3 | 2000/3602 batches | lr 0.0001 | ms/batch 90.91 | loss 21.06 | mse 21.06 | mre  0.00 |
scGPT - INFO - | epoch   3 | 2100/3602 batches | lr 0.0001 | ms/batch 92.06 | loss 21.63 | mse 21.63 | mre  0.00 |
scGPT - INFO - | epoch   3 | 2200/3602 batches | lr 0.0001 | ms/batch 91.95 | loss 21.18 | mse 21.18 | mre  0.00 |
scGPT - INFO - | epoch   3 | 2300/3602 batches | lr 0.0001 | ms/batch 92.14 | loss 20.42 | mse 20.42 | mre  0.00 |
scGPT - INFO - | epoch   3 | 2400/3602 batches | lr 0.0001 | ms/batch 91.09 | loss 20.70 | mse 20.70 | mre  0.00 |
scGPT - INFO - | epoch   3 | 2500/3602 batches | lr 0.0001 | ms/batch 90.80 | loss 20.20 | mse 20.20 | mre  0.00 |
scGPT - INFO - | epoch   3 | 2600/3602 batches | lr 0.0001 | ms/batch 90.77 | loss 21.82 | mse 21.82 | mre  0.00 |
scGPT - INFO - | epoch   3 | 2700/3602 batches | lr 0.0001 | ms/batch 90.83 | loss 21.33 | mse 21.33 | mre  0.00 |
scGPT - INFO - | epoch   3 | 2800/3602 batches | lr 0.0001 | ms/batch 91.27 | loss 20.42 | mse 20.42 | mre  0.00 |
scGPT - INFO - | epoch   3 | 2900/3602 batches | lr 0.0001 | ms/batch 90.79 | loss 20.20 | mse 20.20 | mre  0.00 |
scGPT - INFO - | epoch   3 | 3000/3602 batches | lr 0.0001 | ms/batch 90.88 | loss 22.59 | mse 22.59 | mre  0.00 |
scGPT - INFO - | epoch   3 | 3100/3602 batches | lr 0.0001 | ms/batch 90.92 | loss 20.08 | mse 20.08 | mre  0.00 |
scGPT - INFO - | epoch   3 | 3200/3602 batches | lr 0.0001 | ms/batch 90.77 | loss 20.93 | mse 20.93 | mre  0.00 |
scGPT - INFO - | epoch   3 | 3300/3602 batches | lr 0.0001 | ms/batch 90.70 | loss 20.17 | mse 20.17 | mre  0.00 |
scGPT - INFO - | epoch   3 | 3400/3602 batches | lr 0.0001 | ms/batch 90.76 | loss 20.47 | mse 20.47 | mre  0.00 |
scGPT - INFO - | epoch   3 | 3500/3602 batches | lr 0.0001 | ms/batch 90.76 | loss 18.46 | mse 18.46 | mre  0.00 |
scGPT - INFO - | epoch   3 | 3600/3602 batches | lr 0.0001 | ms/batch 90.78 | loss 23.47 | mse 23.47 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 339.68s | valid loss/mse 20.3285 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 20.3285
best_loss: 20.404878672886728, min_delta 0.0001, val_loss 20.328545244147865
Loss error: 0.07633342873886306
epoch:  3
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[5.543 3.875 3.783 ... 3.758 3.88  3.732]
 [5.547 3.879 3.787 ... 3.76  3.883 3.734]
 [5.543 3.875 3.783 ... 3.758 3.88  3.732]
 ...
 [5.547 3.877 3.785 ... 3.76  3.883 3.734]
 [5.547 3.877 3.787 ... 3.76  3.885 3.734]
 [5.555 3.883 3.791 ... 3.764 3.889 3.74 ]]
(801, 11) (801, 11)
By feature:  MSE:  0.9242¬±0.0 MAE:  0.6947¬±0.0 R2:  -0.1827¬±0.0 PCC:  0.5804¬±0.0 Cosine Similarity:  0.981¬±0.0
By sample:  MSE:  0.9242¬±0.0 MAE:  0.6947¬±0.0 R2:  -0.0116¬±0.0 PCC:  0.0608¬±0.0 Cosine Similarity:  0.9775¬±0.0
scGPT - INFO - By feature: MSE: 0.9241999983787537¬±0.0 MAE: 0.6947000026702881¬±0.0 R2: -0.1827¬±0.0 PCC: 0.5804¬±0.0 Cosine Similarity: 0.981¬±0.0
scGPT - INFO - By sample: MSE: 0.9241999983787537¬±0.0 MAE: 0.6947000026702881¬±0.0 R2: -0.0116¬±0.0 PCC: 0.0608¬±0.0 Cosine Similarity: 0.9775¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  4
Training model
scGPT - INFO - | epoch   4 | 100/3602 batches | lr 0.0001 | ms/batch 91.80 | loss 21.81 | mse 21.81 | mre  0.00 |
scGPT - INFO - | epoch   4 | 200/3602 batches | lr 0.0001 | ms/batch 91.14 | loss 21.80 | mse 21.80 | mre  0.00 |
scGPT - INFO - | epoch   4 | 300/3602 batches | lr 0.0001 | ms/batch 90.58 | loss 18.85 | mse 18.85 | mre  0.00 |
scGPT - INFO - | epoch   4 | 400/3602 batches | lr 0.0001 | ms/batch 90.64 | loss 20.15 | mse 20.15 | mre  0.00 |
scGPT - INFO - | epoch   4 | 500/3602 batches | lr 0.0001 | ms/batch 90.68 | loss 19.13 | mse 19.13 | mre  0.00 |
scGPT - INFO - | epoch   4 | 600/3602 batches | lr 0.0001 | ms/batch 90.58 | loss 19.37 | mse 19.37 | mre  0.00 |
scGPT - INFO - | epoch   4 | 700/3602 batches | lr 0.0001 | ms/batch 90.57 | loss 21.27 | mse 21.27 | mre  0.00 |
scGPT - INFO - | epoch   4 | 800/3602 batches | lr 0.0001 | ms/batch 90.56 | loss 21.31 | mse 21.31 | mre  0.00 |
scGPT - INFO - | epoch   4 | 900/3602 batches | lr 0.0001 | ms/batch 90.53 | loss 20.64 | mse 20.64 | mre  0.00 |
scGPT - INFO - | epoch   4 | 1000/3602 batches | lr 0.0001 | ms/batch 90.59 | loss 20.84 | mse 20.84 | mre  0.00 |
scGPT - INFO - | epoch   4 | 1100/3602 batches | lr 0.0001 | ms/batch 90.64 | loss 21.19 | mse 21.19 | mre  0.00 |
scGPT - INFO - | epoch   4 | 1200/3602 batches | lr 0.0001 | ms/batch 91.17 | loss 21.04 | mse 21.04 | mre  0.00 |
scGPT - INFO - | epoch   4 | 1300/3602 batches | lr 0.0001 | ms/batch 90.63 | loss 21.39 | mse 21.39 | mre  0.00 |
scGPT - INFO - | epoch   4 | 1400/3602 batches | lr 0.0001 | ms/batch 90.66 | loss 23.56 | mse 23.56 | mre  0.00 |
scGPT - INFO - | epoch   4 | 1500/3602 batches | lr 0.0001 | ms/batch 90.63 | loss 20.75 | mse 20.75 | mre  0.00 |
scGPT - INFO - | epoch   4 | 1600/3602 batches | lr 0.0001 | ms/batch 90.64 | loss 19.30 | mse 19.30 | mre  0.00 |
scGPT - INFO - | epoch   4 | 1700/3602 batches | lr 0.0001 | ms/batch 90.69 | loss 21.82 | mse 21.82 | mre  0.00 |
scGPT - INFO - | epoch   4 | 1800/3602 batches | lr 0.0001 | ms/batch 90.66 | loss 19.20 | mse 19.20 | mre  0.00 |
scGPT - INFO - | epoch   4 | 1900/3602 batches | lr 0.0001 | ms/batch 90.70 | loss 22.99 | mse 22.99 | mre  0.00 |
scGPT - INFO - | epoch   4 | 2000/3602 batches | lr 0.0001 | ms/batch 90.65 | loss 20.95 | mse 20.95 | mre  0.00 |
scGPT - INFO - | epoch   4 | 2100/3602 batches | lr 0.0001 | ms/batch 90.77 | loss 21.64 | mse 21.64 | mre  0.00 |
scGPT - INFO - | epoch   4 | 2200/3602 batches | lr 0.0001 | ms/batch 91.26 | loss 21.05 | mse 21.05 | mre  0.00 |
scGPT - INFO - | epoch   4 | 2300/3602 batches | lr 0.0001 | ms/batch 90.65 | loss 20.34 | mse 20.34 | mre  0.00 |
scGPT - INFO - | epoch   4 | 2400/3602 batches | lr 0.0001 | ms/batch 90.67 | loss 20.62 | mse 20.62 | mre  0.00 |
scGPT - INFO - | epoch   4 | 2500/3602 batches | lr 0.0001 | ms/batch 90.68 | loss 20.12 | mse 20.12 | mre  0.00 |
scGPT - INFO - | epoch   4 | 2600/3602 batches | lr 0.0001 | ms/batch 90.77 | loss 21.69 | mse 21.69 | mre  0.00 |
scGPT - INFO - | epoch   4 | 2700/3602 batches | lr 0.0001 | ms/batch 90.68 | loss 21.28 | mse 21.28 | mre  0.00 |
scGPT - INFO - | epoch   4 | 2800/3602 batches | lr 0.0001 | ms/batch 90.71 | loss 20.23 | mse 20.23 | mre  0.00 |
scGPT - INFO - | epoch   4 | 2900/3602 batches | lr 0.0001 | ms/batch 90.79 | loss 20.17 | mse 20.17 | mre  0.00 |
scGPT - INFO - | epoch   4 | 3000/3602 batches | lr 0.0001 | ms/batch 90.72 | loss 22.44 | mse 22.44 | mre  0.00 |
scGPT - INFO - | epoch   4 | 3100/3602 batches | lr 0.0001 | ms/batch 90.70 | loss 19.96 | mse 19.96 | mre  0.00 |
scGPT - INFO - | epoch   4 | 3200/3602 batches | lr 0.0001 | ms/batch 91.29 | loss 20.89 | mse 20.89 | mre  0.00 |
scGPT - INFO - | epoch   4 | 3300/3602 batches | lr 0.0001 | ms/batch 90.71 | loss 20.09 | mse 20.09 | mre  0.00 |
scGPT - INFO - | epoch   4 | 3400/3602 batches | lr 0.0001 | ms/batch 90.70 | loss 20.42 | mse 20.42 | mre  0.00 |
scGPT - INFO - | epoch   4 | 3500/3602 batches | lr 0.0001 | ms/batch 90.72 | loss 18.45 | mse 18.45 | mre  0.00 |
scGPT - INFO - | epoch   4 | 3600/3602 batches | lr 0.0001 | ms/batch 91.99 | loss 23.33 | mse 23.33 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 339.35s | valid loss/mse 20.2631 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 20.2631
best_loss: 20.328545244147865, min_delta 0.0001, val_loss 20.26307667626275
Loss error: 0.0654685678851159
epoch:  4
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[5.496 3.89  3.785 ... 3.756 3.855 3.707]
 [5.504 3.898 3.791 ... 3.764 3.861 3.713]
 [5.492 3.889 3.781 ... 3.754 3.854 3.705]
 ...
 [5.5   3.895 3.79  ... 3.76  3.86  3.71 ]
 [5.5   3.896 3.79  ... 3.762 3.86  3.71 ]
 [5.516 3.908 3.8   ... 3.771 3.871 3.723]]
(801, 11) (801, 11)
By feature:  MSE:  0.9212¬±0.0 MAE:  0.6931¬±0.0 R2:  -0.1757¬±0.0 PCC:  0.5823¬±0.0 Cosine Similarity:  0.9811¬±0.0
By sample:  MSE:  0.9212¬±0.0 MAE:  0.6931¬±0.0 R2:  -0.0088¬±0.0 PCC:  0.0755¬±0.0 Cosine Similarity:  0.9775¬±0.0
scGPT - INFO - By feature: MSE: 0.9211999773979187¬±0.0 MAE: 0.6930999755859375¬±0.0 R2: -0.1757¬±0.0 PCC: 0.5823¬±0.0 Cosine Similarity: 0.9811¬±0.0
scGPT - INFO - By sample: MSE: 0.9211999773979187¬±0.0 MAE: 0.6930999755859375¬±0.0 R2: -0.0088¬±0.0 PCC: 0.0755¬±0.0 Cosine Similarity: 0.9775¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  5
Training model
scGPT - INFO - | epoch   5 | 100/3602 batches | lr 0.0001 | ms/batch 91.86 | loss 21.78 | mse 21.78 | mre  0.00 |
scGPT - INFO - | epoch   5 | 200/3602 batches | lr 0.0001 | ms/batch 90.68 | loss 21.56 | mse 21.56 | mre  0.00 |
scGPT - INFO - | epoch   5 | 300/3602 batches | lr 0.0001 | ms/batch 90.73 | loss 18.76 | mse 18.76 | mre  0.00 |
scGPT - INFO - | epoch   5 | 400/3602 batches | lr 0.0001 | ms/batch 90.66 | loss 19.99 | mse 19.99 | mre  0.00 |
scGPT - INFO - | epoch   5 | 500/3602 batches | lr 0.0001 | ms/batch 90.68 | loss 19.01 | mse 19.01 | mre  0.00 |
scGPT - INFO - | epoch   5 | 600/3602 batches | lr 0.0001 | ms/batch 91.29 | loss 19.25 | mse 19.25 | mre  0.00 |
scGPT - INFO - | epoch   5 | 700/3602 batches | lr 0.0001 | ms/batch 90.68 | loss 21.05 | mse 21.05 | mre  0.00 |
scGPT - INFO - | epoch   5 | 800/3602 batches | lr 0.0001 | ms/batch 90.68 | loss 21.33 | mse 21.33 | mre  0.00 |
scGPT - INFO - | epoch   5 | 900/3602 batches | lr 0.0001 | ms/batch 90.67 | loss 20.59 | mse 20.59 | mre  0.00 |
scGPT - INFO - | epoch   5 | 1000/3602 batches | lr 0.0001 | ms/batch 90.65 | loss 20.75 | mse 20.75 | mre  0.00 |
scGPT - INFO - | epoch   5 | 1100/3602 batches | lr 0.0001 | ms/batch 90.67 | loss 21.12 | mse 21.12 | mre  0.00 |
scGPT - INFO - | epoch   5 | 1200/3602 batches | lr 0.0001 | ms/batch 90.72 | loss 21.04 | mse 21.04 | mre  0.00 |
scGPT - INFO - | epoch   5 | 1300/3602 batches | lr 0.0001 | ms/batch 90.72 | loss 21.28 | mse 21.28 | mre  0.00 |
scGPT - INFO - | epoch   5 | 1400/3602 batches | lr 0.0001 | ms/batch 90.70 | loss 23.42 | mse 23.42 | mre  0.00 |
scGPT - INFO - | epoch   5 | 1500/3602 batches | lr 0.0001 | ms/batch 90.74 | loss 20.70 | mse 20.70 | mre  0.00 |
scGPT - INFO - | epoch   5 | 1600/3602 batches | lr 0.0001 | ms/batch 91.23 | loss 19.16 | mse 19.16 | mre  0.00 |
scGPT - INFO - | epoch   5 | 1700/3602 batches | lr 0.0001 | ms/batch 90.68 | loss 21.64 | mse 21.64 | mre  0.00 |
scGPT - INFO - | epoch   5 | 1800/3602 batches | lr 0.0001 | ms/batch 90.72 | loss 19.16 | mse 19.16 | mre  0.00 |
scGPT - INFO - | epoch   5 | 1900/3602 batches | lr 0.0001 | ms/batch 90.62 | loss 22.74 | mse 22.74 | mre  0.00 |
scGPT - INFO - | epoch   5 | 2000/3602 batches | lr 0.0001 | ms/batch 90.65 | loss 20.85 | mse 20.85 | mre  0.00 |
scGPT - INFO - | epoch   5 | 2100/3602 batches | lr 0.0001 | ms/batch 90.65 | loss 21.50 | mse 21.50 | mre  0.00 |
scGPT - INFO - | epoch   5 | 2200/3602 batches | lr 0.0001 | ms/batch 90.62 | loss 20.91 | mse 20.91 | mre  0.00 |
scGPT - INFO - | epoch   5 | 2300/3602 batches | lr 0.0001 | ms/batch 90.80 | loss 20.30 | mse 20.30 | mre  0.00 |
scGPT - INFO - | epoch   5 | 2400/3602 batches | lr 0.0001 | ms/batch 90.94 | loss 20.56 | mse 20.56 | mre  0.00 |
scGPT - INFO - | epoch   5 | 2500/3602 batches | lr 0.0001 | ms/batch 90.57 | loss 20.13 | mse 20.13 | mre  0.00 |
scGPT - INFO - | epoch   5 | 2600/3602 batches | lr 0.0001 | ms/batch 91.16 | loss 21.73 | mse 21.73 | mre  0.00 |
scGPT - INFO - | epoch   5 | 2700/3602 batches | lr 0.0001 | ms/batch 90.62 | loss 21.12 | mse 21.12 | mre  0.00 |
scGPT - INFO - | epoch   5 | 2800/3602 batches | lr 0.0001 | ms/batch 90.59 | loss 20.07 | mse 20.07 | mre  0.00 |
scGPT - INFO - | epoch   5 | 2900/3602 batches | lr 0.0001 | ms/batch 90.60 | loss 20.06 | mse 20.06 | mre  0.00 |
scGPT - INFO - | epoch   5 | 3000/3602 batches | lr 0.0001 | ms/batch 90.61 | loss 22.32 | mse 22.32 | mre  0.00 |
scGPT - INFO - | epoch   5 | 3100/3602 batches | lr 0.0001 | ms/batch 90.58 | loss 19.83 | mse 19.83 | mre  0.00 |
scGPT - INFO - | epoch   5 | 3200/3602 batches | lr 0.0001 | ms/batch 90.65 | loss 20.79 | mse 20.79 | mre  0.00 |
scGPT - INFO - | epoch   5 | 3300/3602 batches | lr 0.0001 | ms/batch 90.61 | loss 19.93 | mse 19.93 | mre  0.00 |
scGPT - INFO - | epoch   5 | 3400/3602 batches | lr 0.0001 | ms/batch 90.62 | loss 20.31 | mse 20.31 | mre  0.00 |
scGPT - INFO - | epoch   5 | 3500/3602 batches | lr 0.0001 | ms/batch 90.58 | loss 18.30 | mse 18.30 | mre  0.00 |
scGPT - INFO - | epoch   5 | 3600/3602 batches | lr 0.0001 | ms/batch 91.15 | loss 23.17 | mse 23.17 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 339.23s | valid loss/mse 20.1938 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 20.1938
best_loss: 20.26307667626275, min_delta 0.0001, val_loss 20.193820020530406
Loss error: 0.06925665573234241
epoch:  5
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[5.46  3.902 3.771 ... 3.742 3.838 3.695]
 [5.48  3.916 3.785 ... 3.756 3.85  3.707]
 [5.457 3.898 3.768 ... 3.74  3.834 3.693]
 ...
 [5.477 3.912 3.781 ... 3.754 3.846 3.703]
 [5.477 3.914 3.783 ... 3.754 3.848 3.705]
 [5.51  3.938 3.809 ... 3.777 3.87  3.729]]
(801, 11) (801, 11)
By feature:  MSE:  0.9181¬±0.0 MAE:  0.6926¬±0.0 R2:  -0.1704¬±0.0 PCC:  0.5837¬±0.0 Cosine Similarity:  0.9811¬±0.0
By sample:  MSE:  0.9181¬±0.0 MAE:  0.6926¬±0.0 R2:  -0.0051¬±0.0 PCC:  0.0874¬±0.0 Cosine Similarity:  0.9776¬±0.0
scGPT - INFO - By feature: MSE: 0.9180999994277954¬±0.0 MAE: 0.6926000118255615¬±0.0 R2: -0.1704¬±0.0 PCC: 0.5837¬±0.0 Cosine Similarity: 0.9811¬±0.0
scGPT - INFO - By sample: MSE: 0.9180999994277954¬±0.0 MAE: 0.6926000118255615¬±0.0 R2: -0.0051¬±0.0 PCC: 0.0874¬±0.0 Cosine Similarity: 0.9776¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  6
Training model
scGPT - INFO - | epoch   6 | 100/3602 batches | lr 0.0001 | ms/batch 92.49 | loss 21.65 | mse 21.65 | mre  0.00 |
scGPT - INFO - | epoch   6 | 200/3602 batches | lr 0.0001 | ms/batch 91.05 | loss 21.61 | mse 21.61 | mre  0.00 |
scGPT - INFO - | epoch   6 | 300/3602 batches | lr 0.0001 | ms/batch 91.89 | loss 18.71 | mse 18.71 | mre  0.00 |
scGPT - INFO - | epoch   6 | 400/3602 batches | lr 0.0001 | ms/batch 91.11 | loss 19.82 | mse 19.82 | mre  0.00 |
scGPT - INFO - | epoch   6 | 500/3602 batches | lr 0.0001 | ms/batch 90.92 | loss 18.90 | mse 18.90 | mre  0.00 |
scGPT - INFO - | epoch   6 | 600/3602 batches | lr 0.0001 | ms/batch 91.08 | loss 19.17 | mse 19.17 | mre  0.00 |
scGPT - INFO - | epoch   6 | 700/3602 batches | lr 0.0001 | ms/batch 90.82 | loss 21.04 | mse 21.04 | mre  0.00 |
scGPT - INFO - | epoch   6 | 800/3602 batches | lr 0.0001 | ms/batch 90.83 | loss 21.21 | mse 21.21 | mre  0.00 |
scGPT - INFO - | epoch   6 | 900/3602 batches | lr 0.0001 | ms/batch 90.78 | loss 20.51 | mse 20.51 | mre  0.00 |
scGPT - INFO - | epoch   6 | 1000/3602 batches | lr 0.0001 | ms/batch 91.31 | loss 20.74 | mse 20.74 | mre  0.00 |
scGPT - INFO - | epoch   6 | 1100/3602 batches | lr 0.0001 | ms/batch 90.73 | loss 20.98 | mse 20.98 | mre  0.00 |
scGPT - INFO - | epoch   6 | 1200/3602 batches | lr 0.0001 | ms/batch 90.71 | loss 21.01 | mse 21.01 | mre  0.00 |
scGPT - INFO - | epoch   6 | 1300/3602 batches | lr 0.0001 | ms/batch 90.71 | loss 21.26 | mse 21.26 | mre  0.00 |
scGPT - INFO - | epoch   6 | 1400/3602 batches | lr 0.0001 | ms/batch 90.71 | loss 23.16 | mse 23.16 | mre  0.00 |
scGPT - INFO - | epoch   6 | 1500/3602 batches | lr 0.0001 | ms/batch 90.78 | loss 20.59 | mse 20.59 | mre  0.00 |
scGPT - INFO - | epoch   6 | 1600/3602 batches | lr 0.0001 | ms/batch 90.65 | loss 19.13 | mse 19.13 | mre  0.00 |
scGPT - INFO - | epoch   6 | 1700/3602 batches | lr 0.0001 | ms/batch 90.68 | loss 21.59 | mse 21.59 | mre  0.00 |
scGPT - INFO - | epoch   6 | 1800/3602 batches | lr 0.0001 | ms/batch 90.70 | loss 19.10 | mse 19.10 | mre  0.00 |
scGPT - INFO - | epoch   6 | 1900/3602 batches | lr 0.0001 | ms/batch 90.67 | loss 22.78 | mse 22.78 | mre  0.00 |
scGPT - INFO - | epoch   6 | 2000/3602 batches | lr 0.0001 | ms/batch 91.24 | loss 20.86 | mse 20.86 | mre  0.00 |
scGPT - INFO - | epoch   6 | 2100/3602 batches | lr 0.0001 | ms/batch 90.73 | loss 21.41 | mse 21.41 | mre  0.00 |
scGPT - INFO - | epoch   6 | 2200/3602 batches | lr 0.0001 | ms/batch 90.75 | loss 20.79 | mse 20.79 | mre  0.00 |
scGPT - INFO - | epoch   6 | 2300/3602 batches | lr 0.0001 | ms/batch 90.68 | loss 20.28 | mse 20.28 | mre  0.00 |
scGPT - INFO - | epoch   6 | 2400/3602 batches | lr 0.0001 | ms/batch 90.66 | loss 20.42 | mse 20.42 | mre  0.00 |
scGPT - INFO - | epoch   6 | 2500/3602 batches | lr 0.0001 | ms/batch 90.67 | loss 20.06 | mse 20.06 | mre  0.00 |
scGPT - INFO - | epoch   6 | 2600/3602 batches | lr 0.0001 | ms/batch 90.67 | loss 21.57 | mse 21.57 | mre  0.00 |
scGPT - INFO - | epoch   6 | 2700/3602 batches | lr 0.0001 | ms/batch 90.70 | loss 21.10 | mse 21.10 | mre  0.00 |
scGPT - INFO - | epoch   6 | 2800/3602 batches | lr 0.0001 | ms/batch 90.68 | loss 19.95 | mse 19.95 | mre  0.00 |
scGPT - INFO - | epoch   6 | 2900/3602 batches | lr 0.0001 | ms/batch 90.65 | loss 20.11 | mse 20.11 | mre  0.00 |
scGPT - INFO - | epoch   6 | 3000/3602 batches | lr 0.0001 | ms/batch 91.20 | loss 22.20 | mse 22.20 | mre  0.00 |
scGPT - INFO - | epoch   6 | 3100/3602 batches | lr 0.0001 | ms/batch 91.03 | loss 19.73 | mse 19.73 | mre  0.00 |
scGPT - INFO - | epoch   6 | 3200/3602 batches | lr 0.0001 | ms/batch 90.77 | loss 20.69 | mse 20.69 | mre  0.00 |
scGPT - INFO - | epoch   6 | 3300/3602 batches | lr 0.0001 | ms/batch 90.70 | loss 19.85 | mse 19.85 | mre  0.00 |
scGPT - INFO - | epoch   6 | 3400/3602 batches | lr 0.0001 | ms/batch 90.68 | loss 20.18 | mse 20.18 | mre  0.00 |
scGPT - INFO - | epoch   6 | 3500/3602 batches | lr 0.0001 | ms/batch 90.75 | loss 18.32 | mse 18.32 | mre  0.00 |
scGPT - INFO - | epoch   6 | 3600/3602 batches | lr 0.0001 | ms/batch 90.73 | loss 22.98 | mse 22.98 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 339.65s | valid loss/mse 20.1131 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 20.1131
best_loss: 20.193820020530406, min_delta 0.0001, val_loss 20.113095941317365
Loss error: 0.08072407921304148
epoch:  6
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[5.418 3.906 3.771 ... 3.729 3.816 3.693]
 [5.45  3.93  3.793 ... 3.748 3.838 3.713]
 [5.414 3.9   3.768 ... 3.725 3.812 3.69 ]
 ...
 [5.438 3.918 3.783 ... 3.738 3.828 3.705]
 [5.45  3.928 3.791 ... 3.748 3.836 3.713]
 [5.51  3.973 3.834 ... 3.785 3.873 3.752]]
(801, 11) (801, 11)
By feature:  MSE:  0.9144¬±0.0 MAE:  0.6923¬±0.0 R2:  -0.1645¬±0.0 PCC:  0.5846¬±0.0 Cosine Similarity:  0.9812¬±0.0
By sample:  MSE:  0.9144¬±0.0 MAE:  0.6923¬±0.0 R2:  -0.0005¬±0.0 PCC:  0.1005¬±0.0 Cosine Similarity:  0.9776¬±0.0
scGPT - INFO - By feature: MSE: 0.9143999814987183¬±0.0 MAE: 0.692300021648407¬±0.0 R2: -0.1645¬±0.0 PCC: 0.5846¬±0.0 Cosine Similarity: 0.9812¬±0.0
scGPT - INFO - By sample: MSE: 0.9143999814987183¬±0.0 MAE: 0.692300021648407¬±0.0 R2: -0.0005¬±0.0 PCC: 0.1005¬±0.0 Cosine Similarity: 0.9776¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  7
Training model
scGPT - INFO - | epoch   7 | 100/3602 batches | lr 0.0001 | ms/batch 91.95 | loss 21.61 | mse 21.61 | mre  0.00 |
scGPT - INFO - | epoch   7 | 200/3602 batches | lr 0.0001 | ms/batch 90.66 | loss 21.46 | mse 21.46 | mre  0.00 |
scGPT - INFO - | epoch   7 | 300/3602 batches | lr 0.0001 | ms/batch 90.66 | loss 18.65 | mse 18.65 | mre  0.00 |
scGPT - INFO - | epoch   7 | 400/3602 batches | lr 0.0001 | ms/batch 91.24 | loss 19.80 | mse 19.80 | mre  0.00 |
scGPT - INFO - | epoch   7 | 500/3602 batches | lr 0.0001 | ms/batch 90.66 | loss 18.94 | mse 18.94 | mre  0.00 |
scGPT - INFO - | epoch   7 | 600/3602 batches | lr 0.0001 | ms/batch 90.68 | loss 19.07 | mse 19.07 | mre  0.00 |
scGPT - INFO - | epoch   7 | 700/3602 batches | lr 0.0001 | ms/batch 90.70 | loss 20.88 | mse 20.88 | mre  0.00 |
scGPT - INFO - | epoch   7 | 800/3602 batches | lr 0.0001 | ms/batch 90.64 | loss 21.18 | mse 21.18 | mre  0.00 |
scGPT - INFO - | epoch   7 | 900/3602 batches | lr 0.0001 | ms/batch 90.66 | loss 20.35 | mse 20.35 | mre  0.00 |
scGPT - INFO - | epoch   7 | 1000/3602 batches | lr 0.0001 | ms/batch 90.68 | loss 20.74 | mse 20.74 | mre  0.00 |
scGPT - INFO - | epoch   7 | 1100/3602 batches | lr 0.0001 | ms/batch 90.66 | loss 20.89 | mse 20.89 | mre  0.00 |
scGPT - INFO - | epoch   7 | 1200/3602 batches | lr 0.0001 | ms/batch 90.73 | loss 20.96 | mse 20.96 | mre  0.00 |
scGPT - INFO - | epoch   7 | 1300/3602 batches | lr 0.0001 | ms/batch 90.72 | loss 21.18 | mse 21.18 | mre  0.00 |
scGPT - INFO - | epoch   7 | 1400/3602 batches | lr 0.0001 | ms/batch 91.18 | loss 23.03 | mse 23.03 | mre  0.00 |
scGPT - INFO - | epoch   7 | 1500/3602 batches | lr 0.0001 | ms/batch 90.66 | loss 20.41 | mse 20.41 | mre  0.00 |
scGPT - INFO - | epoch   7 | 1600/3602 batches | lr 0.0001 | ms/batch 90.63 | loss 19.01 | mse 19.01 | mre  0.00 |
scGPT - INFO - | epoch   7 | 1700/3602 batches | lr 0.0001 | ms/batch 90.68 | loss 21.44 | mse 21.44 | mre  0.00 |
scGPT - INFO - | epoch   7 | 1800/3602 batches | lr 0.0001 | ms/batch 90.64 | loss 18.92 | mse 18.92 | mre  0.00 |
scGPT - INFO - | epoch   7 | 1900/3602 batches | lr 0.0001 | ms/batch 90.74 | loss 22.47 | mse 22.47 | mre  0.00 |
scGPT - INFO - | epoch   7 | 2000/3602 batches | lr 0.0001 | ms/batch 90.71 | loss 20.63 | mse 20.63 | mre  0.00 |
scGPT - INFO - | epoch   7 | 2100/3602 batches | lr 0.0001 | ms/batch 90.73 | loss 21.18 | mse 21.18 | mre  0.00 |
scGPT - INFO - | epoch   7 | 2200/3602 batches | lr 0.0001 | ms/batch 90.74 | loss 20.61 | mse 20.61 | mre  0.00 |
scGPT - INFO - | epoch   7 | 2300/3602 batches | lr 0.0001 | ms/batch 90.72 | loss 20.29 | mse 20.29 | mre  0.00 |
scGPT - INFO - | epoch   7 | 2400/3602 batches | lr 0.0001 | ms/batch 91.23 | loss 20.41 | mse 20.41 | mre  0.00 |
scGPT - INFO - | epoch   7 | 2500/3602 batches | lr 0.0001 | ms/batch 90.65 | loss 20.14 | mse 20.14 | mre  0.00 |
scGPT - INFO - | epoch   7 | 2600/3602 batches | lr 0.0001 | ms/batch 90.70 | loss 21.43 | mse 21.43 | mre  0.00 |
scGPT - INFO - | epoch   7 | 2700/3602 batches | lr 0.0001 | ms/batch 90.75 | loss 20.99 | mse 20.99 | mre  0.00 |
scGPT - INFO - | epoch   7 | 2800/3602 batches | lr 0.0001 | ms/batch 90.65 | loss 19.99 | mse 19.99 | mre  0.00 |
scGPT - INFO - | epoch   7 | 2900/3602 batches | lr 0.0001 | ms/batch 90.68 | loss 19.99 | mse 19.99 | mre  0.00 |
scGPT - INFO - | epoch   7 | 3000/3602 batches | lr 0.0001 | ms/batch 90.66 | loss 21.90 | mse 21.90 | mre  0.00 |
scGPT - INFO - | epoch   7 | 3100/3602 batches | lr 0.0001 | ms/batch 90.74 | loss 19.61 | mse 19.61 | mre  0.00 |
scGPT - INFO - | epoch   7 | 3200/3602 batches | lr 0.0001 | ms/batch 90.63 | loss 20.72 | mse 20.72 | mre  0.00 |
scGPT - INFO - | epoch   7 | 3300/3602 batches | lr 0.0001 | ms/batch 90.67 | loss 19.74 | mse 19.74 | mre  0.00 |
scGPT - INFO - | epoch   7 | 3400/3602 batches | lr 0.0001 | ms/batch 91.19 | loss 20.11 | mse 20.11 | mre  0.00 |
scGPT - INFO - | epoch   7 | 3500/3602 batches | lr 0.0001 | ms/batch 90.62 | loss 18.36 | mse 18.36 | mre  0.00 |
scGPT - INFO - | epoch   7 | 3600/3602 batches | lr 0.0001 | ms/batch 90.67 | loss 22.80 | mse 22.80 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 339.27s | valid loss/mse 20.0419 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 20.0419
best_loss: 20.113095941317365, min_delta 0.0001, val_loss 20.04192610775189
Loss error: 0.0711698335654738
epoch:  7
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[5.44  3.94  3.8   ... 3.74  3.84  3.723]
 [5.492 3.977 3.834 ... 3.775 3.871 3.752]
 [5.44  3.938 3.799 ... 3.738 3.838 3.72 ]
 ...
 [5.453 3.947 3.809 ... 3.748 3.848 3.729]
 [5.496 3.982 3.838 ... 3.781 3.875 3.756]
 [5.562 4.03  3.883 ... 3.832 3.918 3.799]]
(801, 11) (801, 11)
By feature:  MSE:  0.9112¬±0.0 MAE:  0.6962¬±0.0 R2:  -0.1649¬±0.0 PCC:  0.5854¬±0.0 Cosine Similarity:  0.9812¬±0.0
By sample:  MSE:  0.9112¬±0.0 MAE:  0.6962¬±0.0 R2:  0.0035¬±0.0 PCC:  0.1086¬±0.0 Cosine Similarity:  0.9777¬±0.0
scGPT - INFO - By feature: MSE: 0.9111999869346619¬±0.0 MAE: 0.6962000131607056¬±0.0 R2: -0.1649¬±0.0 PCC: 0.5854¬±0.0 Cosine Similarity: 0.9812¬±0.0
scGPT - INFO - By sample: MSE: 0.9111999869346619¬±0.0 MAE: 0.6962000131607056¬±0.0 R2: 0.0035¬±0.0 PCC: 0.1086¬±0.0 Cosine Similarity: 0.9777¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  8
Training model
scGPT - INFO - | epoch   8 | 100/3602 batches | lr 0.0000 | ms/batch 92.07 | loss 21.49 | mse 21.49 | mre  0.00 |
scGPT - INFO - | epoch   8 | 200/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 21.47 | mse 21.47 | mre  0.00 |
scGPT - INFO - | epoch   8 | 300/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 18.55 | mse 18.55 | mre  0.00 |
scGPT - INFO - | epoch   8 | 400/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 19.77 | mse 19.77 | mre  0.00 |
scGPT - INFO - | epoch   8 | 500/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 18.86 | mse 18.86 | mre  0.00 |
scGPT - INFO - | epoch   8 | 600/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 18.97 | mse 18.97 | mre  0.00 |
scGPT - INFO - | epoch   8 | 700/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 20.77 | mse 20.77 | mre  0.00 |
scGPT - INFO - | epoch   8 | 800/3602 batches | lr 0.0000 | ms/batch 91.29 | loss 21.18 | mse 21.18 | mre  0.00 |
scGPT - INFO - | epoch   8 | 900/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 20.29 | mse 20.29 | mre  0.00 |
scGPT - INFO - | epoch   8 | 1000/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 20.77 | mse 20.77 | mre  0.00 |
scGPT - INFO - | epoch   8 | 1100/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 20.85 | mse 20.85 | mre  0.00 |
scGPT - INFO - | epoch   8 | 1200/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 20.84 | mse 20.84 | mre  0.00 |
scGPT - INFO - | epoch   8 | 1300/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 21.19 | mse 21.19 | mre  0.00 |
scGPT - INFO - | epoch   8 | 1400/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 22.76 | mse 22.76 | mre  0.00 |
scGPT - INFO - | epoch   8 | 1500/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 20.37 | mse 20.37 | mre  0.00 |
scGPT - INFO - | epoch   8 | 1600/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 18.90 | mse 18.90 | mre  0.00 |
scGPT - INFO - | epoch   8 | 1700/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 21.18 | mse 21.18 | mre  0.00 |
scGPT - INFO - | epoch   8 | 1800/3602 batches | lr 0.0000 | ms/batch 91.31 | loss 18.71 | mse 18.71 | mre  0.00 |
scGPT - INFO - | epoch   8 | 1900/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 22.36 | mse 22.36 | mre  0.00 |
scGPT - INFO - | epoch   8 | 2000/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 20.46 | mse 20.46 | mre  0.00 |
scGPT - INFO - | epoch   8 | 2100/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 21.14 | mse 21.14 | mre  0.00 |
scGPT - INFO - | epoch   8 | 2200/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 20.21 | mse 20.21 | mre  0.00 |
scGPT - INFO - | epoch   8 | 2300/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 20.38 | mse 20.38 | mre  0.00 |
scGPT - INFO - | epoch   8 | 2400/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 20.38 | mse 20.38 | mre  0.00 |
scGPT - INFO - | epoch   8 | 2500/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 20.13 | mse 20.13 | mre  0.00 |
scGPT - INFO - | epoch   8 | 2600/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 21.45 | mse 21.45 | mre  0.00 |
scGPT - INFO - | epoch   8 | 2700/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 20.91 | mse 20.91 | mre  0.00 |
scGPT - INFO - | epoch   8 | 2800/3602 batches | lr 0.0000 | ms/batch 91.22 | loss 19.92 | mse 19.92 | mre  0.00 |
scGPT - INFO - | epoch   8 | 2900/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 20.00 | mse 20.00 | mre  0.00 |
scGPT - INFO - | epoch   8 | 3000/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 21.86 | mse 21.86 | mre  0.00 |
scGPT - INFO - | epoch   8 | 3100/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 19.48 | mse 19.48 | mre  0.00 |
scGPT - INFO - | epoch   8 | 3200/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 20.61 | mse 20.61 | mre  0.00 |
scGPT - INFO - | epoch   8 | 3300/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 19.71 | mse 19.71 | mre  0.00 |
scGPT - INFO - | epoch   8 | 3400/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 19.98 | mse 19.98 | mre  0.00 |
scGPT - INFO - | epoch   8 | 3500/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 18.36 | mse 18.36 | mre  0.00 |
scGPT - INFO - | epoch   8 | 3600/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 22.70 | mse 22.70 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   8 | time: 339.52s | valid loss/mse 19.9495 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 19.9495
best_loss: 20.04192610775189, min_delta 0.0001, val_loss 19.949499202875906
Loss error: 0.09242690487598537
epoch:  8
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[5.43  3.941 3.793 ... 3.719 3.82  3.727]
 [5.473 3.979 3.826 ... 3.762 3.854 3.748]
 [5.43  3.943 3.795 ... 3.723 3.822 3.727]
 ...
 [5.438 3.947 3.799 ... 3.725 3.826 3.73 ]
 [5.484 3.988 3.836 ... 3.773 3.861 3.754]
 [5.54  4.04  3.88  ... 3.83  3.904 3.785]]
(801, 11) (801, 11)
By feature:  MSE:  0.907¬±0.0 MAE:  0.6951¬±0.0 R2:  -0.1623¬±0.0 PCC:  0.5867¬±0.0 Cosine Similarity:  0.9813¬±0.0
By sample:  MSE:  0.907¬±0.0 MAE:  0.6951¬±0.0 R2:  0.0089¬±0.0 PCC:  0.1204¬±0.0 Cosine Similarity:  0.9778¬±0.0
scGPT - INFO - By feature: MSE: 0.9070000052452087¬±0.0 MAE: 0.6951000094413757¬±0.0 R2: -0.1623¬±0.0 PCC: 0.5867¬±0.0 Cosine Similarity: 0.9813¬±0.0
scGPT - INFO - By sample: MSE: 0.9070000052452087¬±0.0 MAE: 0.6951000094413757¬±0.0 R2: 0.0089¬±0.0 PCC: 0.1204¬±0.0 Cosine Similarity: 0.9778¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  9
Training model
scGPT - INFO - | epoch   9 | 100/3602 batches | lr 0.0000 | ms/batch 92.02 | loss 21.35 | mse 21.35 | mre  0.00 |
scGPT - INFO - | epoch   9 | 200/3602 batches | lr 0.0000 | ms/batch 91.32 | loss 21.35 | mse 21.35 | mre  0.00 |
scGPT - INFO - | epoch   9 | 300/3602 batches | lr 0.0000 | ms/batch 90.62 | loss 18.46 | mse 18.46 | mre  0.00 |
scGPT - INFO - | epoch   9 | 400/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 19.75 | mse 19.75 | mre  0.00 |
scGPT - INFO - | epoch   9 | 500/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 18.78 | mse 18.78 | mre  0.00 |
scGPT - INFO - | epoch   9 | 600/3602 batches | lr 0.0000 | ms/batch 90.62 | loss 18.84 | mse 18.84 | mre  0.00 |
scGPT - INFO - | epoch   9 | 700/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 20.58 | mse 20.58 | mre  0.00 |
scGPT - INFO - | epoch   9 | 800/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 21.09 | mse 21.09 | mre  0.00 |
scGPT - INFO - | epoch   9 | 900/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 20.29 | mse 20.29 | mre  0.00 |
scGPT - INFO - | epoch   9 | 1000/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 20.62 | mse 20.62 | mre  0.00 |
scGPT - INFO - | epoch   9 | 1100/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 20.61 | mse 20.61 | mre  0.00 |
scGPT - INFO - | epoch   9 | 1200/3602 batches | lr 0.0000 | ms/batch 91.18 | loss 20.62 | mse 20.62 | mre  0.00 |
scGPT - INFO - | epoch   9 | 1300/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 21.04 | mse 21.04 | mre  0.00 |
scGPT - INFO - | epoch   9 | 1400/3602 batches | lr 0.0000 | ms/batch 91.62 | loss 22.41 | mse 22.41 | mre  0.00 |
scGPT - INFO - | epoch   9 | 1500/3602 batches | lr 0.0000 | ms/batch 91.10 | loss 20.33 | mse 20.33 | mre  0.00 |
scGPT - INFO - | epoch   9 | 1600/3602 batches | lr 0.0000 | ms/batch 91.12 | loss 18.75 | mse 18.75 | mre  0.00 |
scGPT - INFO - | epoch   9 | 1700/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 20.91 | mse 20.91 | mre  0.00 |
scGPT - INFO - | epoch   9 | 1800/3602 batches | lr 0.0000 | ms/batch 91.13 | loss 18.77 | mse 18.77 | mre  0.00 |
scGPT - INFO - | epoch   9 | 1900/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 22.34 | mse 22.34 | mre  0.00 |
scGPT - INFO - | epoch   9 | 2000/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 20.33 | mse 20.33 | mre  0.00 |
scGPT - INFO - | epoch   9 | 2100/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 20.93 | mse 20.93 | mre  0.00 |
scGPT - INFO - | epoch   9 | 2200/3602 batches | lr 0.0000 | ms/batch 91.62 | loss 19.96 | mse 19.96 | mre  0.00 |
scGPT - INFO - | epoch   9 | 2300/3602 batches | lr 0.0000 | ms/batch 91.17 | loss 20.34 | mse 20.34 | mre  0.00 |
scGPT - INFO - | epoch   9 | 2400/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 20.24 | mse 20.24 | mre  0.00 |
scGPT - INFO - | epoch   9 | 2500/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 19.89 | mse 19.89 | mre  0.00 |
scGPT - INFO - | epoch   9 | 2600/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 21.16 | mse 21.16 | mre  0.00 |
scGPT - INFO - | epoch   9 | 2700/3602 batches | lr 0.0000 | ms/batch 91.01 | loss 20.85 | mse 20.85 | mre  0.00 |
scGPT - INFO - | epoch   9 | 2800/3602 batches | lr 0.0000 | ms/batch 91.05 | loss 19.73 | mse 19.73 | mre  0.00 |
scGPT - INFO - | epoch   9 | 2900/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 19.99 | mse 19.99 | mre  0.00 |
scGPT - INFO - | epoch   9 | 3000/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 21.31 | mse 21.31 | mre  0.00 |
scGPT - INFO - | epoch   9 | 3100/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 19.05 | mse 19.05 | mre  0.00 |
scGPT - INFO - | epoch   9 | 3200/3602 batches | lr 0.0000 | ms/batch 91.18 | loss 20.33 | mse 20.33 | mre  0.00 |
scGPT - INFO - | epoch   9 | 3300/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 19.26 | mse 19.26 | mre  0.00 |
scGPT - INFO - | epoch   9 | 3400/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 19.55 | mse 19.55 | mre  0.00 |
scGPT - INFO - | epoch   9 | 3500/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 17.86 | mse 17.86 | mre  0.00 |
scGPT - INFO - | epoch   9 | 3600/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 22.36 | mse 22.36 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   9 | time: 339.70s | valid loss/mse 18.7520 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 18.7520
best_loss: 19.949499202875906, min_delta 0.0001, val_loss 18.75195505258891
Loss error: 1.1975441502869941
epoch:  9
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[5.645 3.898 3.773 ... 3.586 3.807 3.832]
 [5.55  3.941 3.803 ... 3.666 3.826 3.752]
 [5.43  3.97  3.822 ... 3.754 3.844 3.7  ]
 ...
 [5.305 4.1   3.93  ... 3.955 3.906 3.729]
 [5.324 4.21  3.992 ... 4.066 3.955 3.777]
 [5.645 3.914 3.781 ... 3.6   3.812 3.834]]
(801, 11) (801, 11)
By feature:  MSE:  0.8525¬±0.0 MAE:  0.6665¬±0.0 R2:  -0.1122¬±0.0 PCC:  0.623¬±0.0 Cosine Similarity:  0.9826¬±0.0
By sample:  MSE:  0.8525¬±0.0 MAE:  0.6665¬±0.0 R2:  0.0403¬±0.0 PCC:  0.171¬±0.0 Cosine Similarity:  0.979¬±0.0
scGPT - INFO - By feature: MSE: 0.8525000214576721¬±0.0 MAE: 0.6664999723434448¬±0.0 R2: -0.1122¬±0.0 PCC: 0.623¬±0.0 Cosine Similarity: 0.9826¬±0.0
scGPT - INFO - By sample: MSE: 0.8525000214576721¬±0.0 MAE: 0.6664999723434448¬±0.0 R2: 0.0403¬±0.0 PCC: 0.171¬±0.0 Cosine Similarity: 0.979¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  10
Training model
scGPT - INFO - | epoch  10 | 100/3602 batches | lr 0.0000 | ms/batch 91.91 | loss 19.62 | mse 19.62 | mre  0.00 |
scGPT - INFO - | epoch  10 | 200/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 20.63 | mse 20.63 | mre  0.00 |
scGPT - INFO - | epoch  10 | 300/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 16.21 | mse 16.21 | mre  0.00 |
scGPT - INFO - | epoch  10 | 400/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 17.37 | mse 17.37 | mre  0.00 |
scGPT - INFO - | epoch  10 | 500/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 16.31 | mse 16.31 | mre  0.00 |
scGPT - INFO - | epoch  10 | 600/3602 batches | lr 0.0000 | ms/batch 91.29 | loss 16.30 | mse 16.30 | mre  0.00 |
scGPT - INFO - | epoch  10 | 700/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 17.61 | mse 17.61 | mre  0.00 |
scGPT - INFO - | epoch  10 | 800/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 19.01 | mse 19.01 | mre  0.00 |
scGPT - INFO - | epoch  10 | 900/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 16.69 | mse 16.69 | mre  0.00 |
scGPT - INFO - | epoch  10 | 1000/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 18.03 | mse 18.03 | mre  0.00 |
scGPT - INFO - | epoch  10 | 1100/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 17.04 | mse 17.04 | mre  0.00 |
scGPT - INFO - | epoch  10 | 1200/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 16.93 | mse 16.93 | mre  0.00 |
scGPT - INFO - | epoch  10 | 1300/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 18.74 | mse 18.74 | mre  0.00 |
scGPT - INFO - | epoch  10 | 1400/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 19.56 | mse 19.56 | mre  0.00 |
scGPT - INFO - | epoch  10 | 1500/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 17.65 | mse 17.65 | mre  0.00 |
scGPT - INFO - | epoch  10 | 1600/3602 batches | lr 0.0000 | ms/batch 91.26 | loss 14.88 | mse 14.88 | mre  0.00 |
scGPT - INFO - | epoch  10 | 1700/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 17.65 | mse 17.65 | mre  0.00 |
scGPT - INFO - | epoch  10 | 1800/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 14.94 | mse 14.94 | mre  0.00 |
scGPT - INFO - | epoch  10 | 1900/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 20.21 | mse 20.21 | mre  0.00 |
scGPT - INFO - | epoch  10 | 2000/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 17.17 | mse 17.17 | mre  0.00 |
scGPT - INFO - | epoch  10 | 2100/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 18.19 | mse 18.19 | mre  0.00 |
scGPT - INFO - | epoch  10 | 2200/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 16.65 | mse 16.65 | mre  0.00 |
scGPT - INFO - | epoch  10 | 2300/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 17.13 | mse 17.13 | mre  0.00 |
scGPT - INFO - | epoch  10 | 2400/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 16.71 | mse 16.71 | mre  0.00 |
scGPT - INFO - | epoch  10 | 2500/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 16.23 | mse 16.23 | mre  0.00 |
scGPT - INFO - | epoch  10 | 2600/3602 batches | lr 0.0000 | ms/batch 91.25 | loss 18.63 | mse 18.63 | mre  0.00 |
scGPT - INFO - | epoch  10 | 2700/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 17.68 | mse 17.68 | mre  0.00 |
scGPT - INFO - | epoch  10 | 2800/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 16.13 | mse 16.13 | mre  0.00 |
scGPT - INFO - | epoch  10 | 2900/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 16.98 | mse 16.98 | mre  0.00 |
scGPT - INFO - | epoch  10 | 3000/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 18.11 | mse 18.11 | mre  0.00 |
scGPT - INFO - | epoch  10 | 3100/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 16.24 | mse 16.24 | mre  0.00 |
scGPT - INFO - | epoch  10 | 3200/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 17.35 | mse 17.35 | mre  0.00 |
scGPT - INFO - | epoch  10 | 3300/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 16.01 | mse 16.01 | mre  0.00 |
scGPT - INFO - | epoch  10 | 3400/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 16.30 | mse 16.30 | mre  0.00 |
scGPT - INFO - | epoch  10 | 3500/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 15.10 | mse 15.10 | mre  0.00 |
scGPT - INFO - | epoch  10 | 3600/3602 batches | lr 0.0000 | ms/batch 91.22 | loss 18.95 | mse 18.95 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  10 | time: 339.49s | valid loss/mse 16.6314 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 16.6314
best_loss: 18.75195505258891, min_delta 0.0001, val_loss 16.631407975852863
Loss error: 2.120547076736049
epoch:  10
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[5.902 3.756 3.531 ... 3.488 3.648 3.74 ]
 [6.22  3.736 3.467 ... 3.5   3.686 3.82 ]
 [4.383 4.137 4.195 ... 4.047 3.908 3.559]
 ...
 [4.46  4.23  4.29  ... 4.156 3.996 3.646]
 [4.5   4.23  4.285 ... 4.16  4.008 3.656]
 [6.562 3.727 3.41  ... 3.537 3.727 3.916]]
(801, 11) (801, 11)
By feature:  MSE:  0.756¬±0.0 MAE:  0.5943¬±0.0 R2:  -0.0313¬±0.0 PCC:  0.6834¬±0.0 Cosine Similarity:  0.9853¬±0.0
By sample:  MSE:  0.756¬±0.0 MAE:  0.5943¬±0.0 R2:  0.1087¬±0.0 PCC:  0.3067¬±0.0 Cosine Similarity:  0.9815¬±0.0
scGPT - INFO - By feature: MSE: 0.7559999823570251¬±0.0 MAE: 0.5942999720573425¬±0.0 R2: -0.0313¬±0.0 PCC: 0.6834¬±0.0 Cosine Similarity: 0.9853¬±0.0
scGPT - INFO - By sample: MSE: 0.7559999823570251¬±0.0 MAE: 0.5942999720573425¬±0.0 R2: 0.1087¬±0.0 PCC: 0.3067¬±0.0 Cosine Similarity: 0.9815¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  11
Training model
scGPT - INFO - | epoch  11 | 100/3602 batches | lr 0.0000 | ms/batch 91.99 | loss 16.77 | mse 16.77 | mre  0.00 |
scGPT - INFO - | epoch  11 | 200/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 17.83 | mse 17.83 | mre  0.00 |
scGPT - INFO - | epoch  11 | 300/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 14.18 | mse 14.18 | mre  0.00 |
scGPT - INFO - | epoch  11 | 400/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 15.83 | mse 15.83 | mre  0.00 |
scGPT - INFO - | epoch  11 | 500/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 14.81 | mse 14.81 | mre  0.00 |
scGPT - INFO - | epoch  11 | 600/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 14.90 | mse 14.90 | mre  0.00 |
scGPT - INFO - | epoch  11 | 700/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 17.08 | mse 17.08 | mre  0.00 |
scGPT - INFO - | epoch  11 | 800/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 17.77 | mse 17.77 | mre  0.00 |
scGPT - INFO - | epoch  11 | 900/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 15.79 | mse 15.79 | mre  0.00 |
scGPT - INFO - | epoch  11 | 1000/3602 batches | lr 0.0000 | ms/batch 91.25 | loss 17.67 | mse 17.67 | mre  0.00 |
scGPT - INFO - | epoch  11 | 1100/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 16.14 | mse 16.14 | mre  0.00 |
scGPT - INFO - | epoch  11 | 1200/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 16.44 | mse 16.44 | mre  0.00 |
scGPT - INFO - | epoch  11 | 1300/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 17.99 | mse 17.99 | mre  0.00 |
scGPT - INFO - | epoch  11 | 1400/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 18.58 | mse 18.58 | mre  0.00 |
scGPT - INFO - | epoch  11 | 1500/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 17.62 | mse 17.62 | mre  0.00 |
scGPT - INFO - | epoch  11 | 1600/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 14.18 | mse 14.18 | mre  0.00 |
scGPT - INFO - | epoch  11 | 1700/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 17.02 | mse 17.02 | mre  0.00 |
scGPT - INFO - | epoch  11 | 1800/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 14.57 | mse 14.57 | mre  0.00 |
scGPT - INFO - | epoch  11 | 1900/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 18.95 | mse 18.95 | mre  0.00 |
scGPT - INFO - | epoch  11 | 2000/3602 batches | lr 0.0000 | ms/batch 91.38 | loss 16.58 | mse 16.58 | mre  0.00 |
scGPT - INFO - | epoch  11 | 2100/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 17.39 | mse 17.39 | mre  0.00 |
scGPT - INFO - | epoch  11 | 2200/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 15.83 | mse 15.83 | mre  0.00 |
scGPT - INFO - | epoch  11 | 2300/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 16.45 | mse 16.45 | mre  0.00 |
scGPT - INFO - | epoch  11 | 2400/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 16.25 | mse 16.25 | mre  0.00 |
scGPT - INFO - | epoch  11 | 2500/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 15.56 | mse 15.56 | mre  0.00 |
scGPT - INFO - | epoch  11 | 2600/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 18.07 | mse 18.07 | mre  0.00 |
scGPT - INFO - | epoch  11 | 2700/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 16.92 | mse 16.92 | mre  0.00 |
scGPT - INFO - | epoch  11 | 2800/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 15.55 | mse 15.55 | mre  0.00 |
scGPT - INFO - | epoch  11 | 2900/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 16.52 | mse 16.52 | mre  0.00 |
scGPT - INFO - | epoch  11 | 3000/3602 batches | lr 0.0000 | ms/batch 91.29 | loss 17.39 | mse 17.39 | mre  0.00 |
scGPT - INFO - | epoch  11 | 3100/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 15.38 | mse 15.38 | mre  0.00 |
scGPT - INFO - | epoch  11 | 3200/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 16.45 | mse 16.45 | mre  0.00 |
scGPT - INFO - | epoch  11 | 3300/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 14.60 | mse 14.60 | mre  0.00 |
scGPT - INFO - | epoch  11 | 3400/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 15.39 | mse 15.39 | mre  0.00 |
scGPT - INFO - | epoch  11 | 3500/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 14.48 | mse 14.48 | mre  0.00 |
scGPT - INFO - | epoch  11 | 3600/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 18.45 | mse 18.45 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  11 | time: 339.46s | valid loss/mse 16.2410 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 16.2410
best_loss: 16.631407975852863, min_delta 0.0001, val_loss 16.24103046371398
Loss error: 0.3903775121388833
epoch:  11
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.215 3.738 3.453 ... 3.48  3.67  3.809]
 [6.543 3.67  3.377 ... 3.484 3.672 3.875]
 [4.66  4.023 4.105 ... 4.09  3.896 3.6  ]
 ...
 [4.965 4.03  4.137 ... 4.227 3.977 3.758]
 [4.98  4.04  4.14  ... 4.21  3.982 3.752]
 [6.594 3.686 3.408 ... 3.518 3.697 3.902]]
(801, 11) (801, 11)
By feature:  MSE:  0.7383¬±0.0 MAE:  0.584¬±0.0 R2:  -0.0139¬±0.0 PCC:  0.6935¬±0.0 Cosine Similarity:  0.9859¬±0.0
By sample:  MSE:  0.7383¬±0.0 MAE:  0.584¬±0.0 R2:  0.1211¬±0.0 PCC:  0.3534¬±0.0 Cosine Similarity:  0.9823¬±0.0
scGPT - INFO - By feature: MSE: 0.7383000254631042¬±0.0 MAE: 0.5839999914169312¬±0.0 R2: -0.0139¬±0.0 PCC: 0.6935¬±0.0 Cosine Similarity: 0.9859¬±0.0
scGPT - INFO - By sample: MSE: 0.7383000254631042¬±0.0 MAE: 0.5839999914169312¬±0.0 R2: 0.1211¬±0.0 PCC: 0.3534¬±0.0 Cosine Similarity: 0.9823¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  12
Training model
scGPT - INFO - | epoch  12 | 100/3602 batches | lr 0.0000 | ms/batch 92.10 | loss 15.93 | mse 15.93 | mre  0.00 |
scGPT - INFO - | epoch  12 | 200/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 16.63 | mse 16.63 | mre  0.00 |
scGPT - INFO - | epoch  12 | 300/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 13.50 | mse 13.50 | mre  0.00 |
scGPT - INFO - | epoch  12 | 400/3602 batches | lr 0.0000 | ms/batch 91.41 | loss 14.63 | mse 14.63 | mre  0.00 |
scGPT - INFO - | epoch  12 | 500/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 13.99 | mse 13.99 | mre  0.00 |
scGPT - INFO - | epoch  12 | 600/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 14.31 | mse 14.31 | mre  0.00 |
scGPT - INFO - | epoch  12 | 700/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 16.05 | mse 16.05 | mre  0.00 |
scGPT - INFO - | epoch  12 | 800/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 16.78 | mse 16.78 | mre  0.00 |
scGPT - INFO - | epoch  12 | 900/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 15.09 | mse 15.09 | mre  0.00 |
scGPT - INFO - | epoch  12 | 1000/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 16.41 | mse 16.41 | mre  0.00 |
scGPT - INFO - | epoch  12 | 1100/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 14.94 | mse 14.94 | mre  0.00 |
scGPT - INFO - | epoch  12 | 1200/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 15.36 | mse 15.36 | mre  0.00 |
scGPT - INFO - | epoch  12 | 1300/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 16.66 | mse 16.66 | mre  0.00 |
scGPT - INFO - | epoch  12 | 1400/3602 batches | lr 0.0000 | ms/batch 91.46 | loss 17.82 | mse 17.82 | mre  0.00 |
scGPT - INFO - | epoch  12 | 1500/3602 batches | lr 0.0000 | ms/batch 91.14 | loss 16.80 | mse 16.80 | mre  0.00 |
scGPT - INFO - | epoch  12 | 1600/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 13.41 | mse 13.41 | mre  0.00 |
scGPT - INFO - | epoch  12 | 1700/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 15.73 | mse 15.73 | mre  0.00 |
scGPT - INFO - | epoch  12 | 1800/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 14.09 | mse 14.09 | mre  0.00 |
scGPT - INFO - | epoch  12 | 1900/3602 batches | lr 0.0000 | ms/batch 91.04 | loss 18.31 | mse 18.31 | mre  0.00 |
scGPT - INFO - | epoch  12 | 2000/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 15.47 | mse 15.47 | mre  0.00 |
scGPT - INFO - | epoch  12 | 2100/3602 batches | lr 0.0000 | ms/batch 91.28 | loss 16.80 | mse 16.80 | mre  0.00 |
scGPT - INFO - | epoch  12 | 2200/3602 batches | lr 0.0000 | ms/batch 91.42 | loss 15.49 | mse 15.49 | mre  0.00 |
scGPT - INFO - | epoch  12 | 2300/3602 batches | lr 0.0000 | ms/batch 91.27 | loss 15.00 | mse 15.00 | mre  0.00 |
scGPT - INFO - | epoch  12 | 2400/3602 batches | lr 0.0000 | ms/batch 91.79 | loss 14.91 | mse 14.91 | mre  0.00 |
scGPT - INFO - | epoch  12 | 2500/3602 batches | lr 0.0000 | ms/batch 91.24 | loss 14.09 | mse 14.09 | mre  0.00 |
scGPT - INFO - | epoch  12 | 2600/3602 batches | lr 0.0000 | ms/batch 90.97 | loss 17.73 | mse 17.73 | mre  0.00 |
scGPT - INFO - | epoch  12 | 2700/3602 batches | lr 0.0000 | ms/batch 91.51 | loss 15.62 | mse 15.62 | mre  0.00 |
scGPT - INFO - | epoch  12 | 2800/3602 batches | lr 0.0000 | ms/batch 91.33 | loss 14.63 | mse 14.63 | mre  0.00 |
scGPT - INFO - | epoch  12 | 2900/3602 batches | lr 0.0000 | ms/batch 91.49 | loss 15.06 | mse 15.06 | mre  0.00 |
scGPT - INFO - | epoch  12 | 3000/3602 batches | lr 0.0000 | ms/batch 91.41 | loss 16.56 | mse 16.56 | mre  0.00 |
scGPT - INFO - | epoch  12 | 3100/3602 batches | lr 0.0000 | ms/batch 91.44 | loss 15.11 | mse 15.11 | mre  0.00 |
scGPT - INFO - | epoch  12 | 3200/3602 batches | lr 0.0000 | ms/batch 91.49 | loss 15.06 | mse 15.06 | mre  0.00 |
scGPT - INFO - | epoch  12 | 3300/3602 batches | lr 0.0000 | ms/batch 91.68 | loss 12.78 | mse 12.78 | mre  0.00 |
scGPT - INFO - | epoch  12 | 3400/3602 batches | lr 0.0000 | ms/batch 92.13 | loss 15.42 | mse 15.42 | mre  0.00 |
scGPT - INFO - | epoch  12 | 3500/3602 batches | lr 0.0000 | ms/batch 91.47 | loss 13.55 | mse 13.55 | mre  0.00 |
scGPT - INFO - | epoch  12 | 3600/3602 batches | lr 0.0000 | ms/batch 91.40 | loss 17.59 | mse 17.59 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  12 | time: 340.81s | valid loss/mse 15.4559 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 15.4559
best_loss: 16.24103046371398, min_delta 0.0001, val_loss 15.455885431181626
Loss error: 0.7851450325323537
epoch:  12
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[4.51  4.27  4.023 ... 3.506 3.797 3.535]
 [6.65  3.629 3.346 ... 3.486 3.65  3.896]
 [4.484 4.12  4.15  ... 4.05  3.898 3.555]
 ...
 [5.375 3.932 4.03  ... 4.27  3.95  3.799]
 [5.34  3.973 4.055 ... 4.25  3.963 3.775]
 [6.617 3.658 3.387 ... 3.512 3.67  3.898]]
(801, 11) (801, 11)
By feature:  MSE:  0.7026¬±0.0 MAE:  0.5667¬±0.0 R2:  0.0552¬±0.0 PCC:  0.715¬±0.0 Cosine Similarity:  0.9868¬±0.0
By sample:  MSE:  0.7026¬±0.0 MAE:  0.5667¬±0.0 R2:  0.1506¬±0.0 PCC:  0.3896¬±0.0 Cosine Similarity:  0.9828¬±0.0
scGPT - INFO - By feature: MSE: 0.7026000022888184¬±0.0 MAE: 0.5666999816894531¬±0.0 R2: 0.0552¬±0.0 PCC: 0.715¬±0.0 Cosine Similarity: 0.9868¬±0.0
scGPT - INFO - By sample: MSE: 0.7026000022888184¬±0.0 MAE: 0.5666999816894531¬±0.0 R2: 0.1506¬±0.0 PCC: 0.3896¬±0.0 Cosine Similarity: 0.9828¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  13
Training model
scGPT - INFO - | epoch  13 | 100/3602 batches | lr 0.0000 | ms/batch 92.96 | loss 15.20 | mse 15.20 | mre  0.00 |
scGPT - INFO - | epoch  13 | 200/3602 batches | lr 0.0000 | ms/batch 91.76 | loss 15.77 | mse 15.77 | mre  0.00 |
scGPT - INFO - | epoch  13 | 300/3602 batches | lr 0.0000 | ms/batch 91.82 | loss 13.18 | mse 13.18 | mre  0.00 |
scGPT - INFO - | epoch  13 | 400/3602 batches | lr 0.0000 | ms/batch 91.86 | loss 14.23 | mse 14.23 | mre  0.00 |
scGPT - INFO - | epoch  13 | 500/3602 batches | lr 0.0000 | ms/batch 92.05 | loss 13.60 | mse 13.60 | mre  0.00 |
scGPT - INFO - | epoch  13 | 600/3602 batches | lr 0.0000 | ms/batch 92.16 | loss 13.48 | mse 13.48 | mre  0.00 |
scGPT - INFO - | epoch  13 | 700/3602 batches | lr 0.0000 | ms/batch 92.19 | loss 15.31 | mse 15.31 | mre  0.00 |
scGPT - INFO - | epoch  13 | 800/3602 batches | lr 0.0000 | ms/batch 92.83 | loss 16.06 | mse 16.06 | mre  0.00 |
scGPT - INFO - | epoch  13 | 900/3602 batches | lr 0.0000 | ms/batch 91.88 | loss 14.58 | mse 14.58 | mre  0.00 |
scGPT - INFO - | epoch  13 | 1000/3602 batches | lr 0.0000 | ms/batch 92.05 | loss 15.96 | mse 15.96 | mre  0.00 |
scGPT - INFO - | epoch  13 | 1100/3602 batches | lr 0.0000 | ms/batch 91.76 | loss 13.84 | mse 13.84 | mre  0.00 |
scGPT - INFO - | epoch  13 | 1200/3602 batches | lr 0.0000 | ms/batch 91.53 | loss 14.63 | mse 14.63 | mre  0.00 |
scGPT - INFO - | epoch  13 | 1300/3602 batches | lr 0.0000 | ms/batch 91.51 | loss 15.93 | mse 15.93 | mre  0.00 |
scGPT - INFO - | epoch  13 | 1400/3602 batches | lr 0.0000 | ms/batch 91.74 | loss 17.44 | mse 17.44 | mre  0.00 |
scGPT - INFO - | epoch  13 | 1500/3602 batches | lr 0.0000 | ms/batch 91.67 | loss 16.33 | mse 16.33 | mre  0.00 |
scGPT - INFO - | epoch  13 | 1600/3602 batches | lr 0.0000 | ms/batch 91.96 | loss 12.79 | mse 12.79 | mre  0.00 |
scGPT - INFO - | epoch  13 | 1700/3602 batches | lr 0.0000 | ms/batch 91.92 | loss 15.29 | mse 15.29 | mre  0.00 |
scGPT - INFO - | epoch  13 | 1800/3602 batches | lr 0.0000 | ms/batch 92.52 | loss 13.54 | mse 13.54 | mre  0.00 |
scGPT - INFO - | epoch  13 | 1900/3602 batches | lr 0.0000 | ms/batch 92.12 | loss 16.42 | mse 16.42 | mre  0.00 |
scGPT - INFO - | epoch  13 | 2000/3602 batches | lr 0.0000 | ms/batch 91.94 | loss 14.87 | mse 14.87 | mre  0.00 |
scGPT - INFO - | epoch  13 | 2100/3602 batches | lr 0.0000 | ms/batch 92.07 | loss 16.57 | mse 16.57 | mre  0.00 |
scGPT - INFO - | epoch  13 | 2200/3602 batches | lr 0.0000 | ms/batch 93.58 | loss 15.04 | mse 15.04 | mre  0.00 |
scGPT - INFO - | epoch  13 | 2300/3602 batches | lr 0.0000 | ms/batch 93.89 | loss 13.69 | mse 13.69 | mre  0.00 |
scGPT - INFO - | epoch  13 | 2400/3602 batches | lr 0.0000 | ms/batch 93.76 | loss 13.96 | mse 13.96 | mre  0.00 |
scGPT - INFO - | epoch  13 | 2500/3602 batches | lr 0.0000 | ms/batch 93.42 | loss 13.38 | mse 13.38 | mre  0.00 |
scGPT - INFO - | epoch  13 | 2600/3602 batches | lr 0.0000 | ms/batch 93.47 | loss 17.43 | mse 17.43 | mre  0.00 |
scGPT - INFO - | epoch  13 | 2700/3602 batches | lr 0.0000 | ms/batch 93.61 | loss 14.80 | mse 14.80 | mre  0.00 |
scGPT - INFO - | epoch  13 | 2800/3602 batches | lr 0.0000 | ms/batch 94.76 | loss 14.22 | mse 14.22 | mre  0.00 |
scGPT - INFO - | epoch  13 | 2900/3602 batches | lr 0.0000 | ms/batch 93.75 | loss 14.31 | mse 14.31 | mre  0.00 |
scGPT - INFO - | epoch  13 | 3000/3602 batches | lr 0.0000 | ms/batch 93.49 | loss 16.04 | mse 16.04 | mre  0.00 |
scGPT - INFO - | epoch  13 | 3100/3602 batches | lr 0.0000 | ms/batch 92.82 | loss 14.53 | mse 14.53 | mre  0.00 |
scGPT - INFO - | epoch  13 | 3200/3602 batches | lr 0.0000 | ms/batch 93.14 | loss 14.70 | mse 14.70 | mre  0.00 |
scGPT - INFO - | epoch  13 | 3300/3602 batches | lr 0.0000 | ms/batch 93.26 | loss 12.45 | mse 12.45 | mre  0.00 |
scGPT - INFO - | epoch  13 | 3400/3602 batches | lr 0.0000 | ms/batch 93.09 | loss 14.63 | mse 14.63 | mre  0.00 |
scGPT - INFO - | epoch  13 | 3500/3602 batches | lr 0.0000 | ms/batch 92.86 | loss 13.18 | mse 13.18 | mre  0.00 |
scGPT - INFO - | epoch  13 | 3600/3602 batches | lr 0.0000 | ms/batch 93.06 | loss 16.62 | mse 16.62 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  13 | time: 346.10s | valid loss/mse 14.2014 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 14.2014
best_loss: 15.455885431181626, min_delta 0.0001, val_loss 14.201426519213545
Loss error: 1.2544589119680811
epoch:  13
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[4.508 4.28  4.137 ... 3.64  3.84  3.58 ]
 [6.758 3.64  3.367 ... 3.5   3.678 3.922]
 [3.812 4.41  4.44  ... 3.857 3.979 3.47 ]
 ...
 [5.254 3.871 3.908 ... 4.18  3.865 3.713]
 [5.176 3.973 4.    ... 4.195 3.908 3.674]
 [6.72  3.662 3.395 ... 3.518 3.691 3.92 ]]
(801, 11) (801, 11)
By feature:  MSE:  0.6456¬±0.0 MAE:  0.5304¬±0.0 R2:  0.106¬±0.0 PCC:  0.7552¬±0.0 Cosine Similarity:  0.9882¬±0.0
By sample:  MSE:  0.6456¬±0.0 MAE:  0.5304¬±0.0 R2:  0.2051¬±0.0 PCC:  0.4392¬±0.0 Cosine Similarity:  0.9838¬±0.0
scGPT - INFO - By feature: MSE: 0.6456000208854675¬±0.0 MAE: 0.5303999781608582¬±0.0 R2: 0.106¬±0.0 PCC: 0.7552¬±0.0 Cosine Similarity: 0.9882¬±0.0
scGPT - INFO - By sample: MSE: 0.6456000208854675¬±0.0 MAE: 0.5303999781608582¬±0.0 R2: 0.2051¬±0.0 PCC: 0.4392¬±0.0 Cosine Similarity: 0.9838¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  14
Training model
scGPT - INFO - | epoch  14 | 100/3602 batches | lr 0.0000 | ms/batch 94.02 | loss 15.17 | mse 15.17 | mre  0.00 |
scGPT - INFO - | epoch  14 | 200/3602 batches | lr 0.0000 | ms/batch 92.89 | loss 14.48 | mse 14.48 | mre  0.00 |
scGPT - INFO - | epoch  14 | 300/3602 batches | lr 0.0000 | ms/batch 92.18 | loss 12.64 | mse 12.64 | mre  0.00 |
scGPT - INFO - | epoch  14 | 400/3602 batches | lr 0.0000 | ms/batch 91.41 | loss 13.40 | mse 13.40 | mre  0.00 |
scGPT - INFO - | epoch  14 | 500/3602 batches | lr 0.0000 | ms/batch 91.63 | loss 13.05 | mse 13.05 | mre  0.00 |
scGPT - INFO - | epoch  14 | 600/3602 batches | lr 0.0000 | ms/batch 92.04 | loss 13.29 | mse 13.29 | mre  0.00 |
scGPT - INFO - | epoch  14 | 700/3602 batches | lr 0.0000 | ms/batch 92.05 | loss 14.77 | mse 14.77 | mre  0.00 |
scGPT - INFO - | epoch  14 | 800/3602 batches | lr 0.0000 | ms/batch 91.71 | loss 16.06 | mse 16.06 | mre  0.00 |
scGPT - INFO - | epoch  14 | 900/3602 batches | lr 0.0000 | ms/batch 91.56 | loss 14.12 | mse 14.12 | mre  0.00 |
scGPT - INFO - | epoch  14 | 1000/3602 batches | lr 0.0000 | ms/batch 91.68 | loss 15.47 | mse 15.47 | mre  0.00 |
scGPT - INFO - | epoch  14 | 1100/3602 batches | lr 0.0000 | ms/batch 91.68 | loss 13.51 | mse 13.51 | mre  0.00 |
scGPT - INFO - | epoch  14 | 1200/3602 batches | lr 0.0000 | ms/batch 92.20 | loss 14.36 | mse 14.36 | mre  0.00 |
scGPT - INFO - | epoch  14 | 1300/3602 batches | lr 0.0000 | ms/batch 91.83 | loss 15.14 | mse 15.14 | mre  0.00 |
scGPT - INFO - | epoch  14 | 1400/3602 batches | lr 0.0000 | ms/batch 91.72 | loss 16.75 | mse 16.75 | mre  0.00 |
scGPT - INFO - | epoch  14 | 1500/3602 batches | lr 0.0000 | ms/batch 92.18 | loss 16.42 | mse 16.42 | mre  0.00 |
scGPT - INFO - | epoch  14 | 1600/3602 batches | lr 0.0000 | ms/batch 92.84 | loss 12.46 | mse 12.46 | mre  0.00 |
scGPT - INFO - | epoch  14 | 1700/3602 batches | lr 0.0000 | ms/batch 92.63 | loss 14.07 | mse 14.07 | mre  0.00 |
scGPT - INFO - | epoch  14 | 1800/3602 batches | lr 0.0000 | ms/batch 92.52 | loss 13.14 | mse 13.14 | mre  0.00 |
scGPT - INFO - | epoch  14 | 1900/3602 batches | lr 0.0000 | ms/batch 90.99 | loss 16.52 | mse 16.52 | mre  0.00 |
scGPT - INFO - | epoch  14 | 2000/3602 batches | lr 0.0000 | ms/batch 90.91 | loss 14.36 | mse 14.36 | mre  0.00 |
scGPT - INFO - | epoch  14 | 2100/3602 batches | lr 0.0000 | ms/batch 91.00 | loss 16.25 | mse 16.25 | mre  0.00 |
scGPT - INFO - | epoch  14 | 2200/3602 batches | lr 0.0000 | ms/batch 91.66 | loss 14.20 | mse 14.20 | mre  0.00 |
scGPT - INFO - | epoch  14 | 2300/3602 batches | lr 0.0000 | ms/batch 91.03 | loss 13.73 | mse 13.73 | mre  0.00 |
scGPT - INFO - | epoch  14 | 2400/3602 batches | lr 0.0000 | ms/batch 91.06 | loss 13.89 | mse 13.89 | mre  0.00 |
scGPT - INFO - | epoch  14 | 2500/3602 batches | lr 0.0000 | ms/batch 91.05 | loss 13.43 | mse 13.43 | mre  0.00 |
scGPT - INFO - | epoch  14 | 2600/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 17.00 | mse 17.00 | mre  0.00 |
scGPT - INFO - | epoch  14 | 2700/3602 batches | lr 0.0000 | ms/batch 91.00 | loss 14.62 | mse 14.62 | mre  0.00 |
scGPT - INFO - | epoch  14 | 2800/3602 batches | lr 0.0000 | ms/batch 91.20 | loss 14.04 | mse 14.04 | mre  0.00 |
scGPT - INFO - | epoch  14 | 2900/3602 batches | lr 0.0000 | ms/batch 90.98 | loss 13.69 | mse 13.69 | mre  0.00 |
scGPT - INFO - | epoch  14 | 3000/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 15.46 | mse 15.46 | mre  0.00 |
scGPT - INFO - | epoch  14 | 3100/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 14.23 | mse 14.23 | mre  0.00 |
scGPT - INFO - | epoch  14 | 3200/3602 batches | lr 0.0000 | ms/batch 95.98 | loss 14.32 | mse 14.32 | mre  0.00 |
scGPT - INFO - | epoch  14 | 3300/3602 batches | lr 0.0000 | ms/batch 91.13 | loss 11.86 | mse 11.86 | mre  0.00 |
scGPT - INFO - | epoch  14 | 3400/3602 batches | lr 0.0000 | ms/batch 91.47 | loss 14.32 | mse 14.32 | mre  0.00 |
scGPT - INFO - | epoch  14 | 3500/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 12.68 | mse 12.68 | mre  0.00 |
scGPT - INFO - | epoch  14 | 3600/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 16.29 | mse 16.29 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  14 | time: 342.74s | valid loss/mse 13.8738 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 13.8738
best_loss: 14.201426519213545, min_delta 0.0001, val_loss 13.873839257808214
Loss error: 0.32758726140533057
epoch:  14
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[5.47  3.922 3.734 ... 3.586 3.707 3.713]
 [6.758 3.615 3.348 ... 3.486 3.652 3.906]
 [3.96  4.246 4.305 ... 3.89  3.906 3.438]
 ...
 [5.062 3.826 3.89  ... 4.164 3.822 3.678]
 [4.957 3.918 3.982 ... 4.17  3.865 3.639]
 [6.707 3.639 3.377 ... 3.504 3.666 3.902]]
(801, 11) (801, 11)
By feature:  MSE:  0.6307¬±0.0 MAE:  0.5205¬±0.0 R2:  0.1394¬±0.0 PCC:  0.7651¬±0.0 Cosine Similarity:  0.9885¬±0.0
By sample:  MSE:  0.6307¬±0.0 MAE:  0.5205¬±0.0 R2:  0.2195¬±0.0 PCC:  0.4541¬±0.0 Cosine Similarity:  0.984¬±0.0
scGPT - INFO - By feature: MSE: 0.6306999921798706¬±0.0 MAE: 0.5205000042915344¬±0.0 R2: 0.1394¬±0.0 PCC: 0.7651¬±0.0 Cosine Similarity: 0.9885¬±0.0
scGPT - INFO - By sample: MSE: 0.6306999921798706¬±0.0 MAE: 0.5205000042915344¬±0.0 R2: 0.2195¬±0.0 PCC: 0.4541¬±0.0 Cosine Similarity: 0.984¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  15
Training model
scGPT - INFO - | epoch  15 | 100/3602 batches | lr 0.0000 | ms/batch 92.26 | loss 15.18 | mse 15.18 | mre  0.00 |
scGPT - INFO - | epoch  15 | 200/3602 batches | lr 0.0000 | ms/batch 91.03 | loss 14.12 | mse 14.12 | mre  0.00 |
scGPT - INFO - | epoch  15 | 300/3602 batches | lr 0.0000 | ms/batch 91.90 | loss 12.22 | mse 12.22 | mre  0.00 |
scGPT - INFO - | epoch  15 | 400/3602 batches | lr 0.0000 | ms/batch 91.91 | loss 13.15 | mse 13.15 | mre  0.00 |
scGPT - INFO - | epoch  15 | 500/3602 batches | lr 0.0000 | ms/batch 91.26 | loss 12.56 | mse 12.56 | mre  0.00 |
scGPT - INFO - | epoch  15 | 600/3602 batches | lr 0.0000 | ms/batch 91.90 | loss 13.22 | mse 13.22 | mre  0.00 |
scGPT - INFO - | epoch  15 | 700/3602 batches | lr 0.0000 | ms/batch 91.26 | loss 14.50 | mse 14.50 | mre  0.00 |
scGPT - INFO - | epoch  15 | 800/3602 batches | lr 0.0000 | ms/batch 91.21 | loss 15.37 | mse 15.37 | mre  0.00 |
scGPT - INFO - | epoch  15 | 900/3602 batches | lr 0.0000 | ms/batch 91.17 | loss 13.77 | mse 13.77 | mre  0.00 |
scGPT - INFO - | epoch  15 | 1000/3602 batches | lr 0.0000 | ms/batch 91.20 | loss 15.18 | mse 15.18 | mre  0.00 |
scGPT - INFO - | epoch  15 | 1100/3602 batches | lr 0.0000 | ms/batch 91.11 | loss 13.06 | mse 13.06 | mre  0.00 |
scGPT - INFO - | epoch  15 | 1200/3602 batches | lr 0.0000 | ms/batch 91.15 | loss 13.72 | mse 13.72 | mre  0.00 |
scGPT - INFO - | epoch  15 | 1300/3602 batches | lr 0.0000 | ms/batch 91.12 | loss 15.01 | mse 15.01 | mre  0.00 |
scGPT - INFO - | epoch  15 | 1400/3602 batches | lr 0.0000 | ms/batch 91.05 | loss 15.97 | mse 15.97 | mre  0.00 |
scGPT - INFO - | epoch  15 | 1500/3602 batches | lr 0.0000 | ms/batch 91.02 | loss 15.87 | mse 15.87 | mre  0.00 |
scGPT - INFO - | epoch  15 | 1600/3602 batches | lr 0.0000 | ms/batch 91.68 | loss 12.14 | mse 12.14 | mre  0.00 |
scGPT - INFO - | epoch  15 | 1700/3602 batches | lr 0.0000 | ms/batch 91.06 | loss 13.57 | mse 13.57 | mre  0.00 |
scGPT - INFO - | epoch  15 | 1800/3602 batches | lr 0.0000 | ms/batch 91.20 | loss 13.21 | mse 13.21 | mre  0.00 |
scGPT - INFO - | epoch  15 | 1900/3602 batches | lr 0.0000 | ms/batch 91.03 | loss 15.72 | mse 15.72 | mre  0.00 |
scGPT - INFO - | epoch  15 | 2000/3602 batches | lr 0.0000 | ms/batch 91.04 | loss 14.44 | mse 14.44 | mre  0.00 |
scGPT - INFO - | epoch  15 | 2100/3602 batches | lr 0.0000 | ms/batch 91.17 | loss 15.96 | mse 15.96 | mre  0.00 |
scGPT - INFO - | epoch  15 | 2200/3602 batches | lr 0.0000 | ms/batch 91.22 | loss 14.06 | mse 14.06 | mre  0.00 |
scGPT - INFO - | epoch  15 | 2300/3602 batches | lr 0.0000 | ms/batch 91.51 | loss 12.90 | mse 12.90 | mre  0.00 |
scGPT - INFO - | epoch  15 | 2400/3602 batches | lr 0.0000 | ms/batch 91.44 | loss 13.31 | mse 13.31 | mre  0.00 |
scGPT - INFO - | epoch  15 | 2500/3602 batches | lr 0.0000 | ms/batch 91.12 | loss 13.18 | mse 13.18 | mre  0.00 |
scGPT - INFO - | epoch  15 | 2600/3602 batches | lr 0.0000 | ms/batch 91.60 | loss 16.58 | mse 16.58 | mre  0.00 |
scGPT - INFO - | epoch  15 | 2700/3602 batches | lr 0.0000 | ms/batch 91.17 | loss 14.36 | mse 14.36 | mre  0.00 |
scGPT - INFO - | epoch  15 | 2800/3602 batches | lr 0.0000 | ms/batch 91.12 | loss 13.74 | mse 13.74 | mre  0.00 |
scGPT - INFO - | epoch  15 | 2900/3602 batches | lr 0.0000 | ms/batch 91.36 | loss 13.48 | mse 13.48 | mre  0.00 |
scGPT - INFO - | epoch  15 | 3000/3602 batches | lr 0.0000 | ms/batch 91.08 | loss 15.08 | mse 15.08 | mre  0.00 |
scGPT - INFO - | epoch  15 | 3100/3602 batches | lr 0.0000 | ms/batch 91.20 | loss 13.81 | mse 13.81 | mre  0.00 |
scGPT - INFO - | epoch  15 | 3200/3602 batches | lr 0.0000 | ms/batch 91.34 | loss 14.21 | mse 14.21 | mre  0.00 |
scGPT - INFO - | epoch  15 | 3300/3602 batches | lr 0.0000 | ms/batch 91.46 | loss 11.60 | mse 11.60 | mre  0.00 |
scGPT - INFO - | epoch  15 | 3400/3602 batches | lr 0.0000 | ms/batch 92.43 | loss 13.78 | mse 13.78 | mre  0.00 |
scGPT - INFO - | epoch  15 | 3500/3602 batches | lr 0.0000 | ms/batch 92.06 | loss 12.17 | mse 12.17 | mre  0.00 |
scGPT - INFO - | epoch  15 | 3600/3602 batches | lr 0.0000 | ms/batch 92.09 | loss 15.47 | mse 15.47 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  15 | time: 341.65s | valid loss/mse 13.5611 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 13.5611
best_loss: 13.873839257808214, min_delta 0.0001, val_loss 13.561064930518766
Loss error: 0.3127743272894481
epoch:  15
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[5.168 3.9   3.746 ... 3.61  3.678 3.66 ]
 [6.746 3.605 3.348 ... 3.477 3.652 3.893]
 [4.56  3.812 3.906 ... 3.996 3.736 3.527]
 ...
 [4.992 3.775 3.871 ... 4.098 3.787 3.623]
 [4.945 3.902 3.998 ... 4.125 3.863 3.613]
 [6.71  3.625 3.373 ... 3.5   3.664 3.895]]
(801, 11) (801, 11)
By feature:  MSE:  0.6164¬±0.0 MAE:  0.5152¬±0.0 R2:  0.1538¬±0.0 PCC:  0.7673¬±0.0 Cosine Similarity:  0.9888¬±0.0
By sample:  MSE:  0.6164¬±0.0 MAE:  0.5152¬±0.0 R2:  0.2358¬±0.0 PCC:  0.4697¬±0.0 Cosine Similarity:  0.9844¬±0.0
scGPT - INFO - By feature: MSE: 0.6164000034332275¬±0.0 MAE: 0.5152000188827515¬±0.0 R2: 0.1538¬±0.0 PCC: 0.7673¬±0.0 Cosine Similarity: 0.9888¬±0.0
scGPT - INFO - By sample: MSE: 0.6164000034332275¬±0.0 MAE: 0.5152000188827515¬±0.0 R2: 0.2358¬±0.0 PCC: 0.4697¬±0.0 Cosine Similarity: 0.9844¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  16
Training model
scGPT - INFO - | epoch  16 | 100/3602 batches | lr 0.0000 | ms/batch 92.30 | loss 14.59 | mse 14.59 | mre  0.00 |
scGPT - INFO - | epoch  16 | 200/3602 batches | lr 0.0000 | ms/batch 91.17 | loss 14.24 | mse 14.24 | mre  0.00 |
scGPT - INFO - | epoch  16 | 300/3602 batches | lr 0.0000 | ms/batch 91.47 | loss 12.62 | mse 12.62 | mre  0.00 |
scGPT - INFO - | epoch  16 | 400/3602 batches | lr 0.0000 | ms/batch 91.07 | loss 13.04 | mse 13.04 | mre  0.00 |
scGPT - INFO - | epoch  16 | 500/3602 batches | lr 0.0000 | ms/batch 91.05 | loss 12.22 | mse 12.22 | mre  0.00 |
scGPT - INFO - | epoch  16 | 600/3602 batches | lr 0.0000 | ms/batch 91.13 | loss 12.82 | mse 12.82 | mre  0.00 |
scGPT - INFO - | epoch  16 | 700/3602 batches | lr 0.0000 | ms/batch 91.21 | loss 14.63 | mse 14.63 | mre  0.00 |
scGPT - INFO - | epoch  16 | 800/3602 batches | lr 0.0000 | ms/batch 91.13 | loss 15.21 | mse 15.21 | mre  0.00 |
scGPT - INFO - | epoch  16 | 900/3602 batches | lr 0.0000 | ms/batch 91.38 | loss 13.32 | mse 13.32 | mre  0.00 |
scGPT - INFO - | epoch  16 | 1000/3602 batches | lr 0.0000 | ms/batch 91.69 | loss 14.89 | mse 14.89 | mre  0.00 |
scGPT - INFO - | epoch  16 | 1100/3602 batches | lr 0.0000 | ms/batch 91.27 | loss 12.83 | mse 12.83 | mre  0.00 |
scGPT - INFO - | epoch  16 | 1200/3602 batches | lr 0.0000 | ms/batch 91.29 | loss 14.18 | mse 14.18 | mre  0.00 |
scGPT - INFO - | epoch  16 | 1300/3602 batches | lr 0.0000 | ms/batch 91.25 | loss 14.24 | mse 14.24 | mre  0.00 |
scGPT - INFO - | epoch  16 | 1400/3602 batches | lr 0.0000 | ms/batch 91.20 | loss 15.54 | mse 15.54 | mre  0.00 |
scGPT - INFO - | epoch  16 | 1500/3602 batches | lr 0.0000 | ms/batch 91.18 | loss 15.39 | mse 15.39 | mre  0.00 |
scGPT - INFO - | epoch  16 | 1600/3602 batches | lr 0.0000 | ms/batch 91.09 | loss 12.17 | mse 12.17 | mre  0.00 |
scGPT - INFO - | epoch  16 | 1700/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 13.03 | mse 13.03 | mre  0.00 |
scGPT - INFO - | epoch  16 | 1800/3602 batches | lr 0.0000 | ms/batch 91.10 | loss 12.44 | mse 12.44 | mre  0.00 |
scGPT - INFO - | epoch  16 | 1900/3602 batches | lr 0.0000 | ms/batch 91.29 | loss 15.57 | mse 15.57 | mre  0.00 |
scGPT - INFO - | epoch  16 | 2000/3602 batches | lr 0.0000 | ms/batch 91.57 | loss 13.36 | mse 13.36 | mre  0.00 |
scGPT - INFO - | epoch  16 | 2100/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 15.61 | mse 15.61 | mre  0.00 |
scGPT - INFO - | epoch  16 | 2200/3602 batches | lr 0.0000 | ms/batch 91.32 | loss 14.08 | mse 14.08 | mre  0.00 |
scGPT - INFO - | epoch  16 | 2300/3602 batches | lr 0.0000 | ms/batch 91.04 | loss 12.85 | mse 12.85 | mre  0.00 |
scGPT - INFO - | epoch  16 | 2400/3602 batches | lr 0.0000 | ms/batch 92.63 | loss 13.24 | mse 13.24 | mre  0.00 |
scGPT - INFO - | epoch  16 | 2500/3602 batches | lr 0.0000 | ms/batch 93.18 | loss 12.63 | mse 12.63 | mre  0.00 |
scGPT - INFO - | epoch  16 | 2600/3602 batches | lr 0.0000 | ms/batch 93.32 | loss 16.14 | mse 16.14 | mre  0.00 |
scGPT - INFO - | epoch  16 | 2700/3602 batches | lr 0.0000 | ms/batch 91.61 | loss 14.41 | mse 14.41 | mre  0.00 |
scGPT - INFO - | epoch  16 | 2800/3602 batches | lr 0.0000 | ms/batch 91.14 | loss 13.62 | mse 13.62 | mre  0.00 |
scGPT - INFO - | epoch  16 | 2900/3602 batches | lr 0.0000 | ms/batch 91.06 | loss 13.47 | mse 13.47 | mre  0.00 |
scGPT - INFO - | epoch  16 | 3000/3602 batches | lr 0.0000 | ms/batch 91.62 | loss 14.27 | mse 14.27 | mre  0.00 |
scGPT - INFO - | epoch  16 | 3100/3602 batches | lr 0.0000 | ms/batch 91.12 | loss 13.44 | mse 13.44 | mre  0.00 |
scGPT - INFO - | epoch  16 | 3200/3602 batches | lr 0.0000 | ms/batch 91.09 | loss 14.15 | mse 14.15 | mre  0.00 |
scGPT - INFO - | epoch  16 | 3300/3602 batches | lr 0.0000 | ms/batch 91.15 | loss 11.35 | mse 11.35 | mre  0.00 |
scGPT - INFO - | epoch  16 | 3400/3602 batches | lr 0.0000 | ms/batch 91.07 | loss 13.73 | mse 13.73 | mre  0.00 |
scGPT - INFO - | epoch  16 | 3500/3602 batches | lr 0.0000 | ms/batch 91.10 | loss 11.94 | mse 11.94 | mre  0.00 |
scGPT - INFO - | epoch  16 | 3600/3602 batches | lr 0.0000 | ms/batch 91.04 | loss 15.07 | mse 15.07 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  16 | time: 341.57s | valid loss/mse 13.7979 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 13.561064930518766, min_delta 0.0001, val_loss 13.797865536105768
Loss error: -0.23680060558700156
scGPT - INFO - INFO: Early stopping counter 1 of 10
epoch:  16
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[5.625 3.838 3.695 ... 3.693 3.703 3.793]
 [6.85  3.578 3.342 ... 3.482 3.67  3.924]
 [4.76  3.688 3.787 ... 3.996 3.678 3.566]
 ...
 [5.01  3.764 3.873 ... 4.094 3.78  3.64 ]
 [4.97  3.87  3.967 ... 4.1   3.844 3.621]
 [6.773 3.611 3.377 ... 3.516 3.682 3.922]]
(801, 11) (801, 11)
By feature:  MSE:  0.6272¬±0.0 MAE:  0.5214¬±0.0 R2:  0.1433¬±0.0 PCC:  0.7656¬±0.0 Cosine Similarity:  0.9885¬±0.0
By sample:  MSE:  0.6272¬±0.0 MAE:  0.5214¬±0.0 R2:  0.2314¬±0.0 PCC:  0.4721¬±0.0 Cosine Similarity:  0.9843¬±0.0
scGPT - INFO - By feature: MSE: 0.6272000074386597¬±0.0 MAE: 0.521399974822998¬±0.0 R2: 0.1433¬±0.0 PCC: 0.7656¬±0.0 Cosine Similarity: 0.9885¬±0.0
scGPT - INFO - By sample: MSE: 0.6272000074386597¬±0.0 MAE: 0.521399974822998¬±0.0 R2: 0.2314¬±0.0 PCC: 0.4721¬±0.0 Cosine Similarity: 0.9843¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  17
Training model
scGPT - INFO - | epoch  17 | 100/3602 batches | lr 0.0000 | ms/batch 92.36 | loss 14.52 | mse 14.52 | mre  0.00 |
scGPT - INFO - | epoch  17 | 200/3602 batches | lr 0.0000 | ms/batch 91.01 | loss 14.16 | mse 14.16 | mre  0.00 |
scGPT - INFO - | epoch  17 | 300/3602 batches | lr 0.0000 | ms/batch 90.95 | loss 11.81 | mse 11.81 | mre  0.00 |
scGPT - INFO - | epoch  17 | 400/3602 batches | lr 0.0000 | ms/batch 91.54 | loss 12.78 | mse 12.78 | mre  0.00 |
scGPT - INFO - | epoch  17 | 500/3602 batches | lr 0.0000 | ms/batch 91.12 | loss 12.21 | mse 12.21 | mre  0.00 |
scGPT - INFO - | epoch  17 | 600/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 12.45 | mse 12.45 | mre  0.00 |
scGPT - INFO - | epoch  17 | 700/3602 batches | lr 0.0000 | ms/batch 90.99 | loss 14.09 | mse 14.09 | mre  0.00 |
scGPT - INFO - | epoch  17 | 800/3602 batches | lr 0.0000 | ms/batch 91.04 | loss 15.14 | mse 15.14 | mre  0.00 |
scGPT - INFO - | epoch  17 | 900/3602 batches | lr 0.0000 | ms/batch 91.03 | loss 13.29 | mse 13.29 | mre  0.00 |
scGPT - INFO - | epoch  17 | 1000/3602 batches | lr 0.0000 | ms/batch 91.05 | loss 14.58 | mse 14.58 | mre  0.00 |
scGPT - INFO - | epoch  17 | 1100/3602 batches | lr 0.0000 | ms/batch 91.10 | loss 12.74 | mse 12.74 | mre  0.00 |
scGPT - INFO - | epoch  17 | 1200/3602 batches | lr 0.0000 | ms/batch 91.08 | loss 13.65 | mse 13.65 | mre  0.00 |
scGPT - INFO - | epoch  17 | 1300/3602 batches | lr 0.0000 | ms/batch 92.46 | loss 14.05 | mse 14.05 | mre  0.00 |
scGPT - INFO - | epoch  17 | 1400/3602 batches | lr 0.0000 | ms/batch 93.58 | loss 15.49 | mse 15.49 | mre  0.00 |
scGPT - INFO - | epoch  17 | 1500/3602 batches | lr 0.0000 | ms/batch 93.04 | loss 15.38 | mse 15.38 | mre  0.00 |
scGPT - INFO - | epoch  17 | 1600/3602 batches | lr 0.0000 | ms/batch 93.03 | loss 11.71 | mse 11.71 | mre  0.00 |
scGPT - INFO - | epoch  17 | 1700/3602 batches | lr 0.0000 | ms/batch 91.21 | loss 13.06 | mse 13.06 | mre  0.00 |
scGPT - INFO - | epoch  17 | 1800/3602 batches | lr 0.0000 | ms/batch 91.10 | loss 12.55 | mse 12.55 | mre  0.00 |
scGPT - INFO - | epoch  17 | 1900/3602 batches | lr 0.0000 | ms/batch 91.06 | loss 14.94 | mse 14.94 | mre  0.00 |
scGPT - INFO - | epoch  17 | 2000/3602 batches | lr 0.0000 | ms/batch 91.01 | loss 13.49 | mse 13.49 | mre  0.00 |
scGPT - INFO - | epoch  17 | 2100/3602 batches | lr 0.0000 | ms/batch 91.06 | loss 15.34 | mse 15.34 | mre  0.00 |
scGPT - INFO - | epoch  17 | 2200/3602 batches | lr 0.0000 | ms/batch 91.10 | loss 13.73 | mse 13.73 | mre  0.00 |
scGPT - INFO - | epoch  17 | 2300/3602 batches | lr 0.0000 | ms/batch 91.10 | loss 12.72 | mse 12.72 | mre  0.00 |
scGPT - INFO - | epoch  17 | 2400/3602 batches | lr 0.0000 | ms/batch 91.72 | loss 13.33 | mse 13.33 | mre  0.00 |
scGPT - INFO - | epoch  17 | 2500/3602 batches | lr 0.0000 | ms/batch 91.09 | loss 12.84 | mse 12.84 | mre  0.00 |
scGPT - INFO - | epoch  17 | 2600/3602 batches | lr 0.0000 | ms/batch 91.10 | loss 16.58 | mse 16.58 | mre  0.00 |
scGPT - INFO - | epoch  17 | 2700/3602 batches | lr 0.0000 | ms/batch 91.12 | loss 13.37 | mse 13.37 | mre  0.00 |
scGPT - INFO - | epoch  17 | 2800/3602 batches | lr 0.0000 | ms/batch 91.07 | loss 13.47 | mse 13.47 | mre  0.00 |
scGPT - INFO - | epoch  17 | 2900/3602 batches | lr 0.0000 | ms/batch 91.03 | loss 13.35 | mse 13.35 | mre  0.00 |
scGPT - INFO - | epoch  17 | 3000/3602 batches | lr 0.0000 | ms/batch 91.11 | loss 13.86 | mse 13.86 | mre  0.00 |
scGPT - INFO - | epoch  17 | 3100/3602 batches | lr 0.0000 | ms/batch 91.04 | loss 13.21 | mse 13.21 | mre  0.00 |
scGPT - INFO - | epoch  17 | 3200/3602 batches | lr 0.0000 | ms/batch 91.01 | loss 13.68 | mse 13.68 | mre  0.00 |
scGPT - INFO - | epoch  17 | 3300/3602 batches | lr 0.0000 | ms/batch 91.03 | loss 11.53 | mse 11.53 | mre  0.00 |
scGPT - INFO - | epoch  17 | 3400/3602 batches | lr 0.0000 | ms/batch 91.66 | loss 13.87 | mse 13.87 | mre  0.00 |
scGPT - INFO - | epoch  17 | 3500/3602 batches | lr 0.0000 | ms/batch 91.26 | loss 11.93 | mse 11.93 | mre  0.00 |
scGPT - INFO - | epoch  17 | 3600/3602 batches | lr 0.0000 | ms/batch 91.25 | loss 14.76 | mse 14.76 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  17 | time: 341.48s | valid loss/mse 13.0732 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 13.0732
best_loss: 13.561064930518766, min_delta 0.0001, val_loss 13.073202234975408
Loss error: 0.4878626955433578
epoch:  17
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[5.484 3.898 3.732 ... 3.617 3.709 3.775]
 [6.81  3.635 3.383 ... 3.498 3.684 3.936]
 [4.203 3.934 4.027 ... 3.844 3.773 3.432]
 ...
 [4.95  3.771 3.885 ... 4.07  3.781 3.643]
 [4.914 3.928 4.043 ... 4.12  3.895 3.63 ]
 [6.76  3.664 3.408 ... 3.52  3.703 3.936]]
(801, 11) (801, 11)
By feature:  MSE:  0.5943¬±0.0 MAE:  0.5091¬±0.0 R2:  0.2002¬±0.0 PCC:  0.7807¬±0.0 Cosine Similarity:  0.9893¬±0.0
By sample:  MSE:  0.5943¬±0.0 MAE:  0.5091¬±0.0 R2:  0.2569¬±0.0 PCC:  0.4906¬±0.0 Cosine Similarity:  0.9848¬±0.0
scGPT - INFO - By feature: MSE: 0.5942999720573425¬±0.0 MAE: 0.5091000199317932¬±0.0 R2: 0.2002¬±0.0 PCC: 0.7807¬±0.0 Cosine Similarity: 0.9893¬±0.0
scGPT - INFO - By sample: MSE: 0.5942999720573425¬±0.0 MAE: 0.5091000199317932¬±0.0 R2: 0.2569¬±0.0 PCC: 0.4906¬±0.0 Cosine Similarity: 0.9848¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  18
Training model
scGPT - INFO - | epoch  18 | 100/3602 batches | lr 0.0000 | ms/batch 92.51 | loss 13.65 | mse 13.65 | mre  0.00 |
scGPT - INFO - | epoch  18 | 200/3602 batches | lr 0.0000 | ms/batch 91.15 | loss 13.90 | mse 13.90 | mre  0.00 |
scGPT - INFO - | epoch  18 | 300/3602 batches | lr 0.0000 | ms/batch 91.27 | loss 11.59 | mse 11.59 | mre  0.00 |
scGPT - INFO - | epoch  18 | 400/3602 batches | lr 0.0000 | ms/batch 91.20 | loss 12.23 | mse 12.23 | mre  0.00 |
scGPT - INFO - | epoch  18 | 500/3602 batches | lr 0.0000 | ms/batch 91.30 | loss 11.73 | mse 11.73 | mre  0.00 |
scGPT - INFO - | epoch  18 | 600/3602 batches | lr 0.0000 | ms/batch 91.16 | loss 11.80 | mse 11.80 | mre  0.00 |
scGPT - INFO - | epoch  18 | 700/3602 batches | lr 0.0000 | ms/batch 91.41 | loss 13.80 | mse 13.80 | mre  0.00 |
scGPT - INFO - | epoch  18 | 800/3602 batches | lr 0.0000 | ms/batch 91.83 | loss 15.13 | mse 15.13 | mre  0.00 |
scGPT - INFO - | epoch  18 | 900/3602 batches | lr 0.0000 | ms/batch 91.17 | loss 13.02 | mse 13.02 | mre  0.00 |
scGPT - INFO - | epoch  18 | 1000/3602 batches | lr 0.0000 | ms/batch 91.16 | loss 14.30 | mse 14.30 | mre  0.00 |
scGPT - INFO - | epoch  18 | 1100/3602 batches | lr 0.0000 | ms/batch 91.12 | loss 12.29 | mse 12.29 | mre  0.00 |
scGPT - INFO - | epoch  18 | 1200/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 13.33 | mse 13.33 | mre  0.00 |
scGPT - INFO - | epoch  18 | 1300/3602 batches | lr 0.0000 | ms/batch 91.08 | loss 13.64 | mse 13.64 | mre  0.00 |
scGPT - INFO - | epoch  18 | 1400/3602 batches | lr 0.0000 | ms/batch 91.24 | loss 14.67 | mse 14.67 | mre  0.00 |
scGPT - INFO - | epoch  18 | 1500/3602 batches | lr 0.0000 | ms/batch 91.21 | loss 15.13 | mse 15.13 | mre  0.00 |
scGPT - INFO - | epoch  18 | 1600/3602 batches | lr 0.0000 | ms/batch 91.20 | loss 11.10 | mse 11.10 | mre  0.00 |
scGPT - INFO - | epoch  18 | 1700/3602 batches | lr 0.0000 | ms/batch 91.08 | loss 12.92 | mse 12.92 | mre  0.00 |
scGPT - INFO - | epoch  18 | 1800/3602 batches | lr 0.0000 | ms/batch 92.05 | loss 12.50 | mse 12.50 | mre  0.00 |
scGPT - INFO - | epoch  18 | 1900/3602 batches | lr 0.0000 | ms/batch 91.37 | loss 15.05 | mse 15.05 | mre  0.00 |
scGPT - INFO - | epoch  18 | 2000/3602 batches | lr 0.0000 | ms/batch 91.15 | loss 12.90 | mse 12.90 | mre  0.00 |
scGPT - INFO - | epoch  18 | 2100/3602 batches | lr 0.0000 | ms/batch 91.26 | loss 15.00 | mse 15.00 | mre  0.00 |
scGPT - INFO - | epoch  18 | 2200/3602 batches | lr 0.0000 | ms/batch 91.27 | loss 13.54 | mse 13.54 | mre  0.00 |
scGPT - INFO - | epoch  18 | 2300/3602 batches | lr 0.0000 | ms/batch 91.16 | loss 11.94 | mse 11.94 | mre  0.00 |
scGPT - INFO - | epoch  18 | 2400/3602 batches | lr 0.0000 | ms/batch 91.15 | loss 12.94 | mse 12.94 | mre  0.00 |
scGPT - INFO - | epoch  18 | 2500/3602 batches | lr 0.0000 | ms/batch 91.11 | loss 12.85 | mse 12.85 | mre  0.00 |
scGPT - INFO - | epoch  18 | 2600/3602 batches | lr 0.0000 | ms/batch 91.16 | loss 16.08 | mse 16.08 | mre  0.00 |
scGPT - INFO - | epoch  18 | 2700/3602 batches | lr 0.0000 | ms/batch 91.11 | loss 13.12 | mse 13.12 | mre  0.00 |
scGPT - INFO - | epoch  18 | 2800/3602 batches | lr 0.0000 | ms/batch 91.72 | loss 13.50 | mse 13.50 | mre  0.00 |
scGPT - INFO - | epoch  18 | 2900/3602 batches | lr 0.0000 | ms/batch 91.19 | loss 12.95 | mse 12.95 | mre  0.00 |
scGPT - INFO - | epoch  18 | 3000/3602 batches | lr 0.0000 | ms/batch 91.15 | loss 14.18 | mse 14.18 | mre  0.00 |
scGPT - INFO - | epoch  18 | 3100/3602 batches | lr 0.0000 | ms/batch 90.98 | loss 13.28 | mse 13.28 | mre  0.00 |
scGPT - INFO - | epoch  18 | 3200/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 13.50 | mse 13.50 | mre  0.00 |
scGPT - INFO - | epoch  18 | 3300/3602 batches | lr 0.0000 | ms/batch 91.02 | loss 11.24 | mse 11.24 | mre  0.00 |
scGPT - INFO - | epoch  18 | 3400/3602 batches | lr 0.0000 | ms/batch 91.04 | loss 13.14 | mse 13.14 | mre  0.00 |
scGPT - INFO - | epoch  18 | 3500/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 11.68 | mse 11.68 | mre  0.00 |
scGPT - INFO - | epoch  18 | 3600/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 14.50 | mse 14.50 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  18 | time: 340.97s | valid loss/mse 13.4617 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 13.073202234975408, min_delta 0.0001, val_loss 13.461653874682428
Loss error: -0.3884516397070197
scGPT - INFO - INFO: Early stopping counter 1 of 10
epoch:  18
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[5.105 3.959 3.871 ... 3.684 3.738 3.73 ]
 [6.832 3.623 3.385 ... 3.482 3.69  3.932]
 [4.453 3.697 3.824 ... 3.904 3.648 3.48 ]
 ...
 [4.902 3.764 3.887 ... 4.043 3.762 3.615]
 [4.883 3.885 4.016 ... 4.08  3.86  3.602]
 [6.617 3.69  3.438 ... 3.514 3.715 3.898]]
(801, 11) (801, 11)
By feature:  MSE:  0.6119¬±0.0 MAE:  0.5169¬±0.0 R2:  0.1712¬±0.0 PCC:  0.7707¬±0.0 Cosine Similarity:  0.9889¬±0.0
By sample:  MSE:  0.6119¬±0.0 MAE:  0.5169¬±0.0 R2:  0.2426¬±0.0 PCC:  0.485¬±0.0 Cosine Similarity:  0.9846¬±0.0
scGPT - INFO - By feature: MSE: 0.6118999719619751¬±0.0 MAE: 0.5169000029563904¬±0.0 R2: 0.1712¬±0.0 PCC: 0.7707¬±0.0 Cosine Similarity: 0.9889¬±0.0
scGPT - INFO - By sample: MSE: 0.6118999719619751¬±0.0 MAE: 0.5169000029563904¬±0.0 R2: 0.2426¬±0.0 PCC: 0.485¬±0.0 Cosine Similarity: 0.9846¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  19
Training model
scGPT - INFO - | epoch  19 | 100/3602 batches | lr 0.0000 | ms/batch 92.49 | loss 13.16 | mse 13.16 | mre  0.00 |
scGPT - INFO - | epoch  19 | 200/3602 batches | lr 0.0000 | ms/batch 92.04 | loss 13.96 | mse 13.96 | mre  0.00 |
scGPT - INFO - | epoch  19 | 300/3602 batches | lr 0.0000 | ms/batch 91.12 | loss 11.76 | mse 11.76 | mre  0.00 |
scGPT - INFO - | epoch  19 | 400/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 11.69 | mse 11.69 | mre  0.00 |
scGPT - INFO - | epoch  19 | 500/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 11.26 | mse 11.26 | mre  0.00 |
scGPT - INFO - | epoch  19 | 600/3602 batches | lr 0.0000 | ms/batch 91.01 | loss 12.42 | mse 12.42 | mre  0.00 |
scGPT - INFO - | epoch  19 | 700/3602 batches | lr 0.0000 | ms/batch 91.27 | loss 13.51 | mse 13.51 | mre  0.00 |
scGPT - INFO - | epoch  19 | 800/3602 batches | lr 0.0000 | ms/batch 91.08 | loss 14.80 | mse 14.80 | mre  0.00 |
scGPT - INFO - | epoch  19 | 900/3602 batches | lr 0.0000 | ms/batch 91.16 | loss 12.64 | mse 12.64 | mre  0.00 |
scGPT - INFO - | epoch  19 | 1000/3602 batches | lr 0.0000 | ms/batch 91.07 | loss 14.02 | mse 14.02 | mre  0.00 |
scGPT - INFO - | epoch  19 | 1100/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 12.35 | mse 12.35 | mre  0.00 |
scGPT - INFO - | epoch  19 | 1200/3602 batches | lr 0.0000 | ms/batch 91.51 | loss 13.37 | mse 13.37 | mre  0.00 |
scGPT - INFO - | epoch  19 | 1300/3602 batches | lr 0.0000 | ms/batch 91.39 | loss 13.37 | mse 13.37 | mre  0.00 |
scGPT - INFO - | epoch  19 | 1400/3602 batches | lr 0.0000 | ms/batch 91.00 | loss 14.90 | mse 14.90 | mre  0.00 |
scGPT - INFO - | epoch  19 | 1500/3602 batches | lr 0.0000 | ms/batch 90.99 | loss 15.56 | mse 15.56 | mre  0.00 |
scGPT - INFO - | epoch  19 | 1600/3602 batches | lr 0.0000 | ms/batch 90.97 | loss 11.11 | mse 11.11 | mre  0.00 |
scGPT - INFO - | epoch  19 | 1700/3602 batches | lr 0.0000 | ms/batch 90.95 | loss 12.37 | mse 12.37 | mre  0.00 |
scGPT - INFO - | epoch  19 | 1800/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 11.96 | mse 11.96 | mre  0.00 |
scGPT - INFO - | epoch  19 | 1900/3602 batches | lr 0.0000 | ms/batch 91.00 | loss 14.32 | mse 14.32 | mre  0.00 |
scGPT - INFO - | epoch  19 | 2000/3602 batches | lr 0.0000 | ms/batch 91.11 | loss 13.31 | mse 13.31 | mre  0.00 |
scGPT - INFO - | epoch  19 | 2100/3602 batches | lr 0.0000 | ms/batch 91.24 | loss 15.15 | mse 15.15 | mre  0.00 |
scGPT - INFO - | epoch  19 | 2200/3602 batches | lr 0.0000 | ms/batch 91.52 | loss 13.48 | mse 13.48 | mre  0.00 |
scGPT - INFO - | epoch  19 | 2300/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 11.87 | mse 11.87 | mre  0.00 |
scGPT - INFO - | epoch  19 | 2400/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 13.04 | mse 13.04 | mre  0.00 |
scGPT - INFO - | epoch  19 | 2500/3602 batches | lr 0.0000 | ms/batch 90.93 | loss 12.65 | mse 12.65 | mre  0.00 |
scGPT - INFO - | epoch  19 | 2600/3602 batches | lr 0.0000 | ms/batch 91.02 | loss 15.49 | mse 15.49 | mre  0.00 |
scGPT - INFO - | epoch  19 | 2700/3602 batches | lr 0.0000 | ms/batch 91.03 | loss 12.97 | mse 12.97 | mre  0.00 |
scGPT - INFO - | epoch  19 | 2800/3602 batches | lr 0.0000 | ms/batch 91.05 | loss 12.88 | mse 12.88 | mre  0.00 |
scGPT - INFO - | epoch  19 | 2900/3602 batches | lr 0.0000 | ms/batch 90.91 | loss 12.88 | mse 12.88 | mre  0.00 |
scGPT - INFO - | epoch  19 | 3000/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 13.74 | mse 13.74 | mre  0.00 |
scGPT - INFO - | epoch  19 | 3100/3602 batches | lr 0.0000 | ms/batch 90.95 | loss 12.96 | mse 12.96 | mre  0.00 |
scGPT - INFO - | epoch  19 | 3200/3602 batches | lr 0.0000 | ms/batch 91.45 | loss 13.35 | mse 13.35 | mre  0.00 |
scGPT - INFO - | epoch  19 | 3300/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 10.60 | mse 10.60 | mre  0.00 |
scGPT - INFO - | epoch  19 | 3400/3602 batches | lr 0.0000 | ms/batch 91.26 | loss 13.45 | mse 13.45 | mre  0.00 |
scGPT - INFO - | epoch  19 | 3500/3602 batches | lr 0.0000 | ms/batch 91.32 | loss 11.75 | mse 11.75 | mre  0.00 |
scGPT - INFO - | epoch  19 | 3600/3602 batches | lr 0.0000 | ms/batch 91.13 | loss 14.16 | mse 14.16 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  19 | time: 340.67s | valid loss/mse 12.6900 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.6900
best_loss: 13.073202234975408, min_delta 0.0001, val_loss 12.690040358666623
Loss error: 0.38316187630878495
epoch:  19
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.414 3.676 3.484 ... 3.58  3.672 3.924]
 [6.9   3.604 3.363 ... 3.49  3.674 3.943]
 [4.527 3.79  3.904 ... 3.92  3.732 3.506]
 ...
 [4.934 3.775 3.879 ... 4.055 3.752 3.615]
 [4.945 3.908 4.02  ... 4.094 3.873 3.615]
 [6.805 3.637 3.385 ... 3.512 3.69  3.934]]
(801, 11) (801, 11)
By feature:  MSE:  0.5769¬±0.0 MAE:  0.4986¬±0.0 R2:  0.2163¬±0.0 PCC:  0.7921¬±0.0 Cosine Similarity:  0.9897¬±0.0
By sample:  MSE:  0.5769¬±0.0 MAE:  0.4986¬±0.0 R2:  0.274¬±0.0 PCC:  0.5079¬±0.0 Cosine Similarity:  0.9853¬±0.0
scGPT - INFO - By feature: MSE: 0.5769000053405762¬±0.0 MAE: 0.4986000061035156¬±0.0 R2: 0.2163¬±0.0 PCC: 0.7921¬±0.0 Cosine Similarity: 0.9897¬±0.0
scGPT - INFO - By sample: MSE: 0.5769000053405762¬±0.0 MAE: 0.4986000061035156¬±0.0 R2: 0.274¬±0.0 PCC: 0.5079¬±0.0 Cosine Similarity: 0.9853¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  20
Training model
scGPT - INFO - | epoch  20 | 100/3602 batches | lr 0.0000 | ms/batch 92.37 | loss 13.38 | mse 13.38 | mre  0.00 |
scGPT - INFO - | epoch  20 | 200/3602 batches | lr 0.0000 | ms/batch 91.27 | loss 13.69 | mse 13.69 | mre  0.00 |
scGPT - INFO - | epoch  20 | 300/3602 batches | lr 0.0000 | ms/batch 91.41 | loss 11.38 | mse 11.38 | mre  0.00 |
scGPT - INFO - | epoch  20 | 400/3602 batches | lr 0.0000 | ms/batch 91.42 | loss 12.02 | mse 12.02 | mre  0.00 |
scGPT - INFO - | epoch  20 | 500/3602 batches | lr 0.0000 | ms/batch 91.32 | loss 11.20 | mse 11.20 | mre  0.00 |
scGPT - INFO - | epoch  20 | 600/3602 batches | lr 0.0000 | ms/batch 91.59 | loss 11.89 | mse 11.89 | mre  0.00 |
scGPT - INFO - | epoch  20 | 700/3602 batches | lr 0.0000 | ms/batch 91.27 | loss 13.41 | mse 13.41 | mre  0.00 |
scGPT - INFO - | epoch  20 | 800/3602 batches | lr 0.0000 | ms/batch 91.17 | loss 14.67 | mse 14.67 | mre  0.00 |
scGPT - INFO - | epoch  20 | 900/3602 batches | lr 0.0000 | ms/batch 91.24 | loss 12.29 | mse 12.29 | mre  0.00 |
scGPT - INFO - | epoch  20 | 1000/3602 batches | lr 0.0000 | ms/batch 91.25 | loss 13.86 | mse 13.86 | mre  0.00 |
scGPT - INFO - | epoch  20 | 1100/3602 batches | lr 0.0000 | ms/batch 91.27 | loss 12.20 | mse 12.20 | mre  0.00 |
scGPT - INFO - | epoch  20 | 1200/3602 batches | lr 0.0000 | ms/batch 91.05 | loss 13.16 | mse 13.16 | mre  0.00 |
scGPT - INFO - | epoch  20 | 1300/3602 batches | lr 0.0000 | ms/batch 91.13 | loss 13.02 | mse 13.02 | mre  0.00 |
scGPT - INFO - | epoch  20 | 1400/3602 batches | lr 0.0000 | ms/batch 91.21 | loss 14.06 | mse 14.06 | mre  0.00 |
scGPT - INFO - | epoch  20 | 1500/3602 batches | lr 0.0000 | ms/batch 91.19 | loss 14.79 | mse 14.79 | mre  0.00 |
scGPT - INFO - | epoch  20 | 1600/3602 batches | lr 0.0000 | ms/batch 91.63 | loss 11.03 | mse 11.03 | mre  0.00 |
scGPT - INFO - | epoch  20 | 1700/3602 batches | lr 0.0000 | ms/batch 91.12 | loss 12.31 | mse 12.31 | mre  0.00 |
scGPT - INFO - | epoch  20 | 1800/3602 batches | lr 0.0000 | ms/batch 91.13 | loss 11.81 | mse 11.81 | mre  0.00 |
scGPT - INFO - | epoch  20 | 1900/3602 batches | lr 0.0000 | ms/batch 91.11 | loss 14.31 | mse 14.31 | mre  0.00 |
scGPT - INFO - | epoch  20 | 2000/3602 batches | lr 0.0000 | ms/batch 91.45 | loss 12.90 | mse 12.90 | mre  0.00 |
scGPT - INFO - | epoch  20 | 2100/3602 batches | lr 0.0000 | ms/batch 91.10 | loss 15.01 | mse 15.01 | mre  0.00 |
scGPT - INFO - | epoch  20 | 2200/3602 batches | lr 0.0000 | ms/batch 91.07 | loss 13.45 | mse 13.45 | mre  0.00 |
scGPT - INFO - | epoch  20 | 2300/3602 batches | lr 0.0000 | ms/batch 91.07 | loss 11.92 | mse 11.92 | mre  0.00 |
scGPT - INFO - | epoch  20 | 2400/3602 batches | lr 0.0000 | ms/batch 91.03 | loss 12.53 | mse 12.53 | mre  0.00 |
scGPT - INFO - | epoch  20 | 2500/3602 batches | lr 0.0000 | ms/batch 91.13 | loss 12.34 | mse 12.34 | mre  0.00 |
scGPT - INFO - | epoch  20 | 2600/3602 batches | lr 0.0000 | ms/batch 91.50 | loss 15.00 | mse 15.00 | mre  0.00 |
scGPT - INFO - | epoch  20 | 2700/3602 batches | lr 0.0000 | ms/batch 91.00 | loss 12.79 | mse 12.79 | mre  0.00 |
scGPT - INFO - | epoch  20 | 2800/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 12.98 | mse 12.98 | mre  0.00 |
scGPT - INFO - | epoch  20 | 2900/3602 batches | lr 0.0000 | ms/batch 91.06 | loss 12.59 | mse 12.59 | mre  0.00 |
scGPT - INFO - | epoch  20 | 3000/3602 batches | lr 0.0000 | ms/batch 91.14 | loss 13.67 | mse 13.67 | mre  0.00 |
scGPT - INFO - | epoch  20 | 3100/3602 batches | lr 0.0000 | ms/batch 91.11 | loss 12.51 | mse 12.51 | mre  0.00 |
scGPT - INFO - | epoch  20 | 3200/3602 batches | lr 0.0000 | ms/batch 91.03 | loss 13.42 | mse 13.42 | mre  0.00 |
scGPT - INFO - | epoch  20 | 3300/3602 batches | lr 0.0000 | ms/batch 90.98 | loss 10.63 | mse 10.63 | mre  0.00 |
scGPT - INFO - | epoch  20 | 3400/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 12.83 | mse 12.83 | mre  0.00 |
scGPT - INFO - | epoch  20 | 3500/3602 batches | lr 0.0000 | ms/batch 90.97 | loss 11.71 | mse 11.71 | mre  0.00 |
scGPT - INFO - | epoch  20 | 3600/3602 batches | lr 0.0000 | ms/batch 91.54 | loss 13.93 | mse 13.93 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  20 | time: 340.92s | valid loss/mse 12.6597 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.6597
best_loss: 12.690040358666623, min_delta 0.0001, val_loss 12.659677419323152
Loss error: 0.030362939343470785
epoch:  20
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[5.97  3.79  3.604 ... 3.65  3.723 3.875]
 [6.875 3.611 3.375 ... 3.514 3.682 3.95 ]
 [4.453 3.797 3.93  ... 3.943 3.768 3.504]
 ...
 [4.87  3.762 3.875 ... 4.04  3.748 3.604]
 [4.875 3.875 3.998 ... 4.07  3.854 3.596]
 [6.758 3.64  3.39  ... 3.533 3.691 3.932]]
(801, 11) (801, 11)
By feature:  MSE:  0.5755¬±0.0 MAE:  0.4963¬±0.0 R2:  0.2186¬±0.0 PCC:  0.7907¬±0.0 Cosine Similarity:  0.9898¬±0.0
By sample:  MSE:  0.5755¬±0.0 MAE:  0.4963¬±0.0 R2:  0.2752¬±0.0 PCC:  0.5088¬±0.0 Cosine Similarity:  0.9853¬±0.0
scGPT - INFO - By feature: MSE: 0.5755000114440918¬±0.0 MAE: 0.49630001187324524¬±0.0 R2: 0.2186¬±0.0 PCC: 0.7907¬±0.0 Cosine Similarity: 0.9898¬±0.0
scGPT - INFO - By sample: MSE: 0.5755000114440918¬±0.0 MAE: 0.49630001187324524¬±0.0 R2: 0.2752¬±0.0 PCC: 0.5088¬±0.0 Cosine Similarity: 0.9853¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  21
Training model
scGPT - INFO - | epoch  21 | 100/3602 batches | lr 0.0000 | ms/batch 92.27 | loss 12.93 | mse 12.93 | mre  0.00 |
scGPT - INFO - | epoch  21 | 200/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 13.49 | mse 13.49 | mre  0.00 |
scGPT - INFO - | epoch  21 | 300/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 11.19 | mse 11.19 | mre  0.00 |
scGPT - INFO - | epoch  21 | 400/3602 batches | lr 0.0000 | ms/batch 90.99 | loss 11.73 | mse 11.73 | mre  0.00 |
scGPT - INFO - | epoch  21 | 500/3602 batches | lr 0.0000 | ms/batch 90.97 | loss 10.83 | mse 10.83 | mre  0.00 |
scGPT - INFO - | epoch  21 | 600/3602 batches | lr 0.0000 | ms/batch 91.04 | loss 11.93 | mse 11.93 | mre  0.00 |
scGPT - INFO - | epoch  21 | 700/3602 batches | lr 0.0000 | ms/batch 90.95 | loss 13.16 | mse 13.16 | mre  0.00 |
scGPT - INFO - | epoch  21 | 800/3602 batches | lr 0.0000 | ms/batch 91.06 | loss 14.73 | mse 14.73 | mre  0.00 |
scGPT - INFO - | epoch  21 | 900/3602 batches | lr 0.0000 | ms/batch 91.17 | loss 12.36 | mse 12.36 | mre  0.00 |
scGPT - INFO - | epoch  21 | 1000/3602 batches | lr 0.0000 | ms/batch 91.47 | loss 13.78 | mse 13.78 | mre  0.00 |
scGPT - INFO - | epoch  21 | 1100/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 11.98 | mse 11.98 | mre  0.00 |
scGPT - INFO - | epoch  21 | 1200/3602 batches | lr 0.0000 | ms/batch 90.93 | loss 12.88 | mse 12.88 | mre  0.00 |
scGPT - INFO - | epoch  21 | 1300/3602 batches | lr 0.0000 | ms/batch 91.04 | loss 12.75 | mse 12.75 | mre  0.00 |
scGPT - INFO - | epoch  21 | 1400/3602 batches | lr 0.0000 | ms/batch 90.98 | loss 13.80 | mse 13.80 | mre  0.00 |
scGPT - INFO - | epoch  21 | 1500/3602 batches | lr 0.0000 | ms/batch 90.95 | loss 14.98 | mse 14.98 | mre  0.00 |
scGPT - INFO - | epoch  21 | 1600/3602 batches | lr 0.0000 | ms/batch 90.91 | loss 10.61 | mse 10.61 | mre  0.00 |
scGPT - INFO - | epoch  21 | 1700/3602 batches | lr 0.0000 | ms/batch 90.99 | loss 11.86 | mse 11.86 | mre  0.00 |
scGPT - INFO - | epoch  21 | 1800/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 11.87 | mse 11.87 | mre  0.00 |
scGPT - INFO - | epoch  21 | 1900/3602 batches | lr 0.0000 | ms/batch 91.82 | loss 13.35 | mse 13.35 | mre  0.00 |
scGPT - INFO - | epoch  21 | 2000/3602 batches | lr 0.0000 | ms/batch 91.70 | loss 12.80 | mse 12.80 | mre  0.00 |
scGPT - INFO - | epoch  21 | 2100/3602 batches | lr 0.0000 | ms/batch 90.99 | loss 14.37 | mse 14.37 | mre  0.00 |
scGPT - INFO - | epoch  21 | 2200/3602 batches | lr 0.0000 | ms/batch 90.97 | loss 12.92 | mse 12.92 | mre  0.00 |
scGPT - INFO - | epoch  21 | 2300/3602 batches | lr 0.0000 | ms/batch 91.78 | loss 11.54 | mse 11.54 | mre  0.00 |
scGPT - INFO - | epoch  21 | 2400/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 12.46 | mse 12.46 | mre  0.00 |
scGPT - INFO - | epoch  21 | 2500/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 12.32 | mse 12.32 | mre  0.00 |
scGPT - INFO - | epoch  21 | 2600/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 15.18 | mse 15.18 | mre  0.00 |
scGPT - INFO - | epoch  21 | 2700/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 12.28 | mse 12.28 | mre  0.00 |
scGPT - INFO - | epoch  21 | 2800/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 12.83 | mse 12.83 | mre  0.00 |
scGPT - INFO - | epoch  21 | 2900/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 12.45 | mse 12.45 | mre  0.00 |
scGPT - INFO - | epoch  21 | 3000/3602 batches | lr 0.0000 | ms/batch 91.31 | loss 13.68 | mse 13.68 | mre  0.00 |
scGPT - INFO - | epoch  21 | 3100/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 12.29 | mse 12.29 | mre  0.00 |
scGPT - INFO - | epoch  21 | 3200/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 12.85 | mse 12.85 | mre  0.00 |
scGPT - INFO - | epoch  21 | 3300/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 10.44 | mse 10.44 | mre  0.00 |
scGPT - INFO - | epoch  21 | 3400/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 12.68 | mse 12.68 | mre  0.00 |
scGPT - INFO - | epoch  21 | 3500/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 11.42 | mse 11.42 | mre  0.00 |
scGPT - INFO - | epoch  21 | 3600/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 13.53 | mse 13.53 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  21 | time: 340.24s | valid loss/mse 12.5906 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.5906
best_loss: 12.659677419323152, min_delta 0.0001, val_loss 12.59062040402797
Loss error: 0.06905701529518282
epoch:  21
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.18  3.697 3.533 ... 3.592 3.682 3.9  ]
 [6.926 3.574 3.354 ... 3.482 3.662 3.943]
 [4.387 3.77  3.926 ... 3.908 3.766 3.496]
 ...
 [4.867 3.77  3.895 ... 4.03  3.76  3.604]
 [4.895 3.86  3.994 ... 4.055 3.848 3.596]
 [6.824 3.613 3.377 ... 3.51  3.68  3.934]]
(801, 11) (801, 11)
By feature:  MSE:  0.5723¬±0.0 MAE:  0.4931¬±0.0 R2:  0.2144¬±0.0 PCC:  0.7946¬±0.0 Cosine Similarity:  0.9898¬±0.0
By sample:  MSE:  0.5723¬±0.0 MAE:  0.4931¬±0.0 R2:  0.2804¬±0.0 PCC:  0.516¬±0.0 Cosine Similarity:  0.9854¬±0.0
scGPT - INFO - By feature: MSE: 0.5723000168800354¬±0.0 MAE: 0.49309998750686646¬±0.0 R2: 0.2144¬±0.0 PCC: 0.7946¬±0.0 Cosine Similarity: 0.9898¬±0.0
scGPT - INFO - By sample: MSE: 0.5723000168800354¬±0.0 MAE: 0.49309998750686646¬±0.0 R2: 0.2804¬±0.0 PCC: 0.516¬±0.0 Cosine Similarity: 0.9854¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  22
Training model
scGPT - INFO - | epoch  22 | 100/3602 batches | lr 0.0000 | ms/batch 92.09 | loss 12.51 | mse 12.51 | mre  0.00 |
scGPT - INFO - | epoch  22 | 200/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 13.05 | mse 13.05 | mre  0.00 |
scGPT - INFO - | epoch  22 | 300/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 11.51 | mse 11.51 | mre  0.00 |
scGPT - INFO - | epoch  22 | 400/3602 batches | lr 0.0000 | ms/batch 91.37 | loss 11.53 | mse 11.53 | mre  0.00 |
scGPT - INFO - | epoch  22 | 500/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 11.32 | mse 11.32 | mre  0.00 |
scGPT - INFO - | epoch  22 | 600/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 11.51 | mse 11.51 | mre  0.00 |
scGPT - INFO - | epoch  22 | 700/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 12.70 | mse 12.70 | mre  0.00 |
scGPT - INFO - | epoch  22 | 800/3602 batches | lr 0.0000 | ms/batch 91.03 | loss 14.95 | mse 14.95 | mre  0.00 |
scGPT - INFO - | epoch  22 | 900/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 12.00 | mse 12.00 | mre  0.00 |
scGPT - INFO - | epoch  22 | 1000/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 13.42 | mse 13.42 | mre  0.00 |
scGPT - INFO - | epoch  22 | 1100/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 11.93 | mse 11.93 | mre  0.00 |
scGPT - INFO - | epoch  22 | 1200/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 12.33 | mse 12.33 | mre  0.00 |
scGPT - INFO - | epoch  22 | 1300/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 12.88 | mse 12.88 | mre  0.00 |
scGPT - INFO - | epoch  22 | 1400/3602 batches | lr 0.0000 | ms/batch 91.40 | loss 13.40 | mse 13.40 | mre  0.00 |
scGPT - INFO - | epoch  22 | 1500/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 15.19 | mse 15.19 | mre  0.00 |
scGPT - INFO - | epoch  22 | 1600/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 10.64 | mse 10.64 | mre  0.00 |
scGPT - INFO - | epoch  22 | 1700/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 11.75 | mse 11.75 | mre  0.00 |
scGPT - INFO - | epoch  22 | 1800/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 11.39 | mse 11.39 | mre  0.00 |
scGPT - INFO - | epoch  22 | 1900/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 13.30 | mse 13.30 | mre  0.00 |
scGPT - INFO - | epoch  22 | 2000/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 12.55 | mse 12.55 | mre  0.00 |
scGPT - INFO - | epoch  22 | 2100/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 14.47 | mse 14.47 | mre  0.00 |
scGPT - INFO - | epoch  22 | 2200/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 12.71 | mse 12.71 | mre  0.00 |
scGPT - INFO - | epoch  22 | 2300/3602 batches | lr 0.0000 | ms/batch 91.00 | loss 11.36 | mse 11.36 | mre  0.00 |
scGPT - INFO - | epoch  22 | 2400/3602 batches | lr 0.0000 | ms/batch 91.40 | loss 12.53 | mse 12.53 | mre  0.00 |
scGPT - INFO - | epoch  22 | 2500/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 11.84 | mse 11.84 | mre  0.00 |
scGPT - INFO - | epoch  22 | 2600/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 15.66 | mse 15.66 | mre  0.00 |
scGPT - INFO - | epoch  22 | 2700/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 12.19 | mse 12.19 | mre  0.00 |
scGPT - INFO - | epoch  22 | 2800/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 12.85 | mse 12.85 | mre  0.00 |
scGPT - INFO - | epoch  22 | 2900/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 12.52 | mse 12.52 | mre  0.00 |
scGPT - INFO - | epoch  22 | 3000/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 12.93 | mse 12.93 | mre  0.00 |
scGPT - INFO - | epoch  22 | 3100/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 12.19 | mse 12.19 | mre  0.00 |
scGPT - INFO - | epoch  22 | 3200/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 12.93 | mse 12.93 | mre  0.00 |
scGPT - INFO - | epoch  22 | 3300/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.43 | mse 10.43 | mre  0.00 |
scGPT - INFO - | epoch  22 | 3400/3602 batches | lr 0.0000 | ms/batch 91.30 | loss 12.20 | mse 12.20 | mre  0.00 |
scGPT - INFO - | epoch  22 | 3500/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 11.23 | mse 11.23 | mre  0.00 |
scGPT - INFO - | epoch  22 | 3600/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 13.32 | mse 13.32 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  22 | time: 339.81s | valid loss/mse 12.4266 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.4266
best_loss: 12.59062040402797, min_delta 0.0001, val_loss 12.426564168171042
Loss error: 0.1640562358569273
epoch:  22
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.67  3.602 3.395 ... 3.51  3.64  3.936]
 [6.953 3.6   3.373 ... 3.504 3.682 3.959]
 [4.184 3.93  4.086 ... 3.896 3.889 3.514]
 ...
 [4.812 3.773 3.885 ... 4.03  3.74  3.584]
 [4.87  3.865 3.984 ... 4.06  3.842 3.586]
 [6.844 3.654 3.406 ... 3.54  3.705 3.953]]
(801, 11) (801, 11)
By feature:  MSE:  0.5649¬±0.0 MAE:  0.4908¬±0.0 R2:  0.2093¬±0.0 PCC:  0.7991¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5649¬±0.0 MAE:  0.4908¬±0.0 R2:  0.2903¬±0.0 PCC:  0.5247¬±0.0 Cosine Similarity:  0.9856¬±0.0
scGPT - INFO - By feature: MSE: 0.5648999810218811¬±0.0 MAE: 0.49079999327659607¬±0.0 R2: 0.2093¬±0.0 PCC: 0.7991¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5648999810218811¬±0.0 MAE: 0.49079999327659607¬±0.0 R2: 0.2903¬±0.0 PCC: 0.5247¬±0.0 Cosine Similarity: 0.9856¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  23
Training model
scGPT - INFO - | epoch  23 | 100/3602 batches | lr 0.0000 | ms/batch 92.03 | loss 12.17 | mse 12.17 | mre  0.00 |
scGPT - INFO - | epoch  23 | 200/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 12.87 | mse 12.87 | mre  0.00 |
scGPT - INFO - | epoch  23 | 300/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 10.94 | mse 10.94 | mre  0.00 |
scGPT - INFO - | epoch  23 | 400/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 11.21 | mse 11.21 | mre  0.00 |
scGPT - INFO - | epoch  23 | 500/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 10.29 | mse 10.29 | mre  0.00 |
scGPT - INFO - | epoch  23 | 600/3602 batches | lr 0.0000 | ms/batch 91.05 | loss 11.38 | mse 11.38 | mre  0.00 |
scGPT - INFO - | epoch  23 | 700/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 12.42 | mse 12.42 | mre  0.00 |
scGPT - INFO - | epoch  23 | 800/3602 batches | lr 0.0000 | ms/batch 91.38 | loss 14.69 | mse 14.69 | mre  0.00 |
scGPT - INFO - | epoch  23 | 900/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 12.01 | mse 12.01 | mre  0.00 |
scGPT - INFO - | epoch  23 | 1000/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 13.44 | mse 13.44 | mre  0.00 |
scGPT - INFO - | epoch  23 | 1100/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 11.52 | mse 11.52 | mre  0.00 |
scGPT - INFO - | epoch  23 | 1200/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 12.62 | mse 12.62 | mre  0.00 |
scGPT - INFO - | epoch  23 | 1300/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 12.28 | mse 12.28 | mre  0.00 |
scGPT - INFO - | epoch  23 | 1400/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 13.28 | mse 13.28 | mre  0.00 |
scGPT - INFO - | epoch  23 | 1500/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 15.11 | mse 15.11 | mre  0.00 |
scGPT - INFO - | epoch  23 | 1600/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 10.62 | mse 10.62 | mre  0.00 |
scGPT - INFO - | epoch  23 | 1700/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 11.32 | mse 11.32 | mre  0.00 |
scGPT - INFO - | epoch  23 | 1800/3602 batches | lr 0.0000 | ms/batch 91.37 | loss 11.37 | mse 11.37 | mre  0.00 |
scGPT - INFO - | epoch  23 | 1900/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 13.14 | mse 13.14 | mre  0.00 |
scGPT - INFO - | epoch  23 | 2000/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 12.14 | mse 12.14 | mre  0.00 |
scGPT - INFO - | epoch  23 | 2100/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 14.45 | mse 14.45 | mre  0.00 |
scGPT - INFO - | epoch  23 | 2200/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 12.66 | mse 12.66 | mre  0.00 |
scGPT - INFO - | epoch  23 | 2300/3602 batches | lr 0.0000 | ms/batch 90.91 | loss 11.41 | mse 11.41 | mre  0.00 |
scGPT - INFO - | epoch  23 | 2400/3602 batches | lr 0.0000 | ms/batch 92.77 | loss 12.24 | mse 12.24 | mre  0.00 |
scGPT - INFO - | epoch  23 | 2500/3602 batches | lr 0.0000 | ms/batch 92.76 | loss 11.88 | mse 11.88 | mre  0.00 |
scGPT - INFO - | epoch  23 | 2600/3602 batches | lr 0.0000 | ms/batch 92.67 | loss 15.05 | mse 15.05 | mre  0.00 |
scGPT - INFO - | epoch  23 | 2700/3602 batches | lr 0.0000 | ms/batch 92.73 | loss 12.33 | mse 12.33 | mre  0.00 |
scGPT - INFO - | epoch  23 | 2800/3602 batches | lr 0.0000 | ms/batch 93.24 | loss 12.35 | mse 12.35 | mre  0.00 |
scGPT - INFO - | epoch  23 | 2900/3602 batches | lr 0.0000 | ms/batch 92.76 | loss 12.35 | mse 12.35 | mre  0.00 |
scGPT - INFO - | epoch  23 | 3000/3602 batches | lr 0.0000 | ms/batch 92.72 | loss 12.88 | mse 12.88 | mre  0.00 |
scGPT - INFO - | epoch  23 | 3100/3602 batches | lr 0.0000 | ms/batch 92.34 | loss 12.05 | mse 12.05 | mre  0.00 |
scGPT - INFO - | epoch  23 | 3200/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 12.71 | mse 12.71 | mre  0.00 |
scGPT - INFO - | epoch  23 | 3300/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 10.16 | mse 10.16 | mre  0.00 |
scGPT - INFO - | epoch  23 | 3400/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 12.44 | mse 12.44 | mre  0.00 |
scGPT - INFO - | epoch  23 | 3500/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 11.35 | mse 11.35 | mre  0.00 |
scGPT - INFO - | epoch  23 | 3600/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 13.15 | mse 13.15 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  23 | time: 341.25s | valid loss/mse 12.6217 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.426564168171042, min_delta 0.0001, val_loss 12.621652356545429
Loss error: -0.19508818837438646
scGPT - INFO - INFO: Early stopping counter 1 of 10
epoch:  23
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.87  3.568 3.354 ... 3.488 3.65  3.959]
 [7.    3.578 3.357 ... 3.49  3.686 3.959]
 [4.473 3.877 4.06  ... 3.986 3.9   3.578]
 ...
 [4.86  3.781 3.908 ... 4.043 3.771 3.604]
 [4.883 3.842 3.973 ... 4.047 3.848 3.586]
 [6.85  3.64  3.395 ... 3.537 3.713 3.951]]
(801, 11) (801, 11)
By feature:  MSE:  0.5738¬±0.0 MAE:  0.4962¬±0.0 R2:  0.1916¬±0.0 PCC:  0.7953¬±0.0 Cosine Similarity:  0.9898¬±0.0
By sample:  MSE:  0.5738¬±0.0 MAE:  0.4962¬±0.0 R2:  0.2805¬±0.0 PCC:  0.5203¬±0.0 Cosine Similarity:  0.9854¬±0.0
scGPT - INFO - By feature: MSE: 0.5738000273704529¬±0.0 MAE: 0.49619999527931213¬±0.0 R2: 0.1916¬±0.0 PCC: 0.7953¬±0.0 Cosine Similarity: 0.9898¬±0.0
scGPT - INFO - By sample: MSE: 0.5738000273704529¬±0.0 MAE: 0.49619999527931213¬±0.0 R2: 0.2805¬±0.0 PCC: 0.5203¬±0.0 Cosine Similarity: 0.9854¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  24
Training model
scGPT - INFO - | epoch  24 | 100/3602 batches | lr 0.0000 | ms/batch 92.05 | loss 11.76 | mse 11.76 | mre  0.00 |
scGPT - INFO - | epoch  24 | 200/3602 batches | lr 0.0000 | ms/batch 91.40 | loss 12.89 | mse 12.89 | mre  0.00 |
scGPT - INFO - | epoch  24 | 300/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 10.58 | mse 10.58 | mre  0.00 |
scGPT - INFO - | epoch  24 | 400/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 11.27 | mse 11.27 | mre  0.00 |
scGPT - INFO - | epoch  24 | 500/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 10.41 | mse 10.41 | mre  0.00 |
scGPT - INFO - | epoch  24 | 600/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 11.08 | mse 11.08 | mre  0.00 |
scGPT - INFO - | epoch  24 | 700/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 12.47 | mse 12.47 | mre  0.00 |
scGPT - INFO - | epoch  24 | 800/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 14.71 | mse 14.71 | mre  0.00 |
scGPT - INFO - | epoch  24 | 900/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 11.74 | mse 11.74 | mre  0.00 |
scGPT - INFO - | epoch  24 | 1000/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 13.33 | mse 13.33 | mre  0.00 |
scGPT - INFO - | epoch  24 | 1100/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 11.54 | mse 11.54 | mre  0.00 |
scGPT - INFO - | epoch  24 | 1200/3602 batches | lr 0.0000 | ms/batch 91.35 | loss 12.60 | mse 12.60 | mre  0.00 |
scGPT - INFO - | epoch  24 | 1300/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 12.43 | mse 12.43 | mre  0.00 |
scGPT - INFO - | epoch  24 | 1400/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 13.79 | mse 13.79 | mre  0.00 |
scGPT - INFO - | epoch  24 | 1500/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 15.12 | mse 15.12 | mre  0.00 |
scGPT - INFO - | epoch  24 | 1600/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 10.60 | mse 10.60 | mre  0.00 |
scGPT - INFO - | epoch  24 | 1700/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 11.32 | mse 11.32 | mre  0.00 |
scGPT - INFO - | epoch  24 | 1800/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 11.22 | mse 11.22 | mre  0.00 |
scGPT - INFO - | epoch  24 | 1900/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 13.24 | mse 13.24 | mre  0.00 |
scGPT - INFO - | epoch  24 | 2000/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 12.49 | mse 12.49 | mre  0.00 |
scGPT - INFO - | epoch  24 | 2100/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 14.20 | mse 14.20 | mre  0.00 |
scGPT - INFO - | epoch  24 | 2200/3602 batches | lr 0.0000 | ms/batch 91.26 | loss 12.47 | mse 12.47 | mre  0.00 |
scGPT - INFO - | epoch  24 | 2300/3602 batches | lr 0.0000 | ms/batch 91.78 | loss 11.77 | mse 11.77 | mre  0.00 |
scGPT - INFO - | epoch  24 | 2400/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 12.43 | mse 12.43 | mre  0.00 |
scGPT - INFO - | epoch  24 | 2500/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 11.89 | mse 11.89 | mre  0.00 |
scGPT - INFO - | epoch  24 | 2600/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 14.40 | mse 14.40 | mre  0.00 |
scGPT - INFO - | epoch  24 | 2700/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 11.63 | mse 11.63 | mre  0.00 |
scGPT - INFO - | epoch  24 | 2800/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 12.27 | mse 12.27 | mre  0.00 |
scGPT - INFO - | epoch  24 | 2900/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 12.32 | mse 12.32 | mre  0.00 |
scGPT - INFO - | epoch  24 | 3000/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 13.15 | mse 13.15 | mre  0.00 |
scGPT - INFO - | epoch  24 | 3100/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 11.79 | mse 11.79 | mre  0.00 |
scGPT - INFO - | epoch  24 | 3200/3602 batches | lr 0.0000 | ms/batch 91.26 | loss 12.47 | mse 12.47 | mre  0.00 |
scGPT - INFO - | epoch  24 | 3300/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 10.39 | mse 10.39 | mre  0.00 |
scGPT - INFO - | epoch  24 | 3400/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 12.06 | mse 12.06 | mre  0.00 |
scGPT - INFO - | epoch  24 | 3500/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 11.03 | mse 11.03 | mre  0.00 |
scGPT - INFO - | epoch  24 | 3600/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 12.97 | mse 12.97 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  24 | time: 339.75s | valid loss/mse 12.3841 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.3841
best_loss: 12.426564168171042, min_delta 0.0001, val_loss 12.384085188346559
Loss error: 0.04247897982448379
epoch:  24
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.957 3.547 3.328 ... 3.45  3.625 3.932]
 [6.973 3.594 3.373 ... 3.488 3.678 3.94 ]
 [4.16  3.99  4.215 ... 3.896 4.    3.578]
 ...
 [4.84  3.79  3.914 ... 4.03  3.764 3.586]
 [4.88  3.85  3.98  ... 4.043 3.848 3.572]
 [6.832 3.648 3.402 ... 3.533 3.701 3.934]]
(801, 11) (801, 11)
By feature:  MSE:  0.563¬±0.0 MAE:  0.4904¬±0.0 R2:  0.219¬±0.0 PCC:  0.8013¬±0.0 Cosine Similarity:  0.9901¬±0.0
By sample:  MSE:  0.563¬±0.0 MAE:  0.4904¬±0.0 R2:  0.2888¬±0.0 PCC:  0.5255¬±0.0 Cosine Similarity:  0.9857¬±0.0
scGPT - INFO - By feature: MSE: 0.5630000233650208¬±0.0 MAE: 0.4903999865055084¬±0.0 R2: 0.219¬±0.0 PCC: 0.8013¬±0.0 Cosine Similarity: 0.9901¬±0.0
scGPT - INFO - By sample: MSE: 0.5630000233650208¬±0.0 MAE: 0.4903999865055084¬±0.0 R2: 0.2888¬±0.0 PCC: 0.5255¬±0.0 Cosine Similarity: 0.9857¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  25
Training model
scGPT - INFO - | epoch  25 | 100/3602 batches | lr 0.0000 | ms/batch 92.24 | loss 11.56 | mse 11.56 | mre  0.00 |
scGPT - INFO - | epoch  25 | 200/3602 batches | lr 0.0000 | ms/batch 91.06 | loss 12.75 | mse 12.75 | mre  0.00 |
scGPT - INFO - | epoch  25 | 300/3602 batches | lr 0.0000 | ms/batch 91.12 | loss 10.58 | mse 10.58 | mre  0.00 |
scGPT - INFO - | epoch  25 | 400/3602 batches | lr 0.0000 | ms/batch 91.12 | loss 11.13 | mse 11.13 | mre  0.00 |
scGPT - INFO - | epoch  25 | 500/3602 batches | lr 0.0000 | ms/batch 91.03 | loss 10.20 | mse 10.20 | mre  0.00 |
scGPT - INFO - | epoch  25 | 600/3602 batches | lr 0.0000 | ms/batch 91.42 | loss 11.40 | mse 11.40 | mre  0.00 |
scGPT - INFO - | epoch  25 | 700/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 12.42 | mse 12.42 | mre  0.00 |
scGPT - INFO - | epoch  25 | 800/3602 batches | lr 0.0000 | ms/batch 90.93 | loss 14.80 | mse 14.80 | mre  0.00 |
scGPT - INFO - | epoch  25 | 900/3602 batches | lr 0.0000 | ms/batch 91.01 | loss 11.53 | mse 11.53 | mre  0.00 |
scGPT - INFO - | epoch  25 | 1000/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 13.01 | mse 13.01 | mre  0.00 |
scGPT - INFO - | epoch  25 | 1100/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 11.41 | mse 11.41 | mre  0.00 |
scGPT - INFO - | epoch  25 | 1200/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 12.45 | mse 12.45 | mre  0.00 |
scGPT - INFO - | epoch  25 | 1300/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 12.37 | mse 12.37 | mre  0.00 |
scGPT - INFO - | epoch  25 | 1400/3602 batches | lr 0.0000 | ms/batch 90.91 | loss 13.41 | mse 13.41 | mre  0.00 |
scGPT - INFO - | epoch  25 | 1500/3602 batches | lr 0.0000 | ms/batch 90.97 | loss 15.16 | mse 15.16 | mre  0.00 |
scGPT - INFO - | epoch  25 | 1600/3602 batches | lr 0.0000 | ms/batch 91.46 | loss 10.48 | mse 10.48 | mre  0.00 |
scGPT - INFO - | epoch  25 | 1700/3602 batches | lr 0.0000 | ms/batch 90.91 | loss 11.21 | mse 11.21 | mre  0.00 |
scGPT - INFO - | epoch  25 | 1800/3602 batches | lr 0.0000 | ms/batch 90.97 | loss 10.69 | mse 10.69 | mre  0.00 |
scGPT - INFO - | epoch  25 | 1900/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 12.90 | mse 12.90 | mre  0.00 |
scGPT - INFO - | epoch  25 | 2000/3602 batches | lr 0.0000 | ms/batch 91.05 | loss 12.34 | mse 12.34 | mre  0.00 |
scGPT - INFO - | epoch  25 | 2100/3602 batches | lr 0.0000 | ms/batch 91.01 | loss 14.30 | mse 14.30 | mre  0.00 |
scGPT - INFO - | epoch  25 | 2200/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 12.18 | mse 12.18 | mre  0.00 |
scGPT - INFO - | epoch  25 | 2300/3602 batches | lr 0.0000 | ms/batch 90.97 | loss 11.45 | mse 11.45 | mre  0.00 |
scGPT - INFO - | epoch  25 | 2400/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 12.05 | mse 12.05 | mre  0.00 |
scGPT - INFO - | epoch  25 | 2500/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 11.88 | mse 11.88 | mre  0.00 |
scGPT - INFO - | epoch  25 | 2600/3602 batches | lr 0.0000 | ms/batch 91.28 | loss 14.18 | mse 14.18 | mre  0.00 |
scGPT - INFO - | epoch  25 | 2700/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 12.06 | mse 12.06 | mre  0.00 |
scGPT - INFO - | epoch  25 | 2800/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 12.09 | mse 12.09 | mre  0.00 |
scGPT - INFO - | epoch  25 | 2900/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 12.46 | mse 12.46 | mre  0.00 |
scGPT - INFO - | epoch  25 | 3000/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 12.80 | mse 12.80 | mre  0.00 |
scGPT - INFO - | epoch  25 | 3100/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 11.90 | mse 11.90 | mre  0.00 |
scGPT - INFO - | epoch  25 | 3200/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 12.44 | mse 12.44 | mre  0.00 |
scGPT - INFO - | epoch  25 | 3300/3602 batches | lr 0.0000 | ms/batch 92.06 | loss 10.01 | mse 10.01 | mre  0.00 |
scGPT - INFO - | epoch  25 | 3400/3602 batches | lr 0.0000 | ms/batch 92.91 | loss 11.65 | mse 11.65 | mre  0.00 |
scGPT - INFO - | epoch  25 | 3500/3602 batches | lr 0.0000 | ms/batch 92.91 | loss 11.41 | mse 11.41 | mre  0.00 |
scGPT - INFO - | epoch  25 | 3600/3602 batches | lr 0.0000 | ms/batch 93.44 | loss 13.01 | mse 13.01 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  25 | time: 341.37s | valid loss/mse 12.2961 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.2961
best_loss: 12.384085188346559, min_delta 0.0001, val_loss 12.296099969659108
Loss error: 0.08798521868745013
epoch:  25
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.9   3.525 3.318 ... 3.424 3.615 3.918]
 [6.973 3.58  3.363 ... 3.475 3.68  3.945]
 [3.766 4.188 4.43  ... 3.81  4.117 3.656]
 ...
 [4.83  3.773 3.898 ... 4.03  3.752 3.598]
 [4.875 3.83  3.96  ... 4.035 3.836 3.586]
 [6.84  3.643 3.398 ... 3.527 3.707 3.943]]
(801, 11) (801, 11)
By feature:  MSE:  0.559¬±0.0 MAE:  0.4875¬±0.0 R2:  0.2232¬±0.0 PCC:  0.7996¬±0.0 Cosine Similarity:  0.99¬±0.0
By sample:  MSE:  0.559¬±0.0 MAE:  0.4875¬±0.0 R2:  0.2972¬±0.0 PCC:  0.5327¬±0.0 Cosine Similarity:  0.9858¬±0.0
scGPT - INFO - By feature: MSE: 0.5590000152587891¬±0.0 MAE: 0.48750001192092896¬±0.0 R2: 0.2232¬±0.0 PCC: 0.7996¬±0.0 Cosine Similarity: 0.99¬±0.0
scGPT - INFO - By sample: MSE: 0.5590000152587891¬±0.0 MAE: 0.48750001192092896¬±0.0 R2: 0.2972¬±0.0 PCC: 0.5327¬±0.0 Cosine Similarity: 0.9858¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  26
Training model
scGPT - INFO - | epoch  26 | 100/3602 batches | lr 0.0000 | ms/batch 94.35 | loss 11.38 | mse 11.38 | mre  0.00 |
scGPT - INFO - | epoch  26 | 200/3602 batches | lr 0.0000 | ms/batch 93.06 | loss 12.58 | mse 12.58 | mre  0.00 |
scGPT - INFO - | epoch  26 | 300/3602 batches | lr 0.0000 | ms/batch 93.01 | loss 10.61 | mse 10.61 | mre  0.00 |
scGPT - INFO - | epoch  26 | 400/3602 batches | lr 0.0000 | ms/batch 92.99 | loss 11.21 | mse 11.21 | mre  0.00 |
scGPT - INFO - | epoch  26 | 500/3602 batches | lr 0.0000 | ms/batch 93.05 | loss 10.14 | mse 10.14 | mre  0.00 |
scGPT - INFO - | epoch  26 | 600/3602 batches | lr 0.0000 | ms/batch 92.91 | loss 11.24 | mse 11.24 | mre  0.00 |
scGPT - INFO - | epoch  26 | 700/3602 batches | lr 0.0000 | ms/batch 92.83 | loss 12.11 | mse 12.11 | mre  0.00 |
scGPT - INFO - | epoch  26 | 800/3602 batches | lr 0.0000 | ms/batch 92.95 | loss 14.57 | mse 14.57 | mre  0.00 |
scGPT - INFO - | epoch  26 | 900/3602 batches | lr 0.0000 | ms/batch 93.04 | loss 11.87 | mse 11.87 | mre  0.00 |
scGPT - INFO - | epoch  26 | 1000/3602 batches | lr 0.0000 | ms/batch 93.59 | loss 12.93 | mse 12.93 | mre  0.00 |
scGPT - INFO - | epoch  26 | 1100/3602 batches | lr 0.0000 | ms/batch 92.87 | loss 11.46 | mse 11.46 | mre  0.00 |
scGPT - INFO - | epoch  26 | 1200/3602 batches | lr 0.0000 | ms/batch 93.01 | loss 12.42 | mse 12.42 | mre  0.00 |
scGPT - INFO - | epoch  26 | 1300/3602 batches | lr 0.0000 | ms/batch 92.85 | loss 12.10 | mse 12.10 | mre  0.00 |
scGPT - INFO - | epoch  26 | 1400/3602 batches | lr 0.0000 | ms/batch 92.78 | loss 13.09 | mse 13.09 | mre  0.00 |
scGPT - INFO - | epoch  26 | 1500/3602 batches | lr 0.0000 | ms/batch 92.91 | loss 15.16 | mse 15.16 | mre  0.00 |
scGPT - INFO - | epoch  26 | 1600/3602 batches | lr 0.0000 | ms/batch 92.97 | loss 10.23 | mse 10.23 | mre  0.00 |
scGPT - INFO - | epoch  26 | 1700/3602 batches | lr 0.0000 | ms/batch 92.83 | loss 11.29 | mse 11.29 | mre  0.00 |
scGPT - INFO - | epoch  26 | 1800/3602 batches | lr 0.0000 | ms/batch 92.94 | loss 11.10 | mse 11.10 | mre  0.00 |
scGPT - INFO - | epoch  26 | 1900/3602 batches | lr 0.0000 | ms/batch 92.78 | loss 12.52 | mse 12.52 | mre  0.00 |
scGPT - INFO - | epoch  26 | 2000/3602 batches | lr 0.0000 | ms/batch 93.40 | loss 12.19 | mse 12.19 | mre  0.00 |
scGPT - INFO - | epoch  26 | 2100/3602 batches | lr 0.0000 | ms/batch 92.82 | loss 14.12 | mse 14.12 | mre  0.00 |
scGPT - INFO - | epoch  26 | 2200/3602 batches | lr 0.0000 | ms/batch 92.79 | loss 12.39 | mse 12.39 | mre  0.00 |
scGPT - INFO - | epoch  26 | 2300/3602 batches | lr 0.0000 | ms/batch 92.83 | loss 11.02 | mse 11.02 | mre  0.00 |
scGPT - INFO - | epoch  26 | 2400/3602 batches | lr 0.0000 | ms/batch 92.89 | loss 12.13 | mse 12.13 | mre  0.00 |
scGPT - INFO - | epoch  26 | 2500/3602 batches | lr 0.0000 | ms/batch 92.88 | loss 11.54 | mse 11.54 | mre  0.00 |
scGPT - INFO - | epoch  26 | 2600/3602 batches | lr 0.0000 | ms/batch 93.26 | loss 14.07 | mse 14.07 | mre  0.00 |
scGPT - INFO - | epoch  26 | 2700/3602 batches | lr 0.0000 | ms/batch 93.05 | loss 11.58 | mse 11.58 | mre  0.00 |
scGPT - INFO - | epoch  26 | 2800/3602 batches | lr 0.0000 | ms/batch 92.91 | loss 12.10 | mse 12.10 | mre  0.00 |
scGPT - INFO - | epoch  26 | 2900/3602 batches | lr 0.0000 | ms/batch 93.09 | loss 12.63 | mse 12.63 | mre  0.00 |
scGPT - INFO - | epoch  26 | 3000/3602 batches | lr 0.0000 | ms/batch 93.46 | loss 12.35 | mse 12.35 | mre  0.00 |
scGPT - INFO - | epoch  26 | 3100/3602 batches | lr 0.0000 | ms/batch 93.01 | loss 11.73 | mse 11.73 | mre  0.00 |
scGPT - INFO - | epoch  26 | 3200/3602 batches | lr 0.0000 | ms/batch 92.94 | loss 12.41 | mse 12.41 | mre  0.00 |
scGPT - INFO - | epoch  26 | 3300/3602 batches | lr 0.0000 | ms/batch 92.97 | loss 10.28 | mse 10.28 | mre  0.00 |
scGPT - INFO - | epoch  26 | 3400/3602 batches | lr 0.0000 | ms/batch 92.95 | loss 11.68 | mse 11.68 | mre  0.00 |
scGPT - INFO - | epoch  26 | 3500/3602 batches | lr 0.0000 | ms/batch 92.90 | loss 10.78 | mse 10.78 | mre  0.00 |
scGPT - INFO - | epoch  26 | 3600/3602 batches | lr 0.0000 | ms/batch 92.88 | loss 12.74 | mse 12.74 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  26 | time: 347.37s | valid loss/mse 12.1842 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.1842
best_loss: 12.296099969659108, min_delta 0.0001, val_loss 12.184247482581979
Loss error: 0.11185248707712958
epoch:  26
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.965 3.52  3.314 ... 3.422 3.607 3.922]
 [7.004 3.574 3.36  ... 3.469 3.668 3.941]
 [3.979 4.07  4.332 ... 3.873 4.09  3.652]
 ...
 [4.793 3.764 3.887 ... 4.008 3.736 3.564]
 [4.832 3.81  3.936 ... 4.004 3.81  3.547]
 [6.86  3.645 3.398 ... 3.53  3.701 3.945]]
(801, 11) (801, 11)
By feature:  MSE:  0.5539¬±0.0 MAE:  0.4846¬±0.0 R2:  0.2239¬±0.0 PCC:  0.8045¬±0.0 Cosine Similarity:  0.9901¬±0.0
By sample:  MSE:  0.5539¬±0.0 MAE:  0.4846¬±0.0 R2:  0.3024¬±0.0 PCC:  0.5367¬±0.0 Cosine Similarity:  0.9859¬±0.0
scGPT - INFO - By feature: MSE: 0.5539000034332275¬±0.0 MAE: 0.4846000075340271¬±0.0 R2: 0.2239¬±0.0 PCC: 0.8045¬±0.0 Cosine Similarity: 0.9901¬±0.0
scGPT - INFO - By sample: MSE: 0.5539000034332275¬±0.0 MAE: 0.4846000075340271¬±0.0 R2: 0.3024¬±0.0 PCC: 0.5367¬±0.0 Cosine Similarity: 0.9859¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  27
Training model
scGPT - INFO - | epoch  27 | 100/3602 batches | lr 0.0000 | ms/batch 91.99 | loss 11.14 | mse 11.14 | mre  0.00 |
scGPT - INFO - | epoch  27 | 200/3602 batches | lr 0.0000 | ms/batch 92.69 | loss 12.38 | mse 12.38 | mre  0.00 |
scGPT - INFO - | epoch  27 | 300/3602 batches | lr 0.0000 | ms/batch 92.86 | loss 10.57 | mse 10.57 | mre  0.00 |
scGPT - INFO - | epoch  27 | 400/3602 batches | lr 0.0000 | ms/batch 93.48 | loss 11.41 | mse 11.41 | mre  0.00 |
scGPT - INFO - | epoch  27 | 500/3602 batches | lr 0.0000 | ms/batch 92.92 | loss 10.16 | mse 10.16 | mre  0.00 |
scGPT - INFO - | epoch  27 | 600/3602 batches | lr 0.0000 | ms/batch 93.00 | loss 11.11 | mse 11.11 | mre  0.00 |
scGPT - INFO - | epoch  27 | 700/3602 batches | lr 0.0000 | ms/batch 92.90 | loss 12.17 | mse 12.17 | mre  0.00 |
scGPT - INFO - | epoch  27 | 800/3602 batches | lr 0.0000 | ms/batch 93.10 | loss 14.09 | mse 14.09 | mre  0.00 |
scGPT - INFO - | epoch  27 | 900/3602 batches | lr 0.0000 | ms/batch 92.91 | loss 11.56 | mse 11.56 | mre  0.00 |
scGPT - INFO - | epoch  27 | 1000/3602 batches | lr 0.0000 | ms/batch 92.77 | loss 12.96 | mse 12.96 | mre  0.00 |
scGPT - INFO - | epoch  27 | 1100/3602 batches | lr 0.0000 | ms/batch 92.90 | loss 11.11 | mse 11.11 | mre  0.00 |
scGPT - INFO - | epoch  27 | 1200/3602 batches | lr 0.0000 | ms/batch 92.91 | loss 12.64 | mse 12.64 | mre  0.00 |
scGPT - INFO - | epoch  27 | 1300/3602 batches | lr 0.0000 | ms/batch 92.91 | loss 12.10 | mse 12.10 | mre  0.00 |
scGPT - INFO - | epoch  27 | 1400/3602 batches | lr 0.0000 | ms/batch 93.38 | loss 12.97 | mse 12.97 | mre  0.00 |
scGPT - INFO - | epoch  27 | 1500/3602 batches | lr 0.0000 | ms/batch 92.98 | loss 14.94 | mse 14.94 | mre  0.00 |
scGPT - INFO - | epoch  27 | 1600/3602 batches | lr 0.0000 | ms/batch 93.02 | loss 10.17 | mse 10.17 | mre  0.00 |
scGPT - INFO - | epoch  27 | 1700/3602 batches | lr 0.0000 | ms/batch 92.94 | loss 11.05 | mse 11.05 | mre  0.00 |
scGPT - INFO - | epoch  27 | 1800/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 10.74 | mse 10.74 | mre  0.00 |
scGPT - INFO - | epoch  27 | 1900/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 12.46 | mse 12.46 | mre  0.00 |
scGPT - INFO - | epoch  27 | 2000/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 11.98 | mse 11.98 | mre  0.00 |
scGPT - INFO - | epoch  27 | 2100/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 13.79 | mse 13.79 | mre  0.00 |
scGPT - INFO - | epoch  27 | 2200/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 12.42 | mse 12.42 | mre  0.00 |
scGPT - INFO - | epoch  27 | 2300/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 11.35 | mse 11.35 | mre  0.00 |
scGPT - INFO - | epoch  27 | 2400/3602 batches | lr 0.0000 | ms/batch 91.26 | loss 11.60 | mse 11.60 | mre  0.00 |
scGPT - INFO - | epoch  27 | 2500/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 11.39 | mse 11.39 | mre  0.00 |
scGPT - INFO - | epoch  27 | 2600/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 14.36 | mse 14.36 | mre  0.00 |
scGPT - INFO - | epoch  27 | 2700/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 11.17 | mse 11.17 | mre  0.00 |
scGPT - INFO - | epoch  27 | 2800/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 12.06 | mse 12.06 | mre  0.00 |
scGPT - INFO - | epoch  27 | 2900/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 12.02 | mse 12.02 | mre  0.00 |
scGPT - INFO - | epoch  27 | 3000/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 12.31 | mse 12.31 | mre  0.00 |
scGPT - INFO - | epoch  27 | 3100/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 11.22 | mse 11.22 | mre  0.00 |
scGPT - INFO - | epoch  27 | 3200/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 12.11 | mse 12.11 | mre  0.00 |
scGPT - INFO - | epoch  27 | 3300/3602 batches | lr 0.0000 | ms/batch 91.14 | loss  9.76 | mse  9.76 | mre  0.00 |
scGPT - INFO - | epoch  27 | 3400/3602 batches | lr 0.0000 | ms/batch 91.41 | loss 11.66 | mse 11.66 | mre  0.00 |
scGPT - INFO - | epoch  27 | 3500/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 10.94 | mse 10.94 | mre  0.00 |
scGPT - INFO - | epoch  27 | 3600/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 12.33 | mse 12.33 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  27 | time: 343.09s | valid loss/mse 12.5160 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.184247482581979, min_delta 0.0001, val_loss 12.516024678387446
Loss error: -0.3317771958054667
scGPT - INFO - INFO: Early stopping counter 1 of 10
epoch:  27
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.887 3.56  3.328 ... 3.436 3.625 3.93 ]
 [7.03  3.592 3.354 ... 3.477 3.678 3.955]
 [3.56  4.73  4.875 ... 3.674 4.242 3.637]
 ...
 [4.793 3.768 3.879 ... 4.016 3.736 3.578]
 [4.855 3.824 3.941 ... 4.02  3.826 3.572]
 [6.832 3.67  3.396 ... 3.55  3.715 3.95 ]]
(801, 11) (801, 11)
By feature:  MSE:  0.5689¬±0.0 MAE:  0.4958¬±0.0 R2:  0.2056¬±0.0 PCC:  0.7963¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5689¬±0.0 MAE:  0.4958¬±0.0 R2:  0.285¬±0.0 PCC:  0.5257¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5688999891281128¬±0.0 MAE: 0.4957999885082245¬±0.0 R2: 0.2056¬±0.0 PCC: 0.7963¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5688999891281128¬±0.0 MAE: 0.4957999885082245¬±0.0 R2: 0.285¬±0.0 PCC: 0.5257¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  28
Training model
scGPT - INFO - | epoch  28 | 100/3602 batches | lr 0.0000 | ms/batch 92.01 | loss 11.03 | mse 11.03 | mre  0.00 |
scGPT - INFO - | epoch  28 | 200/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 12.55 | mse 12.55 | mre  0.00 |
scGPT - INFO - | epoch  28 | 300/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 10.41 | mse 10.41 | mre  0.00 |
scGPT - INFO - | epoch  28 | 400/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 11.04 | mse 11.04 | mre  0.00 |
scGPT - INFO - | epoch  28 | 500/3602 batches | lr 0.0000 | ms/batch 90.80 | loss  9.87 | mse  9.87 | mre  0.00 |
scGPT - INFO - | epoch  28 | 600/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 10.96 | mse 10.96 | mre  0.00 |
scGPT - INFO - | epoch  28 | 700/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 12.88 | mse 12.88 | mre  0.00 |
scGPT - INFO - | epoch  28 | 800/3602 batches | lr 0.0000 | ms/batch 91.34 | loss 14.43 | mse 14.43 | mre  0.00 |
scGPT - INFO - | epoch  28 | 900/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 11.37 | mse 11.37 | mre  0.00 |
scGPT - INFO - | epoch  28 | 1000/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 12.89 | mse 12.89 | mre  0.00 |
scGPT - INFO - | epoch  28 | 1100/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 11.59 | mse 11.59 | mre  0.00 |
scGPT - INFO - | epoch  28 | 1200/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 12.14 | mse 12.14 | mre  0.00 |
scGPT - INFO - | epoch  28 | 1300/3602 batches | lr 0.0000 | ms/batch 92.16 | loss 11.72 | mse 11.72 | mre  0.00 |
scGPT - INFO - | epoch  28 | 1400/3602 batches | lr 0.0000 | ms/batch 92.97 | loss 12.98 | mse 12.98 | mre  0.00 |
scGPT - INFO - | epoch  28 | 1500/3602 batches | lr 0.0000 | ms/batch 92.94 | loss 15.21 | mse 15.21 | mre  0.00 |
scGPT - INFO - | epoch  28 | 1600/3602 batches | lr 0.0000 | ms/batch 92.88 | loss 10.40 | mse 10.40 | mre  0.00 |
scGPT - INFO - | epoch  28 | 1700/3602 batches | lr 0.0000 | ms/batch 92.97 | loss 11.06 | mse 11.06 | mre  0.00 |
scGPT - INFO - | epoch  28 | 1800/3602 batches | lr 0.0000 | ms/batch 93.48 | loss 10.87 | mse 10.87 | mre  0.00 |
scGPT - INFO - | epoch  28 | 1900/3602 batches | lr 0.0000 | ms/batch 92.96 | loss 12.62 | mse 12.62 | mre  0.00 |
scGPT - INFO - | epoch  28 | 2000/3602 batches | lr 0.0000 | ms/batch 92.95 | loss 11.94 | mse 11.94 | mre  0.00 |
scGPT - INFO - | epoch  28 | 2100/3602 batches | lr 0.0000 | ms/batch 92.79 | loss 13.45 | mse 13.45 | mre  0.00 |
scGPT - INFO - | epoch  28 | 2200/3602 batches | lr 0.0000 | ms/batch 93.00 | loss 12.15 | mse 12.15 | mre  0.00 |
scGPT - INFO - | epoch  28 | 2300/3602 batches | lr 0.0000 | ms/batch 92.98 | loss 10.63 | mse 10.63 | mre  0.00 |
scGPT - INFO - | epoch  28 | 2400/3602 batches | lr 0.0000 | ms/batch 92.98 | loss 11.67 | mse 11.67 | mre  0.00 |
scGPT - INFO - | epoch  28 | 2500/3602 batches | lr 0.0000 | ms/batch 92.89 | loss 11.57 | mse 11.57 | mre  0.00 |
scGPT - INFO - | epoch  28 | 2600/3602 batches | lr 0.0000 | ms/batch 92.98 | loss 14.14 | mse 14.14 | mre  0.00 |
scGPT - INFO - | epoch  28 | 2700/3602 batches | lr 0.0000 | ms/batch 92.98 | loss 11.23 | mse 11.23 | mre  0.00 |
scGPT - INFO - | epoch  28 | 2800/3602 batches | lr 0.0000 | ms/batch 93.54 | loss 12.00 | mse 12.00 | mre  0.00 |
scGPT - INFO - | epoch  28 | 2900/3602 batches | lr 0.0000 | ms/batch 93.01 | loss 11.76 | mse 11.76 | mre  0.00 |
scGPT - INFO - | epoch  28 | 3000/3602 batches | lr 0.0000 | ms/batch 92.91 | loss 12.55 | mse 12.55 | mre  0.00 |
scGPT - INFO - | epoch  28 | 3100/3602 batches | lr 0.0000 | ms/batch 92.91 | loss 11.64 | mse 11.64 | mre  0.00 |
scGPT - INFO - | epoch  28 | 3200/3602 batches | lr 0.0000 | ms/batch 92.86 | loss 11.94 | mse 11.94 | mre  0.00 |
scGPT - INFO - | epoch  28 | 3300/3602 batches | lr 0.0000 | ms/batch 92.88 | loss  9.76 | mse  9.76 | mre  0.00 |
scGPT - INFO - | epoch  28 | 3400/3602 batches | lr 0.0000 | ms/batch 93.05 | loss 11.90 | mse 11.90 | mre  0.00 |
scGPT - INFO - | epoch  28 | 3500/3602 batches | lr 0.0000 | ms/batch 92.90 | loss 10.91 | mse 10.91 | mre  0.00 |
scGPT - INFO - | epoch  28 | 3600/3602 batches | lr 0.0000 | ms/batch 92.86 | loss 12.36 | mse 12.36 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  28 | time: 345.29s | valid loss/mse 12.2368 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.184247482581979, min_delta 0.0001, val_loss 12.236756415105193
Loss error: -0.05250893252321376
scGPT - INFO - INFO: Early stopping counter 2 of 10
epoch:  28
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.938 3.527 3.312 ... 3.404 3.598 3.906]
 [6.996 3.572 3.346 ... 3.46  3.656 3.928]
 [3.709 4.56  4.742 ... 3.748 4.242 3.658]
 ...
 [4.797 3.764 3.885 ... 4.01  3.736 3.57 ]
 [4.87  3.818 3.947 ... 4.02  3.83  3.566]
 [6.844 3.643 3.387 ... 3.523 3.691 3.934]]
(801, 11) (801, 11)
By feature:  MSE:  0.5563¬±0.0 MAE:  0.4876¬±0.0 R2:  0.2154¬±0.0 PCC:  0.8034¬±0.0 Cosine Similarity:  0.9901¬±0.0
By sample:  MSE:  0.5563¬±0.0 MAE:  0.4876¬±0.0 R2:  0.2977¬±0.0 PCC:  0.535¬±0.0 Cosine Similarity:  0.9858¬±0.0
scGPT - INFO - By feature: MSE: 0.5562999844551086¬±0.0 MAE: 0.4875999987125397¬±0.0 R2: 0.2154¬±0.0 PCC: 0.8034¬±0.0 Cosine Similarity: 0.9901¬±0.0
scGPT - INFO - By sample: MSE: 0.5562999844551086¬±0.0 MAE: 0.4875999987125397¬±0.0 R2: 0.2977¬±0.0 PCC: 0.535¬±0.0 Cosine Similarity: 0.9858¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  29
Training model
scGPT - INFO - | epoch  29 | 100/3602 batches | lr 0.0000 | ms/batch 94.14 | loss 11.57 | mse 11.57 | mre  0.00 |
scGPT - INFO - | epoch  29 | 200/3602 batches | lr 0.0000 | ms/batch 93.37 | loss 12.19 | mse 12.19 | mre  0.00 |
scGPT - INFO - | epoch  29 | 300/3602 batches | lr 0.0000 | ms/batch 92.83 | loss 10.51 | mse 10.51 | mre  0.00 |
scGPT - INFO - | epoch  29 | 400/3602 batches | lr 0.0000 | ms/batch 92.86 | loss 10.66 | mse 10.66 | mre  0.00 |
scGPT - INFO - | epoch  29 | 500/3602 batches | lr 0.0000 | ms/batch 92.89 | loss 10.26 | mse 10.26 | mre  0.00 |
scGPT - INFO - | epoch  29 | 600/3602 batches | lr 0.0000 | ms/batch 92.82 | loss 10.79 | mse 10.79 | mre  0.00 |
scGPT - INFO - | epoch  29 | 700/3602 batches | lr 0.0000 | ms/batch 92.85 | loss 12.17 | mse 12.17 | mre  0.00 |
scGPT - INFO - | epoch  29 | 800/3602 batches | lr 0.0000 | ms/batch 92.89 | loss 13.98 | mse 13.98 | mre  0.00 |
scGPT - INFO - | epoch  29 | 900/3602 batches | lr 0.0000 | ms/batch 92.88 | loss 11.59 | mse 11.59 | mre  0.00 |
scGPT - INFO - | epoch  29 | 1000/3602 batches | lr 0.0000 | ms/batch 92.86 | loss 13.29 | mse 13.29 | mre  0.00 |
scGPT - INFO - | epoch  29 | 1100/3602 batches | lr 0.0000 | ms/batch 92.98 | loss 11.00 | mse 11.00 | mre  0.00 |
scGPT - INFO - | epoch  29 | 1200/3602 batches | lr 0.0000 | ms/batch 93.38 | loss 11.91 | mse 11.91 | mre  0.00 |
scGPT - INFO - | epoch  29 | 1300/3602 batches | lr 0.0000 | ms/batch 92.84 | loss 11.78 | mse 11.78 | mre  0.00 |
scGPT - INFO - | epoch  29 | 1400/3602 batches | lr 0.0000 | ms/batch 92.94 | loss 12.90 | mse 12.90 | mre  0.00 |
scGPT - INFO - | epoch  29 | 1500/3602 batches | lr 0.0000 | ms/batch 92.81 | loss 14.47 | mse 14.47 | mre  0.00 |
scGPT - INFO - | epoch  29 | 1600/3602 batches | lr 0.0000 | ms/batch 92.65 | loss 10.57 | mse 10.57 | mre  0.00 |
scGPT - INFO - | epoch  29 | 1700/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 10.97 | mse 10.97 | mre  0.00 |
scGPT - INFO - | epoch  29 | 1800/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 11.06 | mse 11.06 | mre  0.00 |
scGPT - INFO - | epoch  29 | 1900/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 12.53 | mse 12.53 | mre  0.00 |
scGPT - INFO - | epoch  29 | 2000/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 11.68 | mse 11.68 | mre  0.00 |
scGPT - INFO - | epoch  29 | 2100/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 13.60 | mse 13.60 | mre  0.00 |
scGPT - INFO - | epoch  29 | 2200/3602 batches | lr 0.0000 | ms/batch 91.36 | loss 12.06 | mse 12.06 | mre  0.00 |
scGPT - INFO - | epoch  29 | 2300/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 10.89 | mse 10.89 | mre  0.00 |
scGPT - INFO - | epoch  29 | 2400/3602 batches | lr 0.0000 | ms/batch 91.18 | loss 12.05 | mse 12.05 | mre  0.00 |
scGPT - INFO - | epoch  29 | 2500/3602 batches | lr 0.0000 | ms/batch 92.79 | loss 11.15 | mse 11.15 | mre  0.00 |
scGPT - INFO - | epoch  29 | 2600/3602 batches | lr 0.0000 | ms/batch 92.88 | loss 13.86 | mse 13.86 | mre  0.00 |
scGPT - INFO - | epoch  29 | 2700/3602 batches | lr 0.0000 | ms/batch 92.97 | loss 11.21 | mse 11.21 | mre  0.00 |
scGPT - INFO - | epoch  29 | 2800/3602 batches | lr 0.0000 | ms/batch 92.91 | loss 12.12 | mse 12.12 | mre  0.00 |
scGPT - INFO - | epoch  29 | 2900/3602 batches | lr 0.0000 | ms/batch 92.87 | loss 12.17 | mse 12.17 | mre  0.00 |
scGPT - INFO - | epoch  29 | 3000/3602 batches | lr 0.0000 | ms/batch 92.89 | loss 12.67 | mse 12.67 | mre  0.00 |
scGPT - INFO - | epoch  29 | 3100/3602 batches | lr 0.0000 | ms/batch 93.14 | loss 11.46 | mse 11.46 | mre  0.00 |
scGPT - INFO - | epoch  29 | 3200/3602 batches | lr 0.0000 | ms/batch 93.37 | loss 11.73 | mse 11.73 | mre  0.00 |
scGPT - INFO - | epoch  29 | 3300/3602 batches | lr 0.0000 | ms/batch 92.87 | loss 10.01 | mse 10.01 | mre  0.00 |
scGPT - INFO - | epoch  29 | 3400/3602 batches | lr 0.0000 | ms/batch 92.91 | loss 11.50 | mse 11.50 | mre  0.00 |
scGPT - INFO - | epoch  29 | 3500/3602 batches | lr 0.0000 | ms/batch 92.48 | loss 11.16 | mse 11.16 | mre  0.00 |
scGPT - INFO - | epoch  29 | 3600/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 12.23 | mse 12.23 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  29 | time: 345.29s | valid loss/mse 12.1191 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.1191
best_loss: 12.184247482581979, min_delta 0.0001, val_loss 12.1191315918826
Loss error: 0.06511589069937962
epoch:  29
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.934 3.506 3.29  ... 3.387 3.586 3.898]
 [7.016 3.57  3.334 ... 3.45  3.65  3.936]
 [3.85  4.406 4.645 ... 3.86  4.234 3.719]
 ...
 [4.77  3.77  3.877 ... 4.008 3.715 3.572]
 [4.85  3.826 3.947 ... 4.02  3.82  3.576]
 [6.855 3.643 3.375 ... 3.516 3.688 3.94 ]]
(801, 11) (801, 11)
By feature:  MSE:  0.5509¬±0.0 MAE:  0.4843¬±0.0 R2:  0.2254¬±0.0 PCC:  0.8077¬±0.0 Cosine Similarity:  0.9902¬±0.0
By sample:  MSE:  0.5509¬±0.0 MAE:  0.4843¬±0.0 R2:  0.3057¬±0.0 PCC:  0.5425¬±0.0 Cosine Similarity:  0.986¬±0.0
scGPT - INFO - By feature: MSE: 0.5508999824523926¬±0.0 MAE: 0.48429998755455017¬±0.0 R2: 0.2254¬±0.0 PCC: 0.8077¬±0.0 Cosine Similarity: 0.9902¬±0.0
scGPT - INFO - By sample: MSE: 0.5508999824523926¬±0.0 MAE: 0.48429998755455017¬±0.0 R2: 0.3057¬±0.0 PCC: 0.5425¬±0.0 Cosine Similarity: 0.986¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  30
Training model
scGPT - INFO - | epoch  30 | 100/3602 batches | lr 0.0000 | ms/batch 92.02 | loss 11.46 | mse 11.46 | mre  0.00 |
scGPT - INFO - | epoch  30 | 200/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 12.20 | mse 12.20 | mre  0.00 |
scGPT - INFO - | epoch  30 | 300/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 10.57 | mse 10.57 | mre  0.00 |
scGPT - INFO - | epoch  30 | 400/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 10.98 | mse 10.98 | mre  0.00 |
scGPT - INFO - | epoch  30 | 500/3602 batches | lr 0.0000 | ms/batch 90.81 | loss  9.71 | mse  9.71 | mre  0.00 |
scGPT - INFO - | epoch  30 | 600/3602 batches | lr 0.0000 | ms/batch 91.28 | loss 10.69 | mse 10.69 | mre  0.00 |
scGPT - INFO - | epoch  30 | 700/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 11.81 | mse 11.81 | mre  0.00 |
scGPT - INFO - | epoch  30 | 800/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 14.46 | mse 14.46 | mre  0.00 |
scGPT - INFO - | epoch  30 | 900/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 11.58 | mse 11.58 | mre  0.00 |
scGPT - INFO - | epoch  30 | 1000/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 12.48 | mse 12.48 | mre  0.00 |
scGPT - INFO - | epoch  30 | 1100/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 11.33 | mse 11.33 | mre  0.00 |
scGPT - INFO - | epoch  30 | 1200/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 11.76 | mse 11.76 | mre  0.00 |
scGPT - INFO - | epoch  30 | 1300/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 11.99 | mse 11.99 | mre  0.00 |
scGPT - INFO - | epoch  30 | 1400/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 12.74 | mse 12.74 | mre  0.00 |
scGPT - INFO - | epoch  30 | 1500/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 14.90 | mse 14.90 | mre  0.00 |
scGPT - INFO - | epoch  30 | 1600/3602 batches | lr 0.0000 | ms/batch 91.65 | loss 10.11 | mse 10.11 | mre  0.00 |
scGPT - INFO - | epoch  30 | 1700/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 10.80 | mse 10.80 | mre  0.00 |
scGPT - INFO - | epoch  30 | 1800/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 10.76 | mse 10.76 | mre  0.00 |
scGPT - INFO - | epoch  30 | 1900/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 12.09 | mse 12.09 | mre  0.00 |
scGPT - INFO - | epoch  30 | 2000/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 11.67 | mse 11.67 | mre  0.00 |
scGPT - INFO - | epoch  30 | 2100/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 13.33 | mse 13.33 | mre  0.00 |
scGPT - INFO - | epoch  30 | 2200/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 12.52 | mse 12.52 | mre  0.00 |
scGPT - INFO - | epoch  30 | 2300/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 10.75 | mse 10.75 | mre  0.00 |
scGPT - INFO - | epoch  30 | 2400/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 12.06 | mse 12.06 | mre  0.00 |
scGPT - INFO - | epoch  30 | 2500/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 11.17 | mse 11.17 | mre  0.00 |
scGPT - INFO - | epoch  30 | 2600/3602 batches | lr 0.0000 | ms/batch 91.38 | loss 13.76 | mse 13.76 | mre  0.00 |
scGPT - INFO - | epoch  30 | 2700/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 11.23 | mse 11.23 | mre  0.00 |
scGPT - INFO - | epoch  30 | 2800/3602 batches | lr 0.0000 | ms/batch 90.91 | loss 11.67 | mse 11.67 | mre  0.00 |
scGPT - INFO - | epoch  30 | 2900/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 11.81 | mse 11.81 | mre  0.00 |
scGPT - INFO - | epoch  30 | 3000/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 12.35 | mse 12.35 | mre  0.00 |
scGPT - INFO - | epoch  30 | 3100/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 11.17 | mse 11.17 | mre  0.00 |
scGPT - INFO - | epoch  30 | 3200/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 11.76 | mse 11.76 | mre  0.00 |
scGPT - INFO - | epoch  30 | 3300/3602 batches | lr 0.0000 | ms/batch 90.69 | loss  9.56 | mse  9.56 | mre  0.00 |
scGPT - INFO - | epoch  30 | 3400/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 11.44 | mse 11.44 | mre  0.00 |
scGPT - INFO - | epoch  30 | 3500/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 10.63 | mse 10.63 | mre  0.00 |
scGPT - INFO - | epoch  30 | 3600/3602 batches | lr 0.0000 | ms/batch 91.31 | loss 12.06 | mse 12.06 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  30 | time: 339.63s | valid loss/mse 12.1660 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.1191315918826, min_delta 0.0001, val_loss 12.166020696082812
Loss error: -0.04688910420021308
scGPT - INFO - INFO: Early stopping counter 1 of 10
epoch:  30
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.97  3.521 3.31  ... 3.398 3.61  3.908]
 [7.01  3.582 3.355 ... 3.46  3.668 3.938]
 [3.95  4.438 4.656 ... 3.908 4.28  3.744]
 ...
 [4.758 3.758 3.873 ... 4.004 3.715 3.56 ]
 [4.844 3.818 3.945 ... 4.02  3.824 3.568]
 [6.832 3.66  3.396 ... 3.531 3.707 3.94 ]]
(801, 11) (801, 11)
By feature:  MSE:  0.553¬±0.0 MAE:  0.4848¬±0.0 R2:  0.2253¬±0.0 PCC:  0.8084¬±0.0 Cosine Similarity:  0.9902¬±0.0
By sample:  MSE:  0.553¬±0.0 MAE:  0.4848¬±0.0 R2:  0.3026¬±0.0 PCC:  0.5417¬±0.0 Cosine Similarity:  0.986¬±0.0
scGPT - INFO - By feature: MSE: 0.5529999732971191¬±0.0 MAE: 0.4848000109195709¬±0.0 R2: 0.2253¬±0.0 PCC: 0.8084¬±0.0 Cosine Similarity: 0.9902¬±0.0
scGPT - INFO - By sample: MSE: 0.5529999732971191¬±0.0 MAE: 0.4848000109195709¬±0.0 R2: 0.3026¬±0.0 PCC: 0.5417¬±0.0 Cosine Similarity: 0.986¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  31
Training model
scGPT - INFO - | epoch  31 | 100/3602 batches | lr 0.0000 | ms/batch 91.96 | loss 11.11 | mse 11.11 | mre  0.00 |
scGPT - INFO - | epoch  31 | 200/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 12.35 | mse 12.35 | mre  0.00 |
scGPT - INFO - | epoch  31 | 300/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 10.52 | mse 10.52 | mre  0.00 |
scGPT - INFO - | epoch  31 | 400/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 10.79 | mse 10.79 | mre  0.00 |
scGPT - INFO - | epoch  31 | 500/3602 batches | lr 0.0000 | ms/batch 90.82 | loss  9.78 | mse  9.78 | mre  0.00 |
scGPT - INFO - | epoch  31 | 600/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 11.07 | mse 11.07 | mre  0.00 |
scGPT - INFO - | epoch  31 | 700/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 11.83 | mse 11.83 | mre  0.00 |
scGPT - INFO - | epoch  31 | 800/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 14.15 | mse 14.15 | mre  0.00 |
scGPT - INFO - | epoch  31 | 900/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 11.54 | mse 11.54 | mre  0.00 |
scGPT - INFO - | epoch  31 | 1000/3602 batches | lr 0.0000 | ms/batch 91.28 | loss 13.33 | mse 13.33 | mre  0.00 |
scGPT - INFO - | epoch  31 | 1100/3602 batches | lr 0.0000 | ms/batch 91.20 | loss 11.42 | mse 11.42 | mre  0.00 |
scGPT - INFO - | epoch  31 | 1200/3602 batches | lr 0.0000 | ms/batch 91.74 | loss 11.70 | mse 11.70 | mre  0.00 |
scGPT - INFO - | epoch  31 | 1300/3602 batches | lr 0.0000 | ms/batch 91.43 | loss 11.09 | mse 11.09 | mre  0.00 |
scGPT - INFO - | epoch  31 | 1400/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 13.00 | mse 13.00 | mre  0.00 |
scGPT - INFO - | epoch  31 | 1500/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 14.56 | mse 14.56 | mre  0.00 |
scGPT - INFO - | epoch  31 | 1600/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 10.19 | mse 10.19 | mre  0.00 |
scGPT - INFO - | epoch  31 | 1700/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 10.71 | mse 10.71 | mre  0.00 |
scGPT - INFO - | epoch  31 | 1800/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.53 | mse 10.53 | mre  0.00 |
scGPT - INFO - | epoch  31 | 1900/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 12.24 | mse 12.24 | mre  0.00 |
scGPT - INFO - | epoch  31 | 2000/3602 batches | lr 0.0000 | ms/batch 91.21 | loss 11.59 | mse 11.59 | mre  0.00 |
scGPT - INFO - | epoch  31 | 2100/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 13.51 | mse 13.51 | mre  0.00 |
scGPT - INFO - | epoch  31 | 2200/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 12.22 | mse 12.22 | mre  0.00 |
scGPT - INFO - | epoch  31 | 2300/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 10.96 | mse 10.96 | mre  0.00 |
scGPT - INFO - | epoch  31 | 2400/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 11.95 | mse 11.95 | mre  0.00 |
scGPT - INFO - | epoch  31 | 2500/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 11.33 | mse 11.33 | mre  0.00 |
scGPT - INFO - | epoch  31 | 2600/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 13.60 | mse 13.60 | mre  0.00 |
scGPT - INFO - | epoch  31 | 2700/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 11.14 | mse 11.14 | mre  0.00 |
scGPT - INFO - | epoch  31 | 2800/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 11.51 | mse 11.51 | mre  0.00 |
scGPT - INFO - | epoch  31 | 2900/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 12.07 | mse 12.07 | mre  0.00 |
scGPT - INFO - | epoch  31 | 3000/3602 batches | lr 0.0000 | ms/batch 91.28 | loss 11.98 | mse 11.98 | mre  0.00 |
scGPT - INFO - | epoch  31 | 3100/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 11.25 | mse 11.25 | mre  0.00 |
scGPT - INFO - | epoch  31 | 3200/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 11.66 | mse 11.66 | mre  0.00 |
scGPT - INFO - | epoch  31 | 3300/3602 batches | lr 0.0000 | ms/batch 90.66 | loss  9.94 | mse  9.94 | mre  0.00 |
scGPT - INFO - | epoch  31 | 3400/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 11.53 | mse 11.53 | mre  0.00 |
scGPT - INFO - | epoch  31 | 3500/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 11.00 | mse 11.00 | mre  0.00 |
scGPT - INFO - | epoch  31 | 3600/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 11.93 | mse 11.93 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  31 | time: 339.61s | valid loss/mse 12.2154 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.1191315918826, min_delta 0.0001, val_loss 12.215375845054265
Loss error: -0.0962442531716654
scGPT - INFO - INFO: Early stopping counter 2 of 10
epoch:  31
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.953 3.51  3.28  ... 3.373 3.582 3.889]
 [7.004 3.578 3.334 ... 3.451 3.652 3.926]
 [3.842 4.605 4.785 ... 3.809 4.293 3.703]
 ...
 [4.777 3.771 3.871 ... 4.004 3.72  3.56 ]
 [4.86  3.826 3.94  ... 4.016 3.83  3.568]
 [6.855 3.637 3.365 ... 3.512 3.686 3.926]]
(801, 11) (801, 11)
By feature:  MSE:  0.5553¬±0.0 MAE:  0.4849¬±0.0 R2:  0.223¬±0.0 PCC:  0.8051¬±0.0 Cosine Similarity:  0.9901¬±0.0
By sample:  MSE:  0.5553¬±0.0 MAE:  0.4849¬±0.0 R2:  0.3007¬±0.0 PCC:  0.5416¬±0.0 Cosine Similarity:  0.9859¬±0.0
scGPT - INFO - By feature: MSE: 0.5552999973297119¬±0.0 MAE: 0.48489999771118164¬±0.0 R2: 0.223¬±0.0 PCC: 0.8051¬±0.0 Cosine Similarity: 0.9901¬±0.0
scGPT - INFO - By sample: MSE: 0.5552999973297119¬±0.0 MAE: 0.48489999771118164¬±0.0 R2: 0.3007¬±0.0 PCC: 0.5416¬±0.0 Cosine Similarity: 0.9859¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  32
Training model
scGPT - INFO - | epoch  32 | 100/3602 batches | lr 0.0000 | ms/batch 92.05 | loss 11.29 | mse 11.29 | mre  0.00 |
scGPT - INFO - | epoch  32 | 200/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 12.18 | mse 12.18 | mre  0.00 |
scGPT - INFO - | epoch  32 | 300/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 10.03 | mse 10.03 | mre  0.00 |
scGPT - INFO - | epoch  32 | 400/3602 batches | lr 0.0000 | ms/batch 91.26 | loss 10.97 | mse 10.97 | mre  0.00 |
scGPT - INFO - | epoch  32 | 500/3602 batches | lr 0.0000 | ms/batch 90.85 | loss  9.91 | mse  9.91 | mre  0.00 |
scGPT - INFO - | epoch  32 | 600/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 11.06 | mse 11.06 | mre  0.00 |
scGPT - INFO - | epoch  32 | 700/3602 batches | lr 0.0000 | ms/batch 90.91 | loss 11.94 | mse 11.94 | mre  0.00 |
scGPT - INFO - | epoch  32 | 800/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 14.55 | mse 14.55 | mre  0.00 |
scGPT - INFO - | epoch  32 | 900/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 11.26 | mse 11.26 | mre  0.00 |
scGPT - INFO - | epoch  32 | 1000/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 12.95 | mse 12.95 | mre  0.00 |
scGPT - INFO - | epoch  32 | 1100/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 10.90 | mse 10.90 | mre  0.00 |
scGPT - INFO - | epoch  32 | 1200/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 11.34 | mse 11.34 | mre  0.00 |
scGPT - INFO - | epoch  32 | 1300/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 11.54 | mse 11.54 | mre  0.00 |
scGPT - INFO - | epoch  32 | 1400/3602 batches | lr 0.0000 | ms/batch 91.30 | loss 12.69 | mse 12.69 | mre  0.00 |
scGPT - INFO - | epoch  32 | 1500/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 14.75 | mse 14.75 | mre  0.00 |
scGPT - INFO - | epoch  32 | 1600/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 10.38 | mse 10.38 | mre  0.00 |
scGPT - INFO - | epoch  32 | 1700/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 11.20 | mse 11.20 | mre  0.00 |
scGPT - INFO - | epoch  32 | 1800/3602 batches | lr 0.0000 | ms/batch 91.03 | loss 10.81 | mse 10.81 | mre  0.00 |
scGPT - INFO - | epoch  32 | 1900/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 12.28 | mse 12.28 | mre  0.00 |
scGPT - INFO - | epoch  32 | 2000/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 11.77 | mse 11.77 | mre  0.00 |
scGPT - INFO - | epoch  32 | 2100/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 12.99 | mse 12.99 | mre  0.00 |
scGPT - INFO - | epoch  32 | 2200/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 12.14 | mse 12.14 | mre  0.00 |
scGPT - INFO - | epoch  32 | 2300/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 10.84 | mse 10.84 | mre  0.00 |
scGPT - INFO - | epoch  32 | 2400/3602 batches | lr 0.0000 | ms/batch 91.29 | loss 12.03 | mse 12.03 | mre  0.00 |
scGPT - INFO - | epoch  32 | 2500/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 10.64 | mse 10.64 | mre  0.00 |
scGPT - INFO - | epoch  32 | 2600/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 14.09 | mse 14.09 | mre  0.00 |
scGPT - INFO - | epoch  32 | 2700/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 10.93 | mse 10.93 | mre  0.00 |
scGPT - INFO - | epoch  32 | 2800/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 11.58 | mse 11.58 | mre  0.00 |
scGPT - INFO - | epoch  32 | 2900/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 11.80 | mse 11.80 | mre  0.00 |
scGPT - INFO - | epoch  32 | 3000/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 11.99 | mse 11.99 | mre  0.00 |
scGPT - INFO - | epoch  32 | 3100/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 11.36 | mse 11.36 | mre  0.00 |
scGPT - INFO - | epoch  32 | 3200/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 11.64 | mse 11.64 | mre  0.00 |
scGPT - INFO - | epoch  32 | 3300/3602 batches | lr 0.0000 | ms/batch 90.73 | loss  9.56 | mse  9.56 | mre  0.00 |
scGPT - INFO - | epoch  32 | 3400/3602 batches | lr 0.0000 | ms/batch 91.27 | loss 11.35 | mse 11.35 | mre  0.00 |
scGPT - INFO - | epoch  32 | 3500/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 10.67 | mse 10.67 | mre  0.00 |
scGPT - INFO - | epoch  32 | 3600/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 12.26 | mse 12.26 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  32 | time: 339.58s | valid loss/mse 12.1758 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.1191315918826, min_delta 0.0001, val_loss 12.17580228441217
Loss error: -0.05667069252957013
scGPT - INFO - INFO: Early stopping counter 3 of 10
epoch:  32
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.945 3.504 3.271 ... 3.363 3.574 3.877]
 [6.992 3.568 3.326 ... 3.438 3.643 3.914]
 [4.07  4.598 4.76  ... 3.904 4.33  3.76 ]
 ...
 [4.785 3.791 3.889 ... 4.01  3.725 3.559]
 [4.863 3.836 3.95  ... 4.016 3.828 3.56 ]
 [6.867 3.629 3.36  ... 3.498 3.676 3.918]]
(801, 11) (801, 11)
By feature:  MSE:  0.5535¬±0.0 MAE:  0.4816¬±0.0 R2:  0.2171¬±0.0 PCC:  0.8095¬±0.0 Cosine Similarity:  0.9901¬±0.0
By sample:  MSE:  0.5535¬±0.0 MAE:  0.4816¬±0.0 R2:  0.3058¬±0.0 PCC:  0.5453¬±0.0 Cosine Similarity:  0.9859¬±0.0
scGPT - INFO - By feature: MSE: 0.5534999966621399¬±0.0 MAE: 0.48159998655319214¬±0.0 R2: 0.2171¬±0.0 PCC: 0.8095¬±0.0 Cosine Similarity: 0.9901¬±0.0
scGPT - INFO - By sample: MSE: 0.5534999966621399¬±0.0 MAE: 0.48159998655319214¬±0.0 R2: 0.3058¬±0.0 PCC: 0.5453¬±0.0 Cosine Similarity: 0.9859¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  33
Training model
scGPT - INFO - | epoch  33 | 100/3602 batches | lr 0.0000 | ms/batch 91.96 | loss 11.67 | mse 11.67 | mre  0.00 |
scGPT - INFO - | epoch  33 | 200/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 12.34 | mse 12.34 | mre  0.00 |
scGPT - INFO - | epoch  33 | 300/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 10.28 | mse 10.28 | mre  0.00 |
scGPT - INFO - | epoch  33 | 400/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 11.09 | mse 11.09 | mre  0.00 |
scGPT - INFO - | epoch  33 | 500/3602 batches | lr 0.0000 | ms/batch 91.08 | loss  9.28 | mse  9.28 | mre  0.00 |
scGPT - INFO - | epoch  33 | 600/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 11.05 | mse 11.05 | mre  0.00 |
scGPT - INFO - | epoch  33 | 700/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 11.52 | mse 11.52 | mre  0.00 |
scGPT - INFO - | epoch  33 | 800/3602 batches | lr 0.0000 | ms/batch 91.36 | loss 14.33 | mse 14.33 | mre  0.00 |
scGPT - INFO - | epoch  33 | 900/3602 batches | lr 0.0000 | ms/batch 90.93 | loss 11.22 | mse 11.22 | mre  0.00 |
scGPT - INFO - | epoch  33 | 1000/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 12.57 | mse 12.57 | mre  0.00 |
scGPT - INFO - | epoch  33 | 1100/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 11.21 | mse 11.21 | mre  0.00 |
scGPT - INFO - | epoch  33 | 1200/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 11.85 | mse 11.85 | mre  0.00 |
scGPT - INFO - | epoch  33 | 1300/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 11.41 | mse 11.41 | mre  0.00 |
scGPT - INFO - | epoch  33 | 1400/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 12.82 | mse 12.82 | mre  0.00 |
scGPT - INFO - | epoch  33 | 1500/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 14.18 | mse 14.18 | mre  0.00 |
scGPT - INFO - | epoch  33 | 1600/3602 batches | lr 0.0000 | ms/batch 90.81 | loss  9.98 | mse  9.98 | mre  0.00 |
scGPT - INFO - | epoch  33 | 1700/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 11.22 | mse 11.22 | mre  0.00 |
scGPT - INFO - | epoch  33 | 1800/3602 batches | lr 0.0000 | ms/batch 91.31 | loss 10.36 | mse 10.36 | mre  0.00 |
scGPT - INFO - | epoch  33 | 1900/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 12.02 | mse 12.02 | mre  0.00 |
scGPT - INFO - | epoch  33 | 2000/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 11.77 | mse 11.77 | mre  0.00 |
scGPT - INFO - | epoch  33 | 2100/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 13.12 | mse 13.12 | mre  0.00 |
scGPT - INFO - | epoch  33 | 2200/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 12.03 | mse 12.03 | mre  0.00 |
scGPT - INFO - | epoch  33 | 2300/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 10.30 | mse 10.30 | mre  0.00 |
scGPT - INFO - | epoch  33 | 2400/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 12.16 | mse 12.16 | mre  0.00 |
scGPT - INFO - | epoch  33 | 2500/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 11.10 | mse 11.10 | mre  0.00 |
scGPT - INFO - | epoch  33 | 2600/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 13.68 | mse 13.68 | mre  0.00 |
scGPT - INFO - | epoch  33 | 2700/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 11.00 | mse 11.00 | mre  0.00 |
scGPT - INFO - | epoch  33 | 2800/3602 batches | lr 0.0000 | ms/batch 91.34 | loss 11.36 | mse 11.36 | mre  0.00 |
scGPT - INFO - | epoch  33 | 2900/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 11.82 | mse 11.82 | mre  0.00 |
scGPT - INFO - | epoch  33 | 3000/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 12.05 | mse 12.05 | mre  0.00 |
scGPT - INFO - | epoch  33 | 3100/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 11.44 | mse 11.44 | mre  0.00 |
scGPT - INFO - | epoch  33 | 3200/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 11.41 | mse 11.41 | mre  0.00 |
scGPT - INFO - | epoch  33 | 3300/3602 batches | lr 0.0000 | ms/batch 90.84 | loss  9.61 | mse  9.61 | mre  0.00 |
scGPT - INFO - | epoch  33 | 3400/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 11.42 | mse 11.42 | mre  0.00 |
scGPT - INFO - | epoch  33 | 3500/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 10.66 | mse 10.66 | mre  0.00 |
scGPT - INFO - | epoch  33 | 3600/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 12.07 | mse 12.07 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  33 | time: 339.67s | valid loss/mse 12.1929 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.1191315918826, min_delta 0.0001, val_loss 12.192946523837234
Loss error: -0.07381493195463484
scGPT - INFO - INFO: Early stopping counter 4 of 10
epoch:  33
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.973 3.527 3.268 ... 3.373 3.584 3.893]
 [7.03  3.586 3.318 ... 3.447 3.648 3.93 ]
 [3.867 4.93  4.977 ... 3.75  4.367 3.703]
 ...
 [4.79  3.793 3.883 ... 4.02  3.73  3.578]
 [4.883 3.848 3.95  ... 4.03  3.846 3.586]
 [6.89  3.645 3.348 ... 3.51  3.678 3.934]]
(801, 11) (801, 11)
By feature:  MSE:  0.5543¬±0.0 MAE:  0.4842¬±0.0 R2:  0.2232¬±0.0 PCC:  0.8064¬±0.0 Cosine Similarity:  0.9901¬±0.0
By sample:  MSE:  0.5543¬±0.0 MAE:  0.4842¬±0.0 R2:  0.3023¬±0.0 PCC:  0.5426¬±0.0 Cosine Similarity:  0.9858¬±0.0
scGPT - INFO - By feature: MSE: 0.5543000102043152¬±0.0 MAE: 0.48420000076293945¬±0.0 R2: 0.2232¬±0.0 PCC: 0.8064¬±0.0 Cosine Similarity: 0.9901¬±0.0
scGPT - INFO - By sample: MSE: 0.5543000102043152¬±0.0 MAE: 0.48420000076293945¬±0.0 R2: 0.3023¬±0.0 PCC: 0.5426¬±0.0 Cosine Similarity: 0.9858¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  34
Training model
scGPT - INFO - | epoch  34 | 100/3602 batches | lr 0.0000 | ms/batch 92.02 | loss 10.91 | mse 10.91 | mre  0.00 |
scGPT - INFO - | epoch  34 | 200/3602 batches | lr 0.0000 | ms/batch 91.23 | loss 11.91 | mse 11.91 | mre  0.00 |
scGPT - INFO - | epoch  34 | 300/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 10.25 | mse 10.25 | mre  0.00 |
scGPT - INFO - | epoch  34 | 400/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 10.63 | mse 10.63 | mre  0.00 |
scGPT - INFO - | epoch  34 | 500/3602 batches | lr 0.0000 | ms/batch 90.69 | loss  9.35 | mse  9.35 | mre  0.00 |
scGPT - INFO - | epoch  34 | 600/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 10.79 | mse 10.79 | mre  0.00 |
scGPT - INFO - | epoch  34 | 700/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 11.58 | mse 11.58 | mre  0.00 |
scGPT - INFO - | epoch  34 | 800/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 14.02 | mse 14.02 | mre  0.00 |
scGPT - INFO - | epoch  34 | 900/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 10.91 | mse 10.91 | mre  0.00 |
scGPT - INFO - | epoch  34 | 1000/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 12.71 | mse 12.71 | mre  0.00 |
scGPT - INFO - | epoch  34 | 1100/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 11.16 | mse 11.16 | mre  0.00 |
scGPT - INFO - | epoch  34 | 1200/3602 batches | lr 0.0000 | ms/batch 91.21 | loss 11.59 | mse 11.59 | mre  0.00 |
scGPT - INFO - | epoch  34 | 1300/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 11.58 | mse 11.58 | mre  0.00 |
scGPT - INFO - | epoch  34 | 1400/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 12.84 | mse 12.84 | mre  0.00 |
scGPT - INFO - | epoch  34 | 1500/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 14.35 | mse 14.35 | mre  0.00 |
scGPT - INFO - | epoch  34 | 1600/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 10.19 | mse 10.19 | mre  0.00 |
scGPT - INFO - | epoch  34 | 1700/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 10.80 | mse 10.80 | mre  0.00 |
scGPT - INFO - | epoch  34 | 1800/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 10.41 | mse 10.41 | mre  0.00 |
scGPT - INFO - | epoch  34 | 1900/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 11.81 | mse 11.81 | mre  0.00 |
scGPT - INFO - | epoch  34 | 2000/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 11.30 | mse 11.30 | mre  0.00 |
scGPT - INFO - | epoch  34 | 2100/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 12.97 | mse 12.97 | mre  0.00 |
scGPT - INFO - | epoch  34 | 2200/3602 batches | lr 0.0000 | ms/batch 91.23 | loss 12.58 | mse 12.58 | mre  0.00 |
scGPT - INFO - | epoch  34 | 2300/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 10.49 | mse 10.49 | mre  0.00 |
scGPT - INFO - | epoch  34 | 2400/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 11.61 | mse 11.61 | mre  0.00 |
scGPT - INFO - | epoch  34 | 2500/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.89 | mse 10.89 | mre  0.00 |
scGPT - INFO - | epoch  34 | 2600/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 13.67 | mse 13.67 | mre  0.00 |
scGPT - INFO - | epoch  34 | 2700/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 11.04 | mse 11.04 | mre  0.00 |
scGPT - INFO - | epoch  34 | 2800/3602 batches | lr 0.0000 | ms/batch 91.50 | loss 11.33 | mse 11.33 | mre  0.00 |
scGPT - INFO - | epoch  34 | 2900/3602 batches | lr 0.0000 | ms/batch 92.79 | loss 11.85 | mse 11.85 | mre  0.00 |
scGPT - INFO - | epoch  34 | 3000/3602 batches | lr 0.0000 | ms/batch 92.82 | loss 12.06 | mse 12.06 | mre  0.00 |
scGPT - INFO - | epoch  34 | 3100/3602 batches | lr 0.0000 | ms/batch 92.75 | loss 11.46 | mse 11.46 | mre  0.00 |
scGPT - INFO - | epoch  34 | 3200/3602 batches | lr 0.0000 | ms/batch 93.31 | loss 11.15 | mse 11.15 | mre  0.00 |
scGPT - INFO - | epoch  34 | 3300/3602 batches | lr 0.0000 | ms/batch 92.77 | loss  9.36 | mse  9.36 | mre  0.00 |
scGPT - INFO - | epoch  34 | 3400/3602 batches | lr 0.0000 | ms/batch 92.79 | loss 11.42 | mse 11.42 | mre  0.00 |
scGPT - INFO - | epoch  34 | 3500/3602 batches | lr 0.0000 | ms/batch 92.91 | loss 10.74 | mse 10.74 | mre  0.00 |
scGPT - INFO - | epoch  34 | 3600/3602 batches | lr 0.0000 | ms/batch 92.85 | loss 11.82 | mse 11.82 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  34 | time: 341.69s | valid loss/mse 12.1551 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.1191315918826, min_delta 0.0001, val_loss 12.155134055498387
Loss error: -0.036002463615787406
scGPT - INFO - INFO: Early stopping counter 5 of 10
epoch:  34
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.973 3.51  3.273 ... 3.36  3.588 3.887]
 [7.004 3.576 3.33  ... 3.44  3.654 3.924]
 [3.88  4.66  4.82  ... 3.79  4.31  3.707]
 ...
 [4.754 3.768 3.857 ... 3.99  3.697 3.553]
 [4.88  3.836 3.943 ... 4.016 3.836 3.572]
 [6.887 3.63  3.357 ... 3.5   3.684 3.928]]
(801, 11) (801, 11)
By feature:  MSE:  0.5525¬±0.0 MAE:  0.4831¬±0.0 R2:  0.2184¬±0.0 PCC:  0.8079¬±0.0 Cosine Similarity:  0.9901¬±0.0
By sample:  MSE:  0.5525¬±0.0 MAE:  0.4831¬±0.0 R2:  0.3051¬±0.0 PCC:  0.5458¬±0.0 Cosine Similarity:  0.986¬±0.0
scGPT - INFO - By feature: MSE: 0.5525000095367432¬±0.0 MAE: 0.4830999970436096¬±0.0 R2: 0.2184¬±0.0 PCC: 0.8079¬±0.0 Cosine Similarity: 0.9901¬±0.0
scGPT - INFO - By sample: MSE: 0.5525000095367432¬±0.0 MAE: 0.4830999970436096¬±0.0 R2: 0.3051¬±0.0 PCC: 0.5458¬±0.0 Cosine Similarity: 0.986¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  35
Training model
scGPT - INFO - | epoch  35 | 100/3602 batches | lr 0.0000 | ms/batch 94.16 | loss 11.10 | mse 11.10 | mre  0.00 |
scGPT - INFO - | epoch  35 | 200/3602 batches | lr 0.0000 | ms/batch 92.88 | loss 11.99 | mse 11.99 | mre  0.00 |
scGPT - INFO - | epoch  35 | 300/3602 batches | lr 0.0000 | ms/batch 92.84 | loss 10.23 | mse 10.23 | mre  0.00 |
scGPT - INFO - | epoch  35 | 400/3602 batches | lr 0.0000 | ms/batch 93.01 | loss 10.77 | mse 10.77 | mre  0.00 |
scGPT - INFO - | epoch  35 | 500/3602 batches | lr 0.0000 | ms/batch 92.89 | loss  8.93 | mse  8.93 | mre  0.00 |
scGPT - INFO - | epoch  35 | 600/3602 batches | lr 0.0000 | ms/batch 93.33 | loss 10.72 | mse 10.72 | mre  0.00 |
scGPT - INFO - | epoch  35 | 700/3602 batches | lr 0.0000 | ms/batch 92.89 | loss 11.86 | mse 11.86 | mre  0.00 |
scGPT - INFO - | epoch  35 | 800/3602 batches | lr 0.0000 | ms/batch 92.54 | loss 14.10 | mse 14.10 | mre  0.00 |
scGPT - INFO - | epoch  35 | 900/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 11.13 | mse 11.13 | mre  0.00 |
scGPT - INFO - | epoch  35 | 1000/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 12.36 | mse 12.36 | mre  0.00 |
scGPT - INFO - | epoch  35 | 1100/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 10.73 | mse 10.73 | mre  0.00 |
scGPT - INFO - | epoch  35 | 1200/3602 batches | lr 0.0000 | ms/batch 92.63 | loss 11.60 | mse 11.60 | mre  0.00 |
scGPT - INFO - | epoch  35 | 1300/3602 batches | lr 0.0000 | ms/batch 92.89 | loss 11.31 | mse 11.31 | mre  0.00 |
scGPT - INFO - | epoch  35 | 1400/3602 batches | lr 0.0000 | ms/batch 92.91 | loss 12.70 | mse 12.70 | mre  0.00 |
scGPT - INFO - | epoch  35 | 1500/3602 batches | lr 0.0000 | ms/batch 92.90 | loss 14.54 | mse 14.54 | mre  0.00 |
scGPT - INFO - | epoch  35 | 1600/3602 batches | lr 0.0000 | ms/batch 93.41 | loss 10.00 | mse 10.00 | mre  0.00 |
scGPT - INFO - | epoch  35 | 1700/3602 batches | lr 0.0000 | ms/batch 92.83 | loss 10.79 | mse 10.79 | mre  0.00 |
scGPT - INFO - | epoch  35 | 1800/3602 batches | lr 0.0000 | ms/batch 92.85 | loss 10.12 | mse 10.12 | mre  0.00 |
scGPT - INFO - | epoch  35 | 1900/3602 batches | lr 0.0000 | ms/batch 92.86 | loss 12.23 | mse 12.23 | mre  0.00 |
scGPT - INFO - | epoch  35 | 2000/3602 batches | lr 0.0000 | ms/batch 92.89 | loss 11.52 | mse 11.52 | mre  0.00 |
scGPT - INFO - | epoch  35 | 2100/3602 batches | lr 0.0000 | ms/batch 92.83 | loss 13.49 | mse 13.49 | mre  0.00 |
scGPT - INFO - | epoch  35 | 2200/3602 batches | lr 0.0000 | ms/batch 92.87 | loss 12.11 | mse 12.11 | mre  0.00 |
scGPT - INFO - | epoch  35 | 2300/3602 batches | lr 0.0000 | ms/batch 92.88 | loss 10.73 | mse 10.73 | mre  0.00 |
scGPT - INFO - | epoch  35 | 2400/3602 batches | lr 0.0000 | ms/batch 92.96 | loss 11.76 | mse 11.76 | mre  0.00 |
scGPT - INFO - | epoch  35 | 2500/3602 batches | lr 0.0000 | ms/batch 92.91 | loss 11.26 | mse 11.26 | mre  0.00 |
scGPT - INFO - | epoch  35 | 2600/3602 batches | lr 0.0000 | ms/batch 93.42 | loss 13.52 | mse 13.52 | mre  0.00 |
scGPT - INFO - | epoch  35 | 2700/3602 batches | lr 0.0000 | ms/batch 92.87 | loss 11.07 | mse 11.07 | mre  0.00 |
scGPT - INFO - | epoch  35 | 2800/3602 batches | lr 0.0000 | ms/batch 92.83 | loss 12.00 | mse 12.00 | mre  0.00 |
scGPT - INFO - | epoch  35 | 2900/3602 batches | lr 0.0000 | ms/batch 92.85 | loss 11.82 | mse 11.82 | mre  0.00 |
scGPT - INFO - | epoch  35 | 3000/3602 batches | lr 0.0000 | ms/batch 92.89 | loss 11.87 | mse 11.87 | mre  0.00 |
scGPT - INFO - | epoch  35 | 3100/3602 batches | lr 0.0000 | ms/batch 92.79 | loss 11.31 | mse 11.31 | mre  0.00 |
scGPT - INFO - | epoch  35 | 3200/3602 batches | lr 0.0000 | ms/batch 92.79 | loss 11.14 | mse 11.14 | mre  0.00 |
scGPT - INFO - | epoch  35 | 3300/3602 batches | lr 0.0000 | ms/batch 92.90 | loss  9.62 | mse  9.62 | mre  0.00 |
scGPT - INFO - | epoch  35 | 3400/3602 batches | lr 0.0000 | ms/batch 92.87 | loss 11.17 | mse 11.17 | mre  0.00 |
scGPT - INFO - | epoch  35 | 3500/3602 batches | lr 0.0000 | ms/batch 92.86 | loss 10.85 | mse 10.85 | mre  0.00 |
scGPT - INFO - | epoch  35 | 3600/3602 batches | lr 0.0000 | ms/batch 93.37 | loss 11.77 | mse 11.77 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  35 | time: 346.50s | valid loss/mse 12.0983 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.0983
best_loss: 12.1191315918826, min_delta 0.0001, val_loss 12.09829021564286
Loss error: 0.020841376239738807
epoch:  35
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.965 3.502 3.252 ... 3.342 3.568 3.871]
 [7.02  3.57  3.312 ... 3.424 3.639 3.916]
 [3.986 4.727 4.867 ... 3.818 4.34  3.729]
 ...
 [4.742 3.771 3.86  ... 3.988 3.695 3.547]
 [4.863 3.836 3.941 ... 4.01  3.83  3.566]
 [6.887 3.623 3.342 ... 3.488 3.668 3.92 ]]
(801, 11) (801, 11)
By feature:  MSE:  0.55¬±0.0 MAE:  0.4809¬±0.0 R2:  0.2252¬±0.0 PCC:  0.8096¬±0.0 Cosine Similarity:  0.9902¬±0.0
By sample:  MSE:  0.55¬±0.0 MAE:  0.4809¬±0.0 R2:  0.3086¬±0.0 PCC:  0.5486¬±0.0 Cosine Similarity:  0.9859¬±0.0
scGPT - INFO - By feature: MSE: 0.550000011920929¬±0.0 MAE: 0.48089998960494995¬±0.0 R2: 0.2252¬±0.0 PCC: 0.8096¬±0.0 Cosine Similarity: 0.9902¬±0.0
scGPT - INFO - By sample: MSE: 0.550000011920929¬±0.0 MAE: 0.48089998960494995¬±0.0 R2: 0.3086¬±0.0 PCC: 0.5486¬±0.0 Cosine Similarity: 0.9859¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  36
Training model
scGPT - INFO - | epoch  36 | 100/3602 batches | lr 0.0000 | ms/batch 92.12 | loss 10.95 | mse 10.95 | mre  0.00 |
scGPT - INFO - | epoch  36 | 200/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 11.77 | mse 11.77 | mre  0.00 |
scGPT - INFO - | epoch  36 | 300/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 10.31 | mse 10.31 | mre  0.00 |
scGPT - INFO - | epoch  36 | 400/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 10.42 | mse 10.42 | mre  0.00 |
scGPT - INFO - | epoch  36 | 500/3602 batches | lr 0.0000 | ms/batch 90.89 | loss  9.12 | mse  9.12 | mre  0.00 |
scGPT - INFO - | epoch  36 | 600/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 10.76 | mse 10.76 | mre  0.00 |
scGPT - INFO - | epoch  36 | 700/3602 batches | lr 0.0000 | ms/batch 90.93 | loss 11.60 | mse 11.60 | mre  0.00 |
scGPT - INFO - | epoch  36 | 800/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 13.75 | mse 13.75 | mre  0.00 |
scGPT - INFO - | epoch  36 | 900/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.82 | mse 10.82 | mre  0.00 |
scGPT - INFO - | epoch  36 | 1000/3602 batches | lr 0.0000 | ms/batch 91.42 | loss 12.71 | mse 12.71 | mre  0.00 |
scGPT - INFO - | epoch  36 | 1100/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 11.48 | mse 11.48 | mre  0.00 |
scGPT - INFO - | epoch  36 | 1200/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 11.63 | mse 11.63 | mre  0.00 |
scGPT - INFO - | epoch  36 | 1300/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 11.01 | mse 11.01 | mre  0.00 |
scGPT - INFO - | epoch  36 | 1400/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 12.86 | mse 12.86 | mre  0.00 |
scGPT - INFO - | epoch  36 | 1500/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 14.40 | mse 14.40 | mre  0.00 |
scGPT - INFO - | epoch  36 | 1600/3602 batches | lr 0.0000 | ms/batch 90.77 | loss  9.75 | mse  9.75 | mre  0.00 |
scGPT - INFO - | epoch  36 | 1700/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 10.80 | mse 10.80 | mre  0.00 |
scGPT - INFO - | epoch  36 | 1800/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 10.06 | mse 10.06 | mre  0.00 |
scGPT - INFO - | epoch  36 | 1900/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 11.90 | mse 11.90 | mre  0.00 |
scGPT - INFO - | epoch  36 | 2000/3602 batches | lr 0.0000 | ms/batch 91.45 | loss 11.72 | mse 11.72 | mre  0.00 |
scGPT - INFO - | epoch  36 | 2100/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 12.72 | mse 12.72 | mre  0.00 |
scGPT - INFO - | epoch  36 | 2200/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 12.31 | mse 12.31 | mre  0.00 |
scGPT - INFO - | epoch  36 | 2300/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 10.27 | mse 10.27 | mre  0.00 |
scGPT - INFO - | epoch  36 | 2400/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 12.01 | mse 12.01 | mre  0.00 |
scGPT - INFO - | epoch  36 | 2500/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 11.04 | mse 11.04 | mre  0.00 |
scGPT - INFO - | epoch  36 | 2600/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 13.65 | mse 13.65 | mre  0.00 |
scGPT - INFO - | epoch  36 | 2700/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 10.64 | mse 10.64 | mre  0.00 |
scGPT - INFO - | epoch  36 | 2800/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 11.60 | mse 11.60 | mre  0.00 |
scGPT - INFO - | epoch  36 | 2900/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 11.61 | mse 11.61 | mre  0.00 |
scGPT - INFO - | epoch  36 | 3000/3602 batches | lr 0.0000 | ms/batch 91.41 | loss 12.26 | mse 12.26 | mre  0.00 |
scGPT - INFO - | epoch  36 | 3100/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 10.86 | mse 10.86 | mre  0.00 |
scGPT - INFO - | epoch  36 | 3200/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 11.44 | mse 11.44 | mre  0.00 |
scGPT - INFO - | epoch  36 | 3300/3602 batches | lr 0.0000 | ms/batch 90.83 | loss  9.48 | mse  9.48 | mre  0.00 |
scGPT - INFO - | epoch  36 | 3400/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 11.34 | mse 11.34 | mre  0.00 |
scGPT - INFO - | epoch  36 | 3500/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 10.59 | mse 10.59 | mre  0.00 |
scGPT - INFO - | epoch  36 | 3600/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 11.42 | mse 11.42 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  36 | time: 339.81s | valid loss/mse 12.1320 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.09829021564286, min_delta 0.0001, val_loss 12.131996166542377
Loss error: -0.03370595089951678
scGPT - INFO - INFO: Early stopping counter 1 of 10
epoch:  36
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.996 3.508 3.27  ... 3.355 3.588 3.883]
 [7.023 3.572 3.322 ... 3.434 3.65  3.918]
 [4.082 4.855 4.918 ... 3.828 4.35  3.725]
 ...
 [4.746 3.78  3.861 ... 3.996 3.703 3.549]
 [4.87  3.846 3.947 ... 4.02  3.842 3.57 ]
 [6.9   3.627 3.352 ... 3.496 3.682 3.924]]
(801, 11) (801, 11)
By feature:  MSE:  0.5515¬±0.0 MAE:  0.4812¬±0.0 R2:  0.2208¬±0.0 PCC:  0.8107¬±0.0 Cosine Similarity:  0.9902¬±0.0
By sample:  MSE:  0.5515¬±0.0 MAE:  0.4812¬±0.0 R2:  0.3083¬±0.0 PCC:  0.5486¬±0.0 Cosine Similarity:  0.986¬±0.0
scGPT - INFO - By feature: MSE: 0.5515000224113464¬±0.0 MAE: 0.4812000095844269¬±0.0 R2: 0.2208¬±0.0 PCC: 0.8107¬±0.0 Cosine Similarity: 0.9902¬±0.0
scGPT - INFO - By sample: MSE: 0.5515000224113464¬±0.0 MAE: 0.4812000095844269¬±0.0 R2: 0.3083¬±0.0 PCC: 0.5486¬±0.0 Cosine Similarity: 0.986¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  37
Training model
scGPT - INFO - | epoch  37 | 100/3602 batches | lr 0.0000 | ms/batch 92.02 | loss 11.11 | mse 11.11 | mre  0.00 |
scGPT - INFO - | epoch  37 | 200/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 12.20 | mse 12.20 | mre  0.00 |
scGPT - INFO - | epoch  37 | 300/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 10.45 | mse 10.45 | mre  0.00 |
scGPT - INFO - | epoch  37 | 400/3602 batches | lr 0.0000 | ms/batch 91.26 | loss 10.49 | mse 10.49 | mre  0.00 |
scGPT - INFO - | epoch  37 | 500/3602 batches | lr 0.0000 | ms/batch 90.67 | loss  9.12 | mse  9.12 | mre  0.00 |
scGPT - INFO - | epoch  37 | 600/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 10.50 | mse 10.50 | mre  0.00 |
scGPT - INFO - | epoch  37 | 700/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 11.52 | mse 11.52 | mre  0.00 |
scGPT - INFO - | epoch  37 | 800/3602 batches | lr 0.0000 | ms/batch 91.25 | loss 14.20 | mse 14.20 | mre  0.00 |
scGPT - INFO - | epoch  37 | 900/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 10.70 | mse 10.70 | mre  0.00 |
scGPT - INFO - | epoch  37 | 1000/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 12.81 | mse 12.81 | mre  0.00 |
scGPT - INFO - | epoch  37 | 1100/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 10.88 | mse 10.88 | mre  0.00 |
scGPT - INFO - | epoch  37 | 1200/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 11.53 | mse 11.53 | mre  0.00 |
scGPT - INFO - | epoch  37 | 1300/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 11.13 | mse 11.13 | mre  0.00 |
scGPT - INFO - | epoch  37 | 1400/3602 batches | lr 0.0000 | ms/batch 91.27 | loss 12.67 | mse 12.67 | mre  0.00 |
scGPT - INFO - | epoch  37 | 1500/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 13.95 | mse 13.95 | mre  0.00 |
scGPT - INFO - | epoch  37 | 1600/3602 batches | lr 0.0000 | ms/batch 90.77 | loss  9.84 | mse  9.84 | mre  0.00 |
scGPT - INFO - | epoch  37 | 1700/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 10.28 | mse 10.28 | mre  0.00 |
scGPT - INFO - | epoch  37 | 1800/3602 batches | lr 0.0000 | ms/batch 90.70 | loss  9.85 | mse  9.85 | mre  0.00 |
scGPT - INFO - | epoch  37 | 1900/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 11.59 | mse 11.59 | mre  0.00 |
scGPT - INFO - | epoch  37 | 2000/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 11.94 | mse 11.94 | mre  0.00 |
scGPT - INFO - | epoch  37 | 2100/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 13.09 | mse 13.09 | mre  0.00 |
scGPT - INFO - | epoch  37 | 2200/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 11.99 | mse 11.99 | mre  0.00 |
scGPT - INFO - | epoch  37 | 2300/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 10.41 | mse 10.41 | mre  0.00 |
scGPT - INFO - | epoch  37 | 2400/3602 batches | lr 0.0000 | ms/batch 91.25 | loss 11.76 | mse 11.76 | mre  0.00 |
scGPT - INFO - | epoch  37 | 2500/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 10.98 | mse 10.98 | mre  0.00 |
scGPT - INFO - | epoch  37 | 2600/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 13.53 | mse 13.53 | mre  0.00 |
scGPT - INFO - | epoch  37 | 2700/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 10.71 | mse 10.71 | mre  0.00 |
scGPT - INFO - | epoch  37 | 2800/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 11.28 | mse 11.28 | mre  0.00 |
scGPT - INFO - | epoch  37 | 2900/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 11.30 | mse 11.30 | mre  0.00 |
scGPT - INFO - | epoch  37 | 3000/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 12.13 | mse 12.13 | mre  0.00 |
scGPT - INFO - | epoch  37 | 3100/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 11.07 | mse 11.07 | mre  0.00 |
scGPT - INFO - | epoch  37 | 3200/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 11.34 | mse 11.34 | mre  0.00 |
scGPT - INFO - | epoch  37 | 3300/3602 batches | lr 0.0000 | ms/batch 90.69 | loss  9.38 | mse  9.38 | mre  0.00 |
scGPT - INFO - | epoch  37 | 3400/3602 batches | lr 0.0000 | ms/batch 91.24 | loss 11.55 | mse 11.55 | mre  0.00 |
scGPT - INFO - | epoch  37 | 3500/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 10.88 | mse 10.88 | mre  0.00 |
scGPT - INFO - | epoch  37 | 3600/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 11.68 | mse 11.68 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  37 | time: 339.43s | valid loss/mse 12.0929 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.0929
best_loss: 12.09829021564286, min_delta 0.0001, val_loss 12.092857617117492
Loss error: 0.005432598525368704
epoch:  37
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.96  3.502 3.244 ... 3.334 3.566 3.867]
 [7.016 3.57  3.299 ... 3.414 3.633 3.912]
 [3.969 4.797 4.875 ... 3.795 4.336 3.709]
 ...
 [4.758 3.79  3.86  ... 4.    3.707 3.56 ]
 [4.89  3.855 3.947 ... 4.027 3.854 3.584]
 [6.87  3.633 3.334 ... 3.488 3.672 3.918]]
(801, 11) (801, 11)
By feature:  MSE:  0.5497¬±0.0 MAE:  0.4805¬±0.0 R2:  0.2237¬±0.0 PCC:  0.8084¬±0.0 Cosine Similarity:  0.9901¬±0.0
By sample:  MSE:  0.5497¬±0.0 MAE:  0.4805¬±0.0 R2:  0.3095¬±0.0 PCC:  0.5505¬±0.0 Cosine Similarity:  0.986¬±0.0
scGPT - INFO - By feature: MSE: 0.5497000217437744¬±0.0 MAE: 0.4805000126361847¬±0.0 R2: 0.2237¬±0.0 PCC: 0.8084¬±0.0 Cosine Similarity: 0.9901¬±0.0
scGPT - INFO - By sample: MSE: 0.5497000217437744¬±0.0 MAE: 0.4805000126361847¬±0.0 R2: 0.3095¬±0.0 PCC: 0.5505¬±0.0 Cosine Similarity: 0.986¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  38
Training model
scGPT - INFO - | epoch  38 | 100/3602 batches | lr 0.0000 | ms/batch 91.96 | loss 10.99 | mse 10.99 | mre  0.00 |
scGPT - INFO - | epoch  38 | 200/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 12.00 | mse 12.00 | mre  0.00 |
scGPT - INFO - | epoch  38 | 300/3602 batches | lr 0.0000 | ms/batch 90.73 | loss  9.90 | mse  9.90 | mre  0.00 |
scGPT - INFO - | epoch  38 | 400/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 10.17 | mse 10.17 | mre  0.00 |
scGPT - INFO - | epoch  38 | 500/3602 batches | lr 0.0000 | ms/batch 90.79 | loss  8.79 | mse  8.79 | mre  0.00 |
scGPT - INFO - | epoch  38 | 600/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.57 | mse 10.57 | mre  0.00 |
scGPT - INFO - | epoch  38 | 700/3602 batches | lr 0.0000 | ms/batch 91.71 | loss 11.42 | mse 11.42 | mre  0.00 |
scGPT - INFO - | epoch  38 | 800/3602 batches | lr 0.0000 | ms/batch 93.31 | loss 14.02 | mse 14.02 | mre  0.00 |
scGPT - INFO - | epoch  38 | 900/3602 batches | lr 0.0000 | ms/batch 92.78 | loss 10.80 | mse 10.80 | mre  0.00 |
scGPT - INFO - | epoch  38 | 1000/3602 batches | lr 0.0000 | ms/batch 92.76 | loss 12.60 | mse 12.60 | mre  0.00 |
scGPT - INFO - | epoch  38 | 1100/3602 batches | lr 0.0000 | ms/batch 92.77 | loss 11.03 | mse 11.03 | mre  0.00 |
scGPT - INFO - | epoch  38 | 1200/3602 batches | lr 0.0000 | ms/batch 92.80 | loss 11.30 | mse 11.30 | mre  0.00 |
scGPT - INFO - | epoch  38 | 1300/3602 batches | lr 0.0000 | ms/batch 92.80 | loss 11.40 | mse 11.40 | mre  0.00 |
scGPT - INFO - | epoch  38 | 1400/3602 batches | lr 0.0000 | ms/batch 92.76 | loss 12.48 | mse 12.48 | mre  0.00 |
scGPT - INFO - | epoch  38 | 1500/3602 batches | lr 0.0000 | ms/batch 93.19 | loss 14.42 | mse 14.42 | mre  0.00 |
scGPT - INFO - | epoch  38 | 1600/3602 batches | lr 0.0000 | ms/batch 92.89 | loss  9.80 | mse  9.80 | mre  0.00 |
scGPT - INFO - | epoch  38 | 1700/3602 batches | lr 0.0000 | ms/batch 92.84 | loss 10.54 | mse 10.54 | mre  0.00 |
scGPT - INFO - | epoch  38 | 1800/3602 batches | lr 0.0000 | ms/batch 93.28 | loss  9.96 | mse  9.96 | mre  0.00 |
scGPT - INFO - | epoch  38 | 1900/3602 batches | lr 0.0000 | ms/batch 92.87 | loss 12.01 | mse 12.01 | mre  0.00 |
scGPT - INFO - | epoch  38 | 2000/3602 batches | lr 0.0000 | ms/batch 92.75 | loss 11.52 | mse 11.52 | mre  0.00 |
scGPT - INFO - | epoch  38 | 2100/3602 batches | lr 0.0000 | ms/batch 92.75 | loss 12.74 | mse 12.74 | mre  0.00 |
scGPT - INFO - | epoch  38 | 2200/3602 batches | lr 0.0000 | ms/batch 92.82 | loss 12.44 | mse 12.44 | mre  0.00 |
scGPT - INFO - | epoch  38 | 2300/3602 batches | lr 0.0000 | ms/batch 92.71 | loss 10.30 | mse 10.30 | mre  0.00 |
scGPT - INFO - | epoch  38 | 2400/3602 batches | lr 0.0000 | ms/batch 92.77 | loss 11.72 | mse 11.72 | mre  0.00 |
scGPT - INFO - | epoch  38 | 2500/3602 batches | lr 0.0000 | ms/batch 92.82 | loss 10.88 | mse 10.88 | mre  0.00 |
scGPT - INFO - | epoch  38 | 2600/3602 batches | lr 0.0000 | ms/batch 92.87 | loss 13.43 | mse 13.43 | mre  0.00 |
scGPT - INFO - | epoch  38 | 2700/3602 batches | lr 0.0000 | ms/batch 92.80 | loss 10.75 | mse 10.75 | mre  0.00 |
scGPT - INFO - | epoch  38 | 2800/3602 batches | lr 0.0000 | ms/batch 93.38 | loss 11.41 | mse 11.41 | mre  0.00 |
scGPT - INFO - | epoch  38 | 2900/3602 batches | lr 0.0000 | ms/batch 92.83 | loss 11.90 | mse 11.90 | mre  0.00 |
scGPT - INFO - | epoch  38 | 3000/3602 batches | lr 0.0000 | ms/batch 92.87 | loss 12.02 | mse 12.02 | mre  0.00 |
scGPT - INFO - | epoch  38 | 3100/3602 batches | lr 0.0000 | ms/batch 92.88 | loss 10.92 | mse 10.92 | mre  0.00 |
scGPT - INFO - | epoch  38 | 3200/3602 batches | lr 0.0000 | ms/batch 92.98 | loss 11.21 | mse 11.21 | mre  0.00 |
scGPT - INFO - | epoch  38 | 3300/3602 batches | lr 0.0000 | ms/batch 93.00 | loss  9.37 | mse  9.37 | mre  0.00 |
scGPT - INFO - | epoch  38 | 3400/3602 batches | lr 0.0000 | ms/batch 92.95 | loss 11.22 | mse 11.22 | mre  0.00 |
scGPT - INFO - | epoch  38 | 3500/3602 batches | lr 0.0000 | ms/batch 92.90 | loss 10.26 | mse 10.26 | mre  0.00 |
scGPT - INFO - | epoch  38 | 3600/3602 batches | lr 0.0000 | ms/batch 92.89 | loss 11.75 | mse 11.75 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  38 | time: 346.23s | valid loss/mse 12.1017 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.092857617117492, min_delta 0.0001, val_loss 12.10169450144345
Loss error: -0.00883688432595875
scGPT - INFO - INFO: Early stopping counter 1 of 10
epoch:  38
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.977 3.49  3.246 ... 3.338 3.566 3.865]
 [7.023 3.56  3.297 ... 3.414 3.629 3.906]
 [4.15  4.76  4.844 ... 3.863 4.344 3.742]
 ...
 [4.746 3.78  3.852 ... 3.994 3.695 3.541]
 [4.87  3.842 3.934 ... 4.016 3.834 3.564]
 [6.88  3.63  3.334 ... 3.492 3.67  3.914]]
(801, 11) (801, 11)
By feature:  MSE:  0.5501¬±0.0 MAE:  0.4804¬±0.0 R2:  0.2176¬±0.0 PCC:  0.8115¬±0.0 Cosine Similarity:  0.9902¬±0.0
By sample:  MSE:  0.5501¬±0.0 MAE:  0.4804¬±0.0 R2:  0.3092¬±0.0 PCC:  0.5511¬±0.0 Cosine Similarity:  0.986¬±0.0
scGPT - INFO - By feature: MSE: 0.5501000285148621¬±0.0 MAE: 0.4803999960422516¬±0.0 R2: 0.2176¬±0.0 PCC: 0.8115¬±0.0 Cosine Similarity: 0.9902¬±0.0
scGPT - INFO - By sample: MSE: 0.5501000285148621¬±0.0 MAE: 0.4803999960422516¬±0.0 R2: 0.3092¬±0.0 PCC: 0.5511¬±0.0 Cosine Similarity: 0.986¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  39
Training model
scGPT - INFO - | epoch  39 | 100/3602 batches | lr 0.0000 | ms/batch 94.22 | loss 11.24 | mse 11.24 | mre  0.00 |
scGPT - INFO - | epoch  39 | 200/3602 batches | lr 0.0000 | ms/batch 93.47 | loss 12.38 | mse 12.38 | mre  0.00 |
scGPT - INFO - | epoch  39 | 300/3602 batches | lr 0.0000 | ms/batch 92.92 | loss 10.09 | mse 10.09 | mre  0.00 |
scGPT - INFO - | epoch  39 | 400/3602 batches | lr 0.0000 | ms/batch 92.77 | loss 10.36 | mse 10.36 | mre  0.00 |
scGPT - INFO - | epoch  39 | 500/3602 batches | lr 0.0000 | ms/batch 92.79 | loss  9.20 | mse  9.20 | mre  0.00 |
scGPT - INFO - | epoch  39 | 600/3602 batches | lr 0.0000 | ms/batch 92.83 | loss 10.66 | mse 10.66 | mre  0.00 |
scGPT - INFO - | epoch  39 | 700/3602 batches | lr 0.0000 | ms/batch 92.87 | loss 11.31 | mse 11.31 | mre  0.00 |
scGPT - INFO - | epoch  39 | 800/3602 batches | lr 0.0000 | ms/batch 92.85 | loss 13.88 | mse 13.88 | mre  0.00 |
scGPT - INFO - | epoch  39 | 900/3602 batches | lr 0.0000 | ms/batch 91.72 | loss 10.92 | mse 10.92 | mre  0.00 |
scGPT - INFO - | epoch  39 | 1000/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 12.71 | mse 12.71 | mre  0.00 |
scGPT - INFO - | epoch  39 | 1100/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 11.11 | mse 11.11 | mre  0.00 |
scGPT - INFO - | epoch  39 | 1200/3602 batches | lr 0.0000 | ms/batch 91.31 | loss 11.55 | mse 11.55 | mre  0.00 |
scGPT - INFO - | epoch  39 | 1300/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 11.07 | mse 11.07 | mre  0.00 |
scGPT - INFO - | epoch  39 | 1400/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 12.62 | mse 12.62 | mre  0.00 |
scGPT - INFO - | epoch  39 | 1500/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 15.03 | mse 15.03 | mre  0.00 |
scGPT - INFO - | epoch  39 | 1600/3602 batches | lr 0.0000 | ms/batch 90.69 | loss  9.80 | mse  9.80 | mre  0.00 |
scGPT - INFO - | epoch  39 | 1700/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 10.64 | mse 10.64 | mre  0.00 |
scGPT - INFO - | epoch  39 | 1800/3602 batches | lr 0.0000 | ms/batch 90.76 | loss  9.97 | mse  9.97 | mre  0.00 |
scGPT - INFO - | epoch  39 | 1900/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 11.95 | mse 11.95 | mre  0.00 |
scGPT - INFO - | epoch  39 | 2000/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 11.53 | mse 11.53 | mre  0.00 |
scGPT - INFO - | epoch  39 | 2100/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 12.86 | mse 12.86 | mre  0.00 |
scGPT - INFO - | epoch  39 | 2200/3602 batches | lr 0.0000 | ms/batch 91.36 | loss 11.94 | mse 11.94 | mre  0.00 |
scGPT - INFO - | epoch  39 | 2300/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 10.03 | mse 10.03 | mre  0.00 |
scGPT - INFO - | epoch  39 | 2400/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 11.70 | mse 11.70 | mre  0.00 |
scGPT - INFO - | epoch  39 | 2500/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 10.30 | mse 10.30 | mre  0.00 |
scGPT - INFO - | epoch  39 | 2600/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 13.69 | mse 13.69 | mre  0.00 |
scGPT - INFO - | epoch  39 | 2700/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 10.81 | mse 10.81 | mre  0.00 |
scGPT - INFO - | epoch  39 | 2800/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 11.28 | mse 11.28 | mre  0.00 |
scGPT - INFO - | epoch  39 | 2900/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 11.52 | mse 11.52 | mre  0.00 |
scGPT - INFO - | epoch  39 | 3000/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 11.89 | mse 11.89 | mre  0.00 |
scGPT - INFO - | epoch  39 | 3100/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 11.02 | mse 11.02 | mre  0.00 |
scGPT - INFO - | epoch  39 | 3200/3602 batches | lr 0.0000 | ms/batch 91.32 | loss 11.60 | mse 11.60 | mre  0.00 |
scGPT - INFO - | epoch  39 | 3300/3602 batches | lr 0.0000 | ms/batch 90.84 | loss  9.53 | mse  9.53 | mre  0.00 |
scGPT - INFO - | epoch  39 | 3400/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 11.24 | mse 11.24 | mre  0.00 |
scGPT - INFO - | epoch  39 | 3500/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 10.44 | mse 10.44 | mre  0.00 |
scGPT - INFO - | epoch  39 | 3600/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 11.78 | mse 11.78 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  39 | time: 341.34s | valid loss/mse 12.0248 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.0248
best_loss: 12.092857617117492, min_delta 0.0001, val_loss 12.024755670187922
Loss error: 0.06810194692956983
epoch:  39
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.957 3.494 3.242 ... 3.332 3.566 3.861]
 [7.01  3.572 3.299 ... 3.412 3.635 3.906]
 [4.125 4.8   4.84  ... 3.846 4.32  3.72 ]
 ...
 [4.72  3.771 3.838 ... 3.982 3.684 3.533]
 [4.855 3.838 3.922 ... 4.008 3.826 3.562]
 [6.875 3.637 3.332 ... 3.488 3.672 3.914]]
(801, 11) (801, 11)
By feature:  MSE:  0.5466¬±0.0 MAE:  0.4782¬±0.0 R2:  0.224¬±0.0 PCC:  0.812¬±0.0 Cosine Similarity:  0.9902¬±0.0
By sample:  MSE:  0.5466¬±0.0 MAE:  0.4782¬±0.0 R2:  0.313¬±0.0 PCC:  0.5531¬±0.0 Cosine Similarity:  0.9861¬±0.0
scGPT - INFO - By feature: MSE: 0.5465999841690063¬±0.0 MAE: 0.4781999886035919¬±0.0 R2: 0.224¬±0.0 PCC: 0.812¬±0.0 Cosine Similarity: 0.9902¬±0.0
scGPT - INFO - By sample: MSE: 0.5465999841690063¬±0.0 MAE: 0.4781999886035919¬±0.0 R2: 0.313¬±0.0 PCC: 0.5531¬±0.0 Cosine Similarity: 0.9861¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  40
Training model
scGPT - INFO - | epoch  40 | 100/3602 batches | lr 0.0000 | ms/batch 92.10 | loss 11.31 | mse 11.31 | mre  0.00 |
scGPT - INFO - | epoch  40 | 200/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 12.14 | mse 12.14 | mre  0.00 |
scGPT - INFO - | epoch  40 | 300/3602 batches | lr 0.0000 | ms/batch 90.92 | loss  9.95 | mse  9.95 | mre  0.00 |
scGPT - INFO - | epoch  40 | 400/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 10.52 | mse 10.52 | mre  0.00 |
scGPT - INFO - | epoch  40 | 500/3602 batches | lr 0.0000 | ms/batch 90.85 | loss  9.15 | mse  9.15 | mre  0.00 |
scGPT - INFO - | epoch  40 | 600/3602 batches | lr 0.0000 | ms/batch 91.42 | loss 10.74 | mse 10.74 | mre  0.00 |
scGPT - INFO - | epoch  40 | 700/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 11.50 | mse 11.50 | mre  0.00 |
scGPT - INFO - | epoch  40 | 800/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 13.83 | mse 13.83 | mre  0.00 |
scGPT - INFO - | epoch  40 | 900/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 11.25 | mse 11.25 | mre  0.00 |
scGPT - INFO - | epoch  40 | 1000/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 12.63 | mse 12.63 | mre  0.00 |
scGPT - INFO - | epoch  40 | 1100/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 11.27 | mse 11.27 | mre  0.00 |
scGPT - INFO - | epoch  40 | 1200/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 11.15 | mse 11.15 | mre  0.00 |
scGPT - INFO - | epoch  40 | 1300/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 11.46 | mse 11.46 | mre  0.00 |
scGPT - INFO - | epoch  40 | 1400/3602 batches | lr 0.0000 | ms/batch 90.95 | loss 12.72 | mse 12.72 | mre  0.00 |
scGPT - INFO - | epoch  40 | 1500/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 14.52 | mse 14.52 | mre  0.00 |
scGPT - INFO - | epoch  40 | 1600/3602 batches | lr 0.0000 | ms/batch 91.49 | loss  9.90 | mse  9.90 | mre  0.00 |
scGPT - INFO - | epoch  40 | 1700/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 10.86 | mse 10.86 | mre  0.00 |
scGPT - INFO - | epoch  40 | 1800/3602 batches | lr 0.0000 | ms/batch 90.86 | loss  9.75 | mse  9.75 | mre  0.00 |
scGPT - INFO - | epoch  40 | 1900/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 11.62 | mse 11.62 | mre  0.00 |
scGPT - INFO - | epoch  40 | 2000/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 11.35 | mse 11.35 | mre  0.00 |
scGPT - INFO - | epoch  40 | 2100/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 13.22 | mse 13.22 | mre  0.00 |
scGPT - INFO - | epoch  40 | 2200/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 12.20 | mse 12.20 | mre  0.00 |
scGPT - INFO - | epoch  40 | 2300/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 10.28 | mse 10.28 | mre  0.00 |
scGPT - INFO - | epoch  40 | 2400/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 12.05 | mse 12.05 | mre  0.00 |
scGPT - INFO - | epoch  40 | 2500/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 10.43 | mse 10.43 | mre  0.00 |
scGPT - INFO - | epoch  40 | 2600/3602 batches | lr 0.0000 | ms/batch 91.42 | loss 14.16 | mse 14.16 | mre  0.00 |
scGPT - INFO - | epoch  40 | 2700/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 10.66 | mse 10.66 | mre  0.00 |
scGPT - INFO - | epoch  40 | 2800/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 11.37 | mse 11.37 | mre  0.00 |
scGPT - INFO - | epoch  40 | 2900/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 11.47 | mse 11.47 | mre  0.00 |
scGPT - INFO - | epoch  40 | 3000/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 12.11 | mse 12.11 | mre  0.00 |
scGPT - INFO - | epoch  40 | 3100/3602 batches | lr 0.0000 | ms/batch 91.99 | loss 10.99 | mse 10.99 | mre  0.00 |
scGPT - INFO - | epoch  40 | 3200/3602 batches | lr 0.0000 | ms/batch 92.78 | loss 11.17 | mse 11.17 | mre  0.00 |
scGPT - INFO - | epoch  40 | 3300/3602 batches | lr 0.0000 | ms/batch 92.88 | loss  9.42 | mse  9.42 | mre  0.00 |
scGPT - INFO - | epoch  40 | 3400/3602 batches | lr 0.0000 | ms/batch 92.78 | loss 11.15 | mse 11.15 | mre  0.00 |
scGPT - INFO - | epoch  40 | 3500/3602 batches | lr 0.0000 | ms/batch 92.97 | loss 10.60 | mse 10.60 | mre  0.00 |
scGPT - INFO - | epoch  40 | 3600/3602 batches | lr 0.0000 | ms/batch 93.76 | loss 11.62 | mse 11.62 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  40 | time: 341.64s | valid loss/mse 12.0095 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.0095
best_loss: 12.024755670187922, min_delta 0.0001, val_loss 12.009459221482128
Loss error: 0.015296448705793608
epoch:  40
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.95  3.492 3.236 ... 3.328 3.564 3.855]
 [7.008 3.572 3.29  ... 3.404 3.629 3.9  ]
 [4.117 4.883 4.9   ... 3.834 4.344 3.715]
 ...
 [4.72  3.773 3.836 ... 3.98  3.686 3.531]
 [4.863 3.844 3.924 ... 4.008 3.832 3.562]
 [6.87  3.635 3.322 ... 3.484 3.666 3.906]]
(801, 11) (801, 11)
By feature:  MSE:  0.5459¬±0.0 MAE:  0.4787¬±0.0 R2:  0.2235¬±0.0 PCC:  0.8123¬±0.0 Cosine Similarity:  0.9903¬±0.0
By sample:  MSE:  0.5459¬±0.0 MAE:  0.4787¬±0.0 R2:  0.3133¬±0.0 PCC:  0.5542¬±0.0 Cosine Similarity:  0.9861¬±0.0
scGPT - INFO - By feature: MSE: 0.5458999872207642¬±0.0 MAE: 0.47870001196861267¬±0.0 R2: 0.2235¬±0.0 PCC: 0.8123¬±0.0 Cosine Similarity: 0.9903¬±0.0
scGPT - INFO - By sample: MSE: 0.5458999872207642¬±0.0 MAE: 0.47870001196861267¬±0.0 R2: 0.3133¬±0.0 PCC: 0.5542¬±0.0 Cosine Similarity: 0.9861¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  41
Training model
scGPT - INFO - | epoch  41 | 100/3602 batches | lr 0.0000 | ms/batch 93.77 | loss 11.16 | mse 11.16 | mre  0.00 |
scGPT - INFO - | epoch  41 | 200/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 12.02 | mse 12.02 | mre  0.00 |
scGPT - INFO - | epoch  41 | 300/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 10.13 | mse 10.13 | mre  0.00 |
scGPT - INFO - | epoch  41 | 400/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 10.72 | mse 10.72 | mre  0.00 |
scGPT - INFO - | epoch  41 | 500/3602 batches | lr 0.0000 | ms/batch 90.79 | loss  8.93 | mse  8.93 | mre  0.00 |
scGPT - INFO - | epoch  41 | 600/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 10.86 | mse 10.86 | mre  0.00 |
scGPT - INFO - | epoch  41 | 700/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 11.69 | mse 11.69 | mre  0.00 |
scGPT - INFO - | epoch  41 | 800/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 13.95 | mse 13.95 | mre  0.00 |
scGPT - INFO - | epoch  41 | 900/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 10.84 | mse 10.84 | mre  0.00 |
scGPT - INFO - | epoch  41 | 1000/3602 batches | lr 0.0000 | ms/batch 91.33 | loss 12.61 | mse 12.61 | mre  0.00 |
scGPT - INFO - | epoch  41 | 1100/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 10.74 | mse 10.74 | mre  0.00 |
scGPT - INFO - | epoch  41 | 1200/3602 batches | lr 0.0000 | ms/batch 91.14 | loss 11.58 | mse 11.58 | mre  0.00 |
scGPT - INFO - | epoch  41 | 1300/3602 batches | lr 0.0000 | ms/batch 90.91 | loss 11.37 | mse 11.37 | mre  0.00 |
scGPT - INFO - | epoch  41 | 1400/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 12.93 | mse 12.93 | mre  0.00 |
scGPT - INFO - | epoch  41 | 1500/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 14.60 | mse 14.60 | mre  0.00 |
scGPT - INFO - | epoch  41 | 1600/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 10.03 | mse 10.03 | mre  0.00 |
scGPT - INFO - | epoch  41 | 1700/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 11.03 | mse 11.03 | mre  0.00 |
scGPT - INFO - | epoch  41 | 1800/3602 batches | lr 0.0000 | ms/batch 90.91 | loss 10.05 | mse 10.05 | mre  0.00 |
scGPT - INFO - | epoch  41 | 1900/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 11.83 | mse 11.83 | mre  0.00 |
scGPT - INFO - | epoch  41 | 2000/3602 batches | lr 0.0000 | ms/batch 91.49 | loss 11.28 | mse 11.28 | mre  0.00 |
scGPT - INFO - | epoch  41 | 2100/3602 batches | lr 0.0000 | ms/batch 91.12 | loss 12.85 | mse 12.85 | mre  0.00 |
scGPT - INFO - | epoch  41 | 2200/3602 batches | lr 0.0000 | ms/batch 91.12 | loss 12.03 | mse 12.03 | mre  0.00 |
scGPT - INFO - | epoch  41 | 2300/3602 batches | lr 0.0000 | ms/batch 91.08 | loss 10.12 | mse 10.12 | mre  0.00 |
scGPT - INFO - | epoch  41 | 2400/3602 batches | lr 0.0000 | ms/batch 91.11 | loss 11.47 | mse 11.47 | mre  0.00 |
scGPT - INFO - | epoch  41 | 2500/3602 batches | lr 0.0000 | ms/batch 91.19 | loss 11.00 | mse 11.00 | mre  0.00 |
scGPT - INFO - | epoch  41 | 2600/3602 batches | lr 0.0000 | ms/batch 91.07 | loss 13.54 | mse 13.54 | mre  0.00 |
scGPT - INFO - | epoch  41 | 2700/3602 batches | lr 0.0000 | ms/batch 91.02 | loss 10.81 | mse 10.81 | mre  0.00 |
scGPT - INFO - | epoch  41 | 2800/3602 batches | lr 0.0000 | ms/batch 91.07 | loss 11.32 | mse 11.32 | mre  0.00 |
scGPT - INFO - | epoch  41 | 2900/3602 batches | lr 0.0000 | ms/batch 91.12 | loss 11.43 | mse 11.43 | mre  0.00 |
scGPT - INFO - | epoch  41 | 3000/3602 batches | lr 0.0000 | ms/batch 91.75 | loss 11.63 | mse 11.63 | mre  0.00 |
scGPT - INFO - | epoch  41 | 3100/3602 batches | lr 0.0000 | ms/batch 91.21 | loss 10.88 | mse 10.88 | mre  0.00 |
scGPT - INFO - | epoch  41 | 3200/3602 batches | lr 0.0000 | ms/batch 92.44 | loss 11.22 | mse 11.22 | mre  0.00 |
scGPT - INFO - | epoch  41 | 3300/3602 batches | lr 0.0000 | ms/batch 92.81 | loss  9.51 | mse  9.51 | mre  0.00 |
scGPT - INFO - | epoch  41 | 3400/3602 batches | lr 0.0000 | ms/batch 92.87 | loss 11.29 | mse 11.29 | mre  0.00 |
scGPT - INFO - | epoch  41 | 3500/3602 batches | lr 0.0000 | ms/batch 92.85 | loss 10.65 | mse 10.65 | mre  0.00 |
scGPT - INFO - | epoch  41 | 3600/3602 batches | lr 0.0000 | ms/batch 92.85 | loss 11.44 | mse 11.44 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  41 | time: 341.45s | valid loss/mse 12.0334 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.009459221482128, min_delta 0.0001, val_loss 12.033404337481763
Loss error: -0.023945115999634226
scGPT - INFO - INFO: Early stopping counter 1 of 10
epoch:  41
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.97  3.482 3.227 ... 3.322 3.559 3.855]
 [7.02  3.56  3.283 ... 3.4   3.625 3.898]
 [4.18  4.926 4.91  ... 3.83  4.34  3.713]
 ...
 [4.72  3.766 3.826 ... 3.979 3.682 3.525]
 [4.867 3.836 3.92  ... 4.008 3.832 3.56 ]
 [6.89  3.625 3.314 ... 3.479 3.662 3.908]]
(801, 11) (801, 11)
By feature:  MSE:  0.547¬±0.0 MAE:  0.4792¬±0.0 R2:  0.2228¬±0.0 PCC:  0.813¬±0.0 Cosine Similarity:  0.9903¬±0.0
By sample:  MSE:  0.547¬±0.0 MAE:  0.4792¬±0.0 R2:  0.3118¬±0.0 PCC:  0.5537¬±0.0 Cosine Similarity:  0.9861¬±0.0
scGPT - INFO - By feature: MSE: 0.546999990940094¬±0.0 MAE: 0.47920000553131104¬±0.0 R2: 0.2228¬±0.0 PCC: 0.813¬±0.0 Cosine Similarity: 0.9903¬±0.0
scGPT - INFO - By sample: MSE: 0.546999990940094¬±0.0 MAE: 0.47920000553131104¬±0.0 R2: 0.3118¬±0.0 PCC: 0.5537¬±0.0 Cosine Similarity: 0.9861¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  42
Training model
scGPT - INFO - | epoch  42 | 100/3602 batches | lr 0.0000 | ms/batch 92.06 | loss 11.22 | mse 11.22 | mre  0.00 |
scGPT - INFO - | epoch  42 | 200/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 11.74 | mse 11.74 | mre  0.00 |
scGPT - INFO - | epoch  42 | 300/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 10.05 | mse 10.05 | mre  0.00 |
scGPT - INFO - | epoch  42 | 400/3602 batches | lr 0.0000 | ms/batch 91.33 | loss 10.60 | mse 10.60 | mre  0.00 |
scGPT - INFO - | epoch  42 | 500/3602 batches | lr 0.0000 | ms/batch 90.81 | loss  9.33 | mse  9.33 | mre  0.00 |
scGPT - INFO - | epoch  42 | 600/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 10.37 | mse 10.37 | mre  0.00 |
scGPT - INFO - | epoch  42 | 700/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 11.52 | mse 11.52 | mre  0.00 |
scGPT - INFO - | epoch  42 | 800/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 13.99 | mse 13.99 | mre  0.00 |
scGPT - INFO - | epoch  42 | 900/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 10.90 | mse 10.90 | mre  0.00 |
scGPT - INFO - | epoch  42 | 1000/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 12.88 | mse 12.88 | mre  0.00 |
scGPT - INFO - | epoch  42 | 1100/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 11.16 | mse 11.16 | mre  0.00 |
scGPT - INFO - | epoch  42 | 1200/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 11.13 | mse 11.13 | mre  0.00 |
scGPT - INFO - | epoch  42 | 1300/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 11.27 | mse 11.27 | mre  0.00 |
scGPT - INFO - | epoch  42 | 1400/3602 batches | lr 0.0000 | ms/batch 91.35 | loss 13.19 | mse 13.19 | mre  0.00 |
scGPT - INFO - | epoch  42 | 1500/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 14.37 | mse 14.37 | mre  0.00 |
scGPT - INFO - | epoch  42 | 1600/3602 batches | lr 0.0000 | ms/batch 90.78 | loss  9.97 | mse  9.97 | mre  0.00 |
scGPT - INFO - | epoch  42 | 1700/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 10.79 | mse 10.79 | mre  0.00 |
scGPT - INFO - | epoch  42 | 1800/3602 batches | lr 0.0000 | ms/batch 90.83 | loss  9.92 | mse  9.92 | mre  0.00 |
scGPT - INFO - | epoch  42 | 1900/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 11.79 | mse 11.79 | mre  0.00 |
scGPT - INFO - | epoch  42 | 2000/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 11.39 | mse 11.39 | mre  0.00 |
scGPT - INFO - | epoch  42 | 2100/3602 batches | lr 0.0000 | ms/batch 91.17 | loss 12.70 | mse 12.70 | mre  0.00 |
scGPT - INFO - | epoch  42 | 2200/3602 batches | lr 0.0000 | ms/batch 92.51 | loss 11.56 | mse 11.56 | mre  0.00 |
scGPT - INFO - | epoch  42 | 2300/3602 batches | lr 0.0000 | ms/batch 92.76 | loss 10.13 | mse 10.13 | mre  0.00 |
scGPT - INFO - | epoch  42 | 2400/3602 batches | lr 0.0000 | ms/batch 93.42 | loss 11.68 | mse 11.68 | mre  0.00 |
scGPT - INFO - | epoch  42 | 2500/3602 batches | lr 0.0000 | ms/batch 92.86 | loss 10.74 | mse 10.74 | mre  0.00 |
scGPT - INFO - | epoch  42 | 2600/3602 batches | lr 0.0000 | ms/batch 93.03 | loss 13.47 | mse 13.47 | mre  0.00 |
scGPT - INFO - | epoch  42 | 2700/3602 batches | lr 0.0000 | ms/batch 93.09 | loss 11.04 | mse 11.04 | mre  0.00 |
scGPT - INFO - | epoch  42 | 2800/3602 batches | lr 0.0000 | ms/batch 92.97 | loss 11.81 | mse 11.81 | mre  0.00 |
scGPT - INFO - | epoch  42 | 2900/3602 batches | lr 0.0000 | ms/batch 92.99 | loss 11.64 | mse 11.64 | mre  0.00 |
scGPT - INFO - | epoch  42 | 3000/3602 batches | lr 0.0000 | ms/batch 92.95 | loss 12.01 | mse 12.01 | mre  0.00 |
scGPT - INFO - | epoch  42 | 3100/3602 batches | lr 0.0000 | ms/batch 92.94 | loss 10.81 | mse 10.81 | mre  0.00 |
scGPT - INFO - | epoch  42 | 3200/3602 batches | lr 0.0000 | ms/batch 92.89 | loss 10.96 | mse 10.96 | mre  0.00 |
scGPT - INFO - | epoch  42 | 3300/3602 batches | lr 0.0000 | ms/batch 92.84 | loss  9.40 | mse  9.40 | mre  0.00 |
scGPT - INFO - | epoch  42 | 3400/3602 batches | lr 0.0000 | ms/batch 93.31 | loss 11.14 | mse 11.14 | mre  0.00 |
scGPT - INFO - | epoch  42 | 3500/3602 batches | lr 0.0000 | ms/batch 92.76 | loss 10.53 | mse 10.53 | mre  0.00 |
scGPT - INFO - | epoch  42 | 3600/3602 batches | lr 0.0000 | ms/batch 92.74 | loss 12.03 | mse 12.03 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  42 | time: 343.47s | valid loss/mse 12.0328 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.009459221482128, min_delta 0.0001, val_loss 12.032847298516167
Loss error: -0.023388077034038446
scGPT - INFO - INFO: Early stopping counter 2 of 10
epoch:  42
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.973 3.498 3.229 ... 3.326 3.562 3.857]
 [7.03  3.574 3.283 ... 3.404 3.629 3.9  ]
 [4.25  5.023 4.957 ... 3.854 4.367 3.729]
 ...
 [4.73  3.785 3.836 ... 3.99  3.693 3.533]
 [4.88  3.85  3.922 ... 4.016 3.84  3.562]
 [6.895 3.645 3.32  ... 3.488 3.672 3.912]]
(801, 11) (801, 11)
By feature:  MSE:  0.547¬±0.0 MAE:  0.4803¬±0.0 R2:  0.2177¬±0.0 PCC:  0.8125¬±0.0 Cosine Similarity:  0.9903¬±0.0
By sample:  MSE:  0.547¬±0.0 MAE:  0.4803¬±0.0 R2:  0.3119¬±0.0 PCC:  0.5532¬±0.0 Cosine Similarity:  0.986¬±0.0
scGPT - INFO - By feature: MSE: 0.546999990940094¬±0.0 MAE: 0.48030000925064087¬±0.0 R2: 0.2177¬±0.0 PCC: 0.8125¬±0.0 Cosine Similarity: 0.9903¬±0.0
scGPT - INFO - By sample: MSE: 0.546999990940094¬±0.0 MAE: 0.48030000925064087¬±0.0 R2: 0.3119¬±0.0 PCC: 0.5532¬±0.0 Cosine Similarity: 0.986¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  43
Training model
scGPT - INFO - | epoch  43 | 100/3602 batches | lr 0.0000 | ms/batch 94.10 | loss 10.74 | mse 10.74 | mre  0.00 |
scGPT - INFO - | epoch  43 | 200/3602 batches | lr 0.0000 | ms/batch 93.00 | loss 11.77 | mse 11.77 | mre  0.00 |
scGPT - INFO - | epoch  43 | 300/3602 batches | lr 0.0000 | ms/batch 92.98 | loss  9.94 | mse  9.94 | mre  0.00 |
scGPT - INFO - | epoch  43 | 400/3602 batches | lr 0.0000 | ms/batch 92.86 | loss 10.42 | mse 10.42 | mre  0.00 |
scGPT - INFO - | epoch  43 | 500/3602 batches | lr 0.0000 | ms/batch 92.78 | loss  9.07 | mse  9.07 | mre  0.00 |
scGPT - INFO - | epoch  43 | 600/3602 batches | lr 0.0000 | ms/batch 92.46 | loss 10.54 | mse 10.54 | mre  0.00 |
scGPT - INFO - | epoch  43 | 700/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 11.27 | mse 11.27 | mre  0.00 |
scGPT - INFO - | epoch  43 | 800/3602 batches | lr 0.0000 | ms/batch 91.38 | loss 13.80 | mse 13.80 | mre  0.00 |
scGPT - INFO - | epoch  43 | 900/3602 batches | lr 0.0000 | ms/batch 92.58 | loss 10.98 | mse 10.98 | mre  0.00 |
scGPT - INFO - | epoch  43 | 1000/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 12.35 | mse 12.35 | mre  0.00 |
scGPT - INFO - | epoch  43 | 1100/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 10.97 | mse 10.97 | mre  0.00 |
scGPT - INFO - | epoch  43 | 1200/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 11.13 | mse 11.13 | mre  0.00 |
scGPT - INFO - | epoch  43 | 1300/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 11.11 | mse 11.11 | mre  0.00 |
scGPT - INFO - | epoch  43 | 1400/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 12.60 | mse 12.60 | mre  0.00 |
scGPT - INFO - | epoch  43 | 1500/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 14.34 | mse 14.34 | mre  0.00 |
scGPT - INFO - | epoch  43 | 1600/3602 batches | lr 0.0000 | ms/batch 90.76 | loss  9.43 | mse  9.43 | mre  0.00 |
scGPT - INFO - | epoch  43 | 1700/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 10.26 | mse 10.26 | mre  0.00 |
scGPT - INFO - | epoch  43 | 1800/3602 batches | lr 0.0000 | ms/batch 91.28 | loss  9.62 | mse  9.62 | mre  0.00 |
scGPT - INFO - | epoch  43 | 1900/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 11.87 | mse 11.87 | mre  0.00 |
scGPT - INFO - | epoch  43 | 2000/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 11.44 | mse 11.44 | mre  0.00 |
scGPT - INFO - | epoch  43 | 2100/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 12.68 | mse 12.68 | mre  0.00 |
scGPT - INFO - | epoch  43 | 2200/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 12.19 | mse 12.19 | mre  0.00 |
scGPT - INFO - | epoch  43 | 2300/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 10.43 | mse 10.43 | mre  0.00 |
scGPT - INFO - | epoch  43 | 2400/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 11.61 | mse 11.61 | mre  0.00 |
scGPT - INFO - | epoch  43 | 2500/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 10.88 | mse 10.88 | mre  0.00 |
scGPT - INFO - | epoch  43 | 2600/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 13.16 | mse 13.16 | mre  0.00 |
scGPT - INFO - | epoch  43 | 2700/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 10.66 | mse 10.66 | mre  0.00 |
scGPT - INFO - | epoch  43 | 2800/3602 batches | lr 0.0000 | ms/batch 91.34 | loss 11.02 | mse 11.02 | mre  0.00 |
scGPT - INFO - | epoch  43 | 2900/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 11.69 | mse 11.69 | mre  0.00 |
scGPT - INFO - | epoch  43 | 3000/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 11.78 | mse 11.78 | mre  0.00 |
scGPT - INFO - | epoch  43 | 3100/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 11.21 | mse 11.21 | mre  0.00 |
scGPT - INFO - | epoch  43 | 3200/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 11.14 | mse 11.14 | mre  0.00 |
scGPT - INFO - | epoch  43 | 3300/3602 batches | lr 0.0000 | ms/batch 90.70 | loss  9.38 | mse  9.38 | mre  0.00 |
scGPT - INFO - | epoch  43 | 3400/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 11.01 | mse 11.01 | mre  0.00 |
scGPT - INFO - | epoch  43 | 3500/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 10.62 | mse 10.62 | mre  0.00 |
scGPT - INFO - | epoch  43 | 3600/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 11.57 | mse 11.57 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  43 | time: 340.93s | valid loss/mse 11.9770 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 11.9770
best_loss: 12.009459221482128, min_delta 0.0001, val_loss 11.977011192157473
Loss error: 0.03244802932465518
epoch:  43
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.953 3.5   3.232 ... 3.332 3.559 3.855]
 [7.004 3.576 3.285 ... 3.406 3.625 3.898]
 [4.363 4.867 4.832 ... 3.938 4.355 3.762]
 ...
 [4.707 3.783 3.828 ... 3.986 3.682 3.53 ]
 [4.855 3.85  3.916 ... 4.01  3.83  3.56 ]
 [6.883 3.639 3.318 ... 3.484 3.664 3.908]]
(801, 11) (801, 11)
By feature:  MSE:  0.5444¬±0.0 MAE:  0.4777¬±0.0 R2:  0.2185¬±0.0 PCC:  0.8151¬±0.0 Cosine Similarity:  0.9903¬±0.0
By sample:  MSE:  0.5444¬±0.0 MAE:  0.4777¬±0.0 R2:  0.3154¬±0.0 PCC:  0.5554¬±0.0 Cosine Similarity:  0.9861¬±0.0
scGPT - INFO - By feature: MSE: 0.5443999767303467¬±0.0 MAE: 0.47769999504089355¬±0.0 R2: 0.2185¬±0.0 PCC: 0.8151¬±0.0 Cosine Similarity: 0.9903¬±0.0
scGPT - INFO - By sample: MSE: 0.5443999767303467¬±0.0 MAE: 0.47769999504089355¬±0.0 R2: 0.3154¬±0.0 PCC: 0.5554¬±0.0 Cosine Similarity: 0.9861¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  44
Training model
scGPT - INFO - | epoch  44 | 100/3602 batches | lr 0.0000 | ms/batch 92.14 | loss 10.87 | mse 10.87 | mre  0.00 |
scGPT - INFO - | epoch  44 | 200/3602 batches | lr 0.0000 | ms/batch 91.31 | loss 12.10 | mse 12.10 | mre  0.00 |
scGPT - INFO - | epoch  44 | 300/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 10.02 | mse 10.02 | mre  0.00 |
scGPT - INFO - | epoch  44 | 400/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 10.28 | mse 10.28 | mre  0.00 |
scGPT - INFO - | epoch  44 | 500/3602 batches | lr 0.0000 | ms/batch 90.83 | loss  8.97 | mse  8.97 | mre  0.00 |
scGPT - INFO - | epoch  44 | 600/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 10.82 | mse 10.82 | mre  0.00 |
scGPT - INFO - | epoch  44 | 700/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 11.07 | mse 11.07 | mre  0.00 |
scGPT - INFO - | epoch  44 | 800/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 13.75 | mse 13.75 | mre  0.00 |
scGPT - INFO - | epoch  44 | 900/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 10.74 | mse 10.74 | mre  0.00 |
scGPT - INFO - | epoch  44 | 1000/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 12.42 | mse 12.42 | mre  0.00 |
scGPT - INFO - | epoch  44 | 1100/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 10.43 | mse 10.43 | mre  0.00 |
scGPT - INFO - | epoch  44 | 1200/3602 batches | lr 0.0000 | ms/batch 91.38 | loss 11.00 | mse 11.00 | mre  0.00 |
scGPT - INFO - | epoch  44 | 1300/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 10.98 | mse 10.98 | mre  0.00 |
scGPT - INFO - | epoch  44 | 1400/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 12.67 | mse 12.67 | mre  0.00 |
scGPT - INFO - | epoch  44 | 1500/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 14.43 | mse 14.43 | mre  0.00 |
scGPT - INFO - | epoch  44 | 1600/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.26 | mse 10.26 | mre  0.00 |
scGPT - INFO - | epoch  44 | 1700/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 10.73 | mse 10.73 | mre  0.00 |
scGPT - INFO - | epoch  44 | 1800/3602 batches | lr 0.0000 | ms/batch 90.75 | loss  9.93 | mse  9.93 | mre  0.00 |
scGPT - INFO - | epoch  44 | 1900/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 11.68 | mse 11.68 | mre  0.00 |
scGPT - INFO - | epoch  44 | 2000/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 11.38 | mse 11.38 | mre  0.00 |
scGPT - INFO - | epoch  44 | 2100/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 13.04 | mse 13.04 | mre  0.00 |
scGPT - INFO - | epoch  44 | 2200/3602 batches | lr 0.0000 | ms/batch 91.32 | loss 11.74 | mse 11.74 | mre  0.00 |
scGPT - INFO - | epoch  44 | 2300/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 10.13 | mse 10.13 | mre  0.00 |
scGPT - INFO - | epoch  44 | 2400/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 11.44 | mse 11.44 | mre  0.00 |
scGPT - INFO - | epoch  44 | 2500/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 11.05 | mse 11.05 | mre  0.00 |
scGPT - INFO - | epoch  44 | 2600/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 13.77 | mse 13.77 | mre  0.00 |
scGPT - INFO - | epoch  44 | 2700/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 10.66 | mse 10.66 | mre  0.00 |
scGPT - INFO - | epoch  44 | 2800/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 11.31 | mse 11.31 | mre  0.00 |
scGPT - INFO - | epoch  44 | 2900/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 11.67 | mse 11.67 | mre  0.00 |
scGPT - INFO - | epoch  44 | 3000/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 11.51 | mse 11.51 | mre  0.00 |
scGPT - INFO - | epoch  44 | 3100/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 10.99 | mse 10.99 | mre  0.00 |
scGPT - INFO - | epoch  44 | 3200/3602 batches | lr 0.0000 | ms/batch 91.27 | loss 11.24 | mse 11.24 | mre  0.00 |
scGPT - INFO - | epoch  44 | 3300/3602 batches | lr 0.0000 | ms/batch 90.72 | loss  9.33 | mse  9.33 | mre  0.00 |
scGPT - INFO - | epoch  44 | 3400/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 11.21 | mse 11.21 | mre  0.00 |
scGPT - INFO - | epoch  44 | 3500/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 10.31 | mse 10.31 | mre  0.00 |
scGPT - INFO - | epoch  44 | 3600/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 11.72 | mse 11.72 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  44 | time: 339.60s | valid loss/mse 12.0202 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 11.977011192157473, min_delta 0.0001, val_loss 12.020214197564215
Loss error: -0.04320300540674182
scGPT - INFO - INFO: Early stopping counter 1 of 10
epoch:  44
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.96  3.506 3.23  ... 3.324 3.566 3.855]
 [7.016 3.578 3.285 ... 3.4   3.63  3.898]
 [4.312 4.87  4.84  ... 3.906 4.348 3.746]
 ...
 [4.69  3.77  3.818 ... 3.979 3.674 3.52 ]
 [4.848 3.842 3.914 ... 4.008 3.828 3.555]
 [6.883 3.654 3.322 ... 3.488 3.676 3.912]]
(801, 11) (801, 11)
By feature:  MSE:  0.5464¬±0.0 MAE:  0.4792¬±0.0 R2:  0.2156¬±0.0 PCC:  0.8138¬±0.0 Cosine Similarity:  0.9903¬±0.0
By sample:  MSE:  0.5464¬±0.0 MAE:  0.4792¬±0.0 R2:  0.3128¬±0.0 PCC:  0.5543¬±0.0 Cosine Similarity:  0.986¬±0.0
scGPT - INFO - By feature: MSE: 0.5464000105857849¬±0.0 MAE: 0.47920000553131104¬±0.0 R2: 0.2156¬±0.0 PCC: 0.8138¬±0.0 Cosine Similarity: 0.9903¬±0.0
scGPT - INFO - By sample: MSE: 0.5464000105857849¬±0.0 MAE: 0.47920000553131104¬±0.0 R2: 0.3128¬±0.0 PCC: 0.5543¬±0.0 Cosine Similarity: 0.986¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  45
Training model
scGPT - INFO - | epoch  45 | 100/3602 batches | lr 0.0000 | ms/batch 92.06 | loss 10.52 | mse 10.52 | mre  0.00 |
scGPT - INFO - | epoch  45 | 200/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 11.96 | mse 11.96 | mre  0.00 |
scGPT - INFO - | epoch  45 | 300/3602 batches | lr 0.0000 | ms/batch 90.81 | loss  9.87 | mse  9.87 | mre  0.00 |
scGPT - INFO - | epoch  45 | 400/3602 batches | lr 0.0000 | ms/batch 90.98 | loss 10.26 | mse 10.26 | mre  0.00 |
scGPT - INFO - | epoch  45 | 500/3602 batches | lr 0.0000 | ms/batch 91.01 | loss  8.62 | mse  8.62 | mre  0.00 |
scGPT - INFO - | epoch  45 | 600/3602 batches | lr 0.0000 | ms/batch 91.53 | loss 10.61 | mse 10.61 | mre  0.00 |
scGPT - INFO - | epoch  45 | 700/3602 batches | lr 0.0000 | ms/batch 90.98 | loss 11.37 | mse 11.37 | mre  0.00 |
scGPT - INFO - | epoch  45 | 800/3602 batches | lr 0.0000 | ms/batch 90.95 | loss 13.53 | mse 13.53 | mre  0.00 |
scGPT - INFO - | epoch  45 | 900/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 10.75 | mse 10.75 | mre  0.00 |
scGPT - INFO - | epoch  45 | 1000/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 12.49 | mse 12.49 | mre  0.00 |
scGPT - INFO - | epoch  45 | 1100/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 10.91 | mse 10.91 | mre  0.00 |
scGPT - INFO - | epoch  45 | 1200/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 11.14 | mse 11.14 | mre  0.00 |
scGPT - INFO - | epoch  45 | 1300/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.80 | mse 10.80 | mre  0.00 |
scGPT - INFO - | epoch  45 | 1400/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 12.89 | mse 12.89 | mre  0.00 |
scGPT - INFO - | epoch  45 | 1500/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 14.26 | mse 14.26 | mre  0.00 |
scGPT - INFO - | epoch  45 | 1600/3602 batches | lr 0.0000 | ms/batch 91.38 | loss  9.72 | mse  9.72 | mre  0.00 |
scGPT - INFO - | epoch  45 | 1700/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.80 | mse 10.80 | mre  0.00 |
scGPT - INFO - | epoch  45 | 1800/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.03 | mse 10.03 | mre  0.00 |
scGPT - INFO - | epoch  45 | 1900/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 11.81 | mse 11.81 | mre  0.00 |
scGPT - INFO - | epoch  45 | 2000/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 11.55 | mse 11.55 | mre  0.00 |
scGPT - INFO - | epoch  45 | 2100/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 13.02 | mse 13.02 | mre  0.00 |
scGPT - INFO - | epoch  45 | 2200/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 11.79 | mse 11.79 | mre  0.00 |
scGPT - INFO - | epoch  45 | 2300/3602 batches | lr 0.0000 | ms/batch 91.13 | loss 10.15 | mse 10.15 | mre  0.00 |
scGPT - INFO - | epoch  45 | 2400/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 11.63 | mse 11.63 | mre  0.00 |
scGPT - INFO - | epoch  45 | 2500/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 10.61 | mse 10.61 | mre  0.00 |
scGPT - INFO - | epoch  45 | 2600/3602 batches | lr 0.0000 | ms/batch 91.41 | loss 13.89 | mse 13.89 | mre  0.00 |
scGPT - INFO - | epoch  45 | 2700/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 10.78 | mse 10.78 | mre  0.00 |
scGPT - INFO - | epoch  45 | 2800/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 11.06 | mse 11.06 | mre  0.00 |
scGPT - INFO - | epoch  45 | 2900/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 11.09 | mse 11.09 | mre  0.00 |
scGPT - INFO - | epoch  45 | 3000/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 11.73 | mse 11.73 | mre  0.00 |
scGPT - INFO - | epoch  45 | 3100/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 11.10 | mse 11.10 | mre  0.00 |
scGPT - INFO - | epoch  45 | 3200/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 11.11 | mse 11.11 | mre  0.00 |
scGPT - INFO - | epoch  45 | 3300/3602 batches | lr 0.0000 | ms/batch 90.78 | loss  9.33 | mse  9.33 | mre  0.00 |
scGPT - INFO - | epoch  45 | 3400/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 11.12 | mse 11.12 | mre  0.00 |
scGPT - INFO - | epoch  45 | 3500/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 10.26 | mse 10.26 | mre  0.00 |
scGPT - INFO - | epoch  45 | 3600/3602 batches | lr 0.0000 | ms/batch 91.29 | loss 11.89 | mse 11.89 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  45 | time: 339.84s | valid loss/mse 11.9955 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 11.977011192157473, min_delta 0.0001, val_loss 11.995468776770746
Loss error: -0.01845758461327307
scGPT - INFO - INFO: Early stopping counter 2 of 10
epoch:  45
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.957 3.498 3.23  ... 3.322 3.564 3.852]
 [6.996 3.568 3.283 ... 3.398 3.627 3.89 ]
 [4.336 4.88  4.83  ... 3.912 4.344 3.742]
 ...
 [4.707 3.783 3.828 ... 3.986 3.68  3.525]
 [4.863 3.855 3.922 ... 4.016 3.836 3.562]
 [6.883 3.637 3.316 ... 3.477 3.668 3.904]]
(801, 11) (801, 11)
By feature:  MSE:  0.5453¬±0.0 MAE:  0.4784¬±0.0 R2:  0.2173¬±0.0 PCC:  0.8148¬±0.0 Cosine Similarity:  0.9903¬±0.0
By sample:  MSE:  0.5453¬±0.0 MAE:  0.4784¬±0.0 R2:  0.3139¬±0.0 PCC:  0.5554¬±0.0 Cosine Similarity:  0.9861¬±0.0
scGPT - INFO - By feature: MSE: 0.5453000068664551¬±0.0 MAE: 0.47839999198913574¬±0.0 R2: 0.2173¬±0.0 PCC: 0.8148¬±0.0 Cosine Similarity: 0.9903¬±0.0
scGPT - INFO - By sample: MSE: 0.5453000068664551¬±0.0 MAE: 0.47839999198913574¬±0.0 R2: 0.3139¬±0.0 PCC: 0.5554¬±0.0 Cosine Similarity: 0.9861¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  46
Training model
scGPT - INFO - | epoch  46 | 100/3602 batches | lr 0.0000 | ms/batch 92.03 | loss 11.22 | mse 11.22 | mre  0.00 |
scGPT - INFO - | epoch  46 | 200/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 12.39 | mse 12.39 | mre  0.00 |
scGPT - INFO - | epoch  46 | 300/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 10.28 | mse 10.28 | mre  0.00 |
scGPT - INFO - | epoch  46 | 400/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 10.76 | mse 10.76 | mre  0.00 |
scGPT - INFO - | epoch  46 | 500/3602 batches | lr 0.0000 | ms/batch 90.81 | loss  9.03 | mse  9.03 | mre  0.00 |
scGPT - INFO - | epoch  46 | 600/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 10.36 | mse 10.36 | mre  0.00 |
scGPT - INFO - | epoch  46 | 700/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 11.29 | mse 11.29 | mre  0.00 |
scGPT - INFO - | epoch  46 | 800/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 13.67 | mse 13.67 | mre  0.00 |
scGPT - INFO - | epoch  46 | 900/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 10.79 | mse 10.79 | mre  0.00 |
scGPT - INFO - | epoch  46 | 1000/3602 batches | lr 0.0000 | ms/batch 91.35 | loss 12.52 | mse 12.52 | mre  0.00 |
scGPT - INFO - | epoch  46 | 1100/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 11.06 | mse 11.06 | mre  0.00 |
scGPT - INFO - | epoch  46 | 1200/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 11.31 | mse 11.31 | mre  0.00 |
scGPT - INFO - | epoch  46 | 1300/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 11.08 | mse 11.08 | mre  0.00 |
scGPT - INFO - | epoch  46 | 1400/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 12.55 | mse 12.55 | mre  0.00 |
scGPT - INFO - | epoch  46 | 1500/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 14.25 | mse 14.25 | mre  0.00 |
scGPT - INFO - | epoch  46 | 1600/3602 batches | lr 0.0000 | ms/batch 90.74 | loss  9.98 | mse  9.98 | mre  0.00 |
scGPT - INFO - | epoch  46 | 1700/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 10.51 | mse 10.51 | mre  0.00 |
scGPT - INFO - | epoch  46 | 1800/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 10.01 | mse 10.01 | mre  0.00 |
scGPT - INFO - | epoch  46 | 1900/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 11.79 | mse 11.79 | mre  0.00 |
scGPT - INFO - | epoch  46 | 2000/3602 batches | lr 0.0000 | ms/batch 91.34 | loss 11.07 | mse 11.07 | mre  0.00 |
scGPT - INFO - | epoch  46 | 2100/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 12.91 | mse 12.91 | mre  0.00 |
scGPT - INFO - | epoch  46 | 2200/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 11.61 | mse 11.61 | mre  0.00 |
scGPT - INFO - | epoch  46 | 2300/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 10.42 | mse 10.42 | mre  0.00 |
scGPT - INFO - | epoch  46 | 2400/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 11.16 | mse 11.16 | mre  0.00 |
scGPT - INFO - | epoch  46 | 2500/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 10.71 | mse 10.71 | mre  0.00 |
scGPT - INFO - | epoch  46 | 2600/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 13.18 | mse 13.18 | mre  0.00 |
scGPT - INFO - | epoch  46 | 2700/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 10.59 | mse 10.59 | mre  0.00 |
scGPT - INFO - | epoch  46 | 2800/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 11.30 | mse 11.30 | mre  0.00 |
scGPT - INFO - | epoch  46 | 2900/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 11.42 | mse 11.42 | mre  0.00 |
scGPT - INFO - | epoch  46 | 3000/3602 batches | lr 0.0000 | ms/batch 91.42 | loss 11.61 | mse 11.61 | mre  0.00 |
scGPT - INFO - | epoch  46 | 3100/3602 batches | lr 0.0000 | ms/batch 90.95 | loss 10.87 | mse 10.87 | mre  0.00 |
scGPT - INFO - | epoch  46 | 3200/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 11.14 | mse 11.14 | mre  0.00 |
scGPT - INFO - | epoch  46 | 3300/3602 batches | lr 0.0000 | ms/batch 90.83 | loss  9.31 | mse  9.31 | mre  0.00 |
scGPT - INFO - | epoch  46 | 3400/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 11.17 | mse 11.17 | mre  0.00 |
scGPT - INFO - | epoch  46 | 3500/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 10.67 | mse 10.67 | mre  0.00 |
scGPT - INFO - | epoch  46 | 3600/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 11.68 | mse 11.68 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  46 | time: 339.75s | valid loss/mse 12.0109 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 11.977011192157473, min_delta 0.0001, val_loss 12.010906469955874
Loss error: -0.033895277798400514
scGPT - INFO - INFO: Early stopping counter 3 of 10
epoch:  46
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.973 3.496 3.22  ... 3.316 3.559 3.852]
 [7.023 3.57  3.275 ... 3.395 3.623 3.896]
 [4.387 4.85  4.812 ... 3.94  4.35  3.758]
 ...
 [4.707 3.78  3.822 ... 3.984 3.68  3.523]
 [4.863 3.85  3.916 ... 4.016 3.834 3.56 ]
 [6.895 3.64  3.312 ... 3.477 3.668 3.908]]
(801, 11) (801, 11)
By feature:  MSE:  0.546¬±0.0 MAE:  0.4791¬±0.0 R2:  0.2144¬±0.0 PCC:  0.8143¬±0.0 Cosine Similarity:  0.9903¬±0.0
By sample:  MSE:  0.546¬±0.0 MAE:  0.4791¬±0.0 R2:  0.313¬±0.0 PCC:  0.5551¬±0.0 Cosine Similarity:  0.9861¬±0.0
scGPT - INFO - By feature: MSE: 0.5460000038146973¬±0.0 MAE: 0.47909998893737793¬±0.0 R2: 0.2144¬±0.0 PCC: 0.8143¬±0.0 Cosine Similarity: 0.9903¬±0.0
scGPT - INFO - By sample: MSE: 0.5460000038146973¬±0.0 MAE: 0.47909998893737793¬±0.0 R2: 0.313¬±0.0 PCC: 0.5551¬±0.0 Cosine Similarity: 0.9861¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  47
Training model
scGPT - INFO - | epoch  47 | 100/3602 batches | lr 0.0000 | ms/batch 92.01 | loss 11.18 | mse 11.18 | mre  0.00 |
scGPT - INFO - | epoch  47 | 200/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 11.74 | mse 11.74 | mre  0.00 |
scGPT - INFO - | epoch  47 | 300/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 10.44 | mse 10.44 | mre  0.00 |
scGPT - INFO - | epoch  47 | 400/3602 batches | lr 0.0000 | ms/batch 91.47 | loss 10.41 | mse 10.41 | mre  0.00 |
scGPT - INFO - | epoch  47 | 500/3602 batches | lr 0.0000 | ms/batch 90.83 | loss  9.01 | mse  9.01 | mre  0.00 |
scGPT - INFO - | epoch  47 | 600/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 10.64 | mse 10.64 | mre  0.00 |
scGPT - INFO - | epoch  47 | 700/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 11.19 | mse 11.19 | mre  0.00 |
scGPT - INFO - | epoch  47 | 800/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 13.73 | mse 13.73 | mre  0.00 |
scGPT - INFO - | epoch  47 | 900/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.85 | mse 10.85 | mre  0.00 |
scGPT - INFO - | epoch  47 | 1000/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 12.39 | mse 12.39 | mre  0.00 |
scGPT - INFO - | epoch  47 | 1100/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 10.72 | mse 10.72 | mre  0.00 |
scGPT - INFO - | epoch  47 | 1200/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 11.35 | mse 11.35 | mre  0.00 |
scGPT - INFO - | epoch  47 | 1300/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 10.85 | mse 10.85 | mre  0.00 |
scGPT - INFO - | epoch  47 | 1400/3602 batches | lr 0.0000 | ms/batch 91.23 | loss 12.72 | mse 12.72 | mre  0.00 |
scGPT - INFO - | epoch  47 | 1500/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 14.17 | mse 14.17 | mre  0.00 |
scGPT - INFO - | epoch  47 | 1600/3602 batches | lr 0.0000 | ms/batch 90.77 | loss  9.45 | mse  9.45 | mre  0.00 |
scGPT - INFO - | epoch  47 | 1700/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 10.48 | mse 10.48 | mre  0.00 |
scGPT - INFO - | epoch  47 | 1800/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 10.22 | mse 10.22 | mre  0.00 |
scGPT - INFO - | epoch  47 | 1900/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 12.15 | mse 12.15 | mre  0.00 |
scGPT - INFO - | epoch  47 | 2000/3602 batches | lr 0.0000 | ms/batch 91.36 | loss 11.30 | mse 11.30 | mre  0.00 |
scGPT - INFO - | epoch  47 | 2100/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 12.92 | mse 12.92 | mre  0.00 |
scGPT - INFO - | epoch  47 | 2200/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 11.50 | mse 11.50 | mre  0.00 |
scGPT - INFO - | epoch  47 | 2300/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 10.22 | mse 10.22 | mre  0.00 |
scGPT - INFO - | epoch  47 | 2400/3602 batches | lr 0.0000 | ms/batch 91.29 | loss 11.61 | mse 11.61 | mre  0.00 |
scGPT - INFO - | epoch  47 | 2500/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 10.70 | mse 10.70 | mre  0.00 |
scGPT - INFO - | epoch  47 | 2600/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 13.61 | mse 13.61 | mre  0.00 |
scGPT - INFO - | epoch  47 | 2700/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 10.85 | mse 10.85 | mre  0.00 |
scGPT - INFO - | epoch  47 | 2800/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 11.25 | mse 11.25 | mre  0.00 |
scGPT - INFO - | epoch  47 | 2900/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 11.56 | mse 11.56 | mre  0.00 |
scGPT - INFO - | epoch  47 | 3000/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 11.96 | mse 11.96 | mre  0.00 |
scGPT - INFO - | epoch  47 | 3100/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 10.90 | mse 10.90 | mre  0.00 |
scGPT - INFO - | epoch  47 | 3200/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 10.99 | mse 10.99 | mre  0.00 |
scGPT - INFO - | epoch  47 | 3300/3602 batches | lr 0.0000 | ms/batch 90.86 | loss  9.21 | mse  9.21 | mre  0.00 |
scGPT - INFO - | epoch  47 | 3400/3602 batches | lr 0.0000 | ms/batch 91.39 | loss 11.05 | mse 11.05 | mre  0.00 |
scGPT - INFO - | epoch  47 | 3500/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 10.47 | mse 10.47 | mre  0.00 |
scGPT - INFO - | epoch  47 | 3600/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 11.45 | mse 11.45 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  47 | time: 339.71s | valid loss/mse 12.0025 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 11.977011192157473, min_delta 0.0001, val_loss 12.002456734391783
Loss error: -0.02544554223431028
scGPT - INFO - INFO: Early stopping counter 4 of 10
epoch:  47
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.953 3.496 3.217 ... 3.31  3.555 3.848]
 [7.01  3.572 3.27  ... 3.389 3.617 3.893]
 [4.4   4.793 4.766 ... 3.955 4.332 3.754]
 ...
 [4.71  3.791 3.826 ... 3.986 3.688 3.527]
 [4.863 3.857 3.916 ... 4.01  3.838 3.56 ]
 [6.88  3.65  3.312 ... 3.48  3.668 3.906]]
(801, 11) (801, 11)
By feature:  MSE:  0.5456¬±0.0 MAE:  0.4794¬±0.0 R2:  0.2139¬±0.0 PCC:  0.8141¬±0.0 Cosine Similarity:  0.9903¬±0.0
By sample:  MSE:  0.5456¬±0.0 MAE:  0.4794¬±0.0 R2:  0.3128¬±0.0 PCC:  0.5547¬±0.0 Cosine Similarity:  0.9861¬±0.0
scGPT - INFO - By feature: MSE: 0.5455999970436096¬±0.0 MAE: 0.47940000891685486¬±0.0 R2: 0.2139¬±0.0 PCC: 0.8141¬±0.0 Cosine Similarity: 0.9903¬±0.0
scGPT - INFO - By sample: MSE: 0.5455999970436096¬±0.0 MAE: 0.47940000891685486¬±0.0 R2: 0.3128¬±0.0 PCC: 0.5547¬±0.0 Cosine Similarity: 0.9861¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  48
Training model
scGPT - INFO - | epoch  48 | 100/3602 batches | lr 0.0000 | ms/batch 92.25 | loss 11.19 | mse 11.19 | mre  0.00 |
scGPT - INFO - | epoch  48 | 200/3602 batches | lr 0.0000 | ms/batch 90.91 | loss 11.75 | mse 11.75 | mre  0.00 |
scGPT - INFO - | epoch  48 | 300/3602 batches | lr 0.0000 | ms/batch 90.99 | loss 10.08 | mse 10.08 | mre  0.00 |
scGPT - INFO - | epoch  48 | 400/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 10.20 | mse 10.20 | mre  0.00 |
scGPT - INFO - | epoch  48 | 500/3602 batches | lr 0.0000 | ms/batch 90.70 | loss  9.19 | mse  9.19 | mre  0.00 |
scGPT - INFO - | epoch  48 | 600/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 10.97 | mse 10.97 | mre  0.00 |
scGPT - INFO - | epoch  48 | 700/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 11.07 | mse 11.07 | mre  0.00 |
scGPT - INFO - | epoch  48 | 800/3602 batches | lr 0.0000 | ms/batch 91.28 | loss 13.65 | mse 13.65 | mre  0.00 |
scGPT - INFO - | epoch  48 | 900/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 10.61 | mse 10.61 | mre  0.00 |
scGPT - INFO - | epoch  48 | 1000/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 12.62 | mse 12.62 | mre  0.00 |
scGPT - INFO - | epoch  48 | 1100/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 10.80 | mse 10.80 | mre  0.00 |
scGPT - INFO - | epoch  48 | 1200/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 11.11 | mse 11.11 | mre  0.00 |
scGPT - INFO - | epoch  48 | 1300/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 10.80 | mse 10.80 | mre  0.00 |
scGPT - INFO - | epoch  48 | 1400/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 12.80 | mse 12.80 | mre  0.00 |
scGPT - INFO - | epoch  48 | 1500/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 14.19 | mse 14.19 | mre  0.00 |
scGPT - INFO - | epoch  48 | 1600/3602 batches | lr 0.0000 | ms/batch 90.67 | loss  9.86 | mse  9.86 | mre  0.00 |
scGPT - INFO - | epoch  48 | 1700/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 10.38 | mse 10.38 | mre  0.00 |
scGPT - INFO - | epoch  48 | 1800/3602 batches | lr 0.0000 | ms/batch 91.22 | loss  9.84 | mse  9.84 | mre  0.00 |
scGPT - INFO - | epoch  48 | 1900/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 11.78 | mse 11.78 | mre  0.00 |
scGPT - INFO - | epoch  48 | 2000/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 11.16 | mse 11.16 | mre  0.00 |
scGPT - INFO - | epoch  48 | 2100/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 12.50 | mse 12.50 | mre  0.00 |
scGPT - INFO - | epoch  48 | 2200/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 11.66 | mse 11.66 | mre  0.00 |
scGPT - INFO - | epoch  48 | 2300/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.12 | mse 10.12 | mre  0.00 |
scGPT - INFO - | epoch  48 | 2400/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 11.13 | mse 11.13 | mre  0.00 |
scGPT - INFO - | epoch  48 | 2500/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 10.87 | mse 10.87 | mre  0.00 |
scGPT - INFO - | epoch  48 | 2600/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 13.73 | mse 13.73 | mre  0.00 |
scGPT - INFO - | epoch  48 | 2700/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 10.73 | mse 10.73 | mre  0.00 |
scGPT - INFO - | epoch  48 | 2800/3602 batches | lr 0.0000 | ms/batch 91.29 | loss 10.92 | mse 10.92 | mre  0.00 |
scGPT - INFO - | epoch  48 | 2900/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 11.73 | mse 11.73 | mre  0.00 |
scGPT - INFO - | epoch  48 | 3000/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 11.42 | mse 11.42 | mre  0.00 |
scGPT - INFO - | epoch  48 | 3100/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 10.92 | mse 10.92 | mre  0.00 |
scGPT - INFO - | epoch  48 | 3200/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 10.96 | mse 10.96 | mre  0.00 |
scGPT - INFO - | epoch  48 | 3300/3602 batches | lr 0.0000 | ms/batch 90.90 | loss  9.45 | mse  9.45 | mre  0.00 |
scGPT - INFO - | epoch  48 | 3400/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 10.98 | mse 10.98 | mre  0.00 |
scGPT - INFO - | epoch  48 | 3500/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 10.77 | mse 10.77 | mre  0.00 |
scGPT - INFO - | epoch  48 | 3600/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 11.51 | mse 11.51 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  48 | time: 339.59s | valid loss/mse 11.9905 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 11.977011192157473, min_delta 0.0001, val_loss 11.990539593791842
Loss error: -0.013528401634369303
scGPT - INFO - INFO: Early stopping counter 5 of 10
epoch:  48
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.953 3.49  3.213 ... 3.31  3.555 3.846]
 [7.01  3.568 3.268 ... 3.387 3.62  3.89 ]
 [4.51  4.68  4.65  ... 4.023 4.312 3.77 ]
 ...
 [4.707 3.793 3.824 ... 3.986 3.686 3.525]
 [4.86  3.86  3.916 ... 4.016 3.838 3.559]
 [6.887 3.645 3.307 ... 3.475 3.666 3.904]]
(801, 11) (801, 11)
By feature:  MSE:  0.5451¬±0.0 MAE:  0.4785¬±0.0 R2:  0.2139¬±0.0 PCC:  0.8156¬±0.0 Cosine Similarity:  0.9903¬±0.0
By sample:  MSE:  0.5451¬±0.0 MAE:  0.4785¬±0.0 R2:  0.3142¬±0.0 PCC:  0.5558¬±0.0 Cosine Similarity:  0.9861¬±0.0
scGPT - INFO - By feature: MSE: 0.5450999736785889¬±0.0 MAE: 0.47850000858306885¬±0.0 R2: 0.2139¬±0.0 PCC: 0.8156¬±0.0 Cosine Similarity: 0.9903¬±0.0
scGPT - INFO - By sample: MSE: 0.5450999736785889¬±0.0 MAE: 0.47850000858306885¬±0.0 R2: 0.3142¬±0.0 PCC: 0.5558¬±0.0 Cosine Similarity: 0.9861¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  49
Training model
scGPT - INFO - | epoch  49 | 100/3602 batches | lr 0.0000 | ms/batch 92.06 | loss 11.02 | mse 11.02 | mre  0.00 |
scGPT - INFO - | epoch  49 | 200/3602 batches | lr 0.0000 | ms/batch 91.39 | loss 11.91 | mse 11.91 | mre  0.00 |
scGPT - INFO - | epoch  49 | 300/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 10.08 | mse 10.08 | mre  0.00 |
scGPT - INFO - | epoch  49 | 400/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 10.00 | mse 10.00 | mre  0.00 |
scGPT - INFO - | epoch  49 | 500/3602 batches | lr 0.0000 | ms/batch 90.84 | loss  8.61 | mse  8.61 | mre  0.00 |
scGPT - INFO - | epoch  49 | 600/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 10.57 | mse 10.57 | mre  0.00 |
scGPT - INFO - | epoch  49 | 700/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 11.24 | mse 11.24 | mre  0.00 |
scGPT - INFO - | epoch  49 | 800/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 13.66 | mse 13.66 | mre  0.00 |
scGPT - INFO - | epoch  49 | 900/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 10.60 | mse 10.60 | mre  0.00 |
scGPT - INFO - | epoch  49 | 1000/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 12.56 | mse 12.56 | mre  0.00 |
scGPT - INFO - | epoch  49 | 1100/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 10.84 | mse 10.84 | mre  0.00 |
scGPT - INFO - | epoch  49 | 1200/3602 batches | lr 0.0000 | ms/batch 91.25 | loss 10.82 | mse 10.82 | mre  0.00 |
scGPT - INFO - | epoch  49 | 1300/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 11.26 | mse 11.26 | mre  0.00 |
scGPT - INFO - | epoch  49 | 1400/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 12.73 | mse 12.73 | mre  0.00 |
scGPT - INFO - | epoch  49 | 1500/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 14.55 | mse 14.55 | mre  0.00 |
scGPT - INFO - | epoch  49 | 1600/3602 batches | lr 0.0000 | ms/batch 90.72 | loss  9.65 | mse  9.65 | mre  0.00 |
scGPT - INFO - | epoch  49 | 1700/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 10.07 | mse 10.07 | mre  0.00 |
scGPT - INFO - | epoch  49 | 1800/3602 batches | lr 0.0000 | ms/batch 90.70 | loss  9.79 | mse  9.79 | mre  0.00 |
scGPT - INFO - | epoch  49 | 1900/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 11.64 | mse 11.64 | mre  0.00 |
scGPT - INFO - | epoch  49 | 2000/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 10.85 | mse 10.85 | mre  0.00 |
scGPT - INFO - | epoch  49 | 2100/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 12.95 | mse 12.95 | mre  0.00 |
scGPT - INFO - | epoch  49 | 2200/3602 batches | lr 0.0000 | ms/batch 91.27 | loss 11.79 | mse 11.79 | mre  0.00 |
scGPT - INFO - | epoch  49 | 2300/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 10.13 | mse 10.13 | mre  0.00 |
scGPT - INFO - | epoch  49 | 2400/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 11.11 | mse 11.11 | mre  0.00 |
scGPT - INFO - | epoch  49 | 2500/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 10.37 | mse 10.37 | mre  0.00 |
scGPT - INFO - | epoch  49 | 2600/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 14.01 | mse 14.01 | mre  0.00 |
scGPT - INFO - | epoch  49 | 2700/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 10.97 | mse 10.97 | mre  0.00 |
scGPT - INFO - | epoch  49 | 2800/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 11.12 | mse 11.12 | mre  0.00 |
scGPT - INFO - | epoch  49 | 2900/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 11.47 | mse 11.47 | mre  0.00 |
scGPT - INFO - | epoch  49 | 3000/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 11.50 | mse 11.50 | mre  0.00 |
scGPT - INFO - | epoch  49 | 3100/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 10.38 | mse 10.38 | mre  0.00 |
scGPT - INFO - | epoch  49 | 3200/3602 batches | lr 0.0000 | ms/batch 91.31 | loss 10.60 | mse 10.60 | mre  0.00 |
scGPT - INFO - | epoch  49 | 3300/3602 batches | lr 0.0000 | ms/batch 90.78 | loss  9.58 | mse  9.58 | mre  0.00 |
scGPT - INFO - | epoch  49 | 3400/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 11.39 | mse 11.39 | mre  0.00 |
scGPT - INFO - | epoch  49 | 3500/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.40 | mse 10.40 | mre  0.00 |
scGPT - INFO - | epoch  49 | 3600/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 11.68 | mse 11.68 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  49 | time: 339.68s | valid loss/mse 12.0278 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 11.977011192157473, min_delta 0.0001, val_loss 12.027826386891054
Loss error: -0.05081519473358043
scGPT - INFO - INFO: Early stopping counter 6 of 10
epoch:  49
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.965 3.494 3.21  ... 3.309 3.553 3.848]
 [7.023 3.57  3.264 ... 3.387 3.617 3.893]
 [4.53  4.76  4.7   ... 4.023 4.324 3.768]
 ...
 [4.69  3.785 3.816 ... 3.98  3.678 3.518]
 [4.848 3.855 3.91  ... 4.01  3.832 3.555]
 [6.887 3.654 3.309 ... 3.48  3.668 3.91 ]]
(801, 11) (801, 11)
By feature:  MSE:  0.5468¬±0.0 MAE:  0.4801¬±0.0 R2:  0.2069¬±0.0 PCC:  0.8151¬±0.0 Cosine Similarity:  0.9903¬±0.0
By sample:  MSE:  0.5468¬±0.0 MAE:  0.4801¬±0.0 R2:  0.3121¬±0.0 PCC:  0.5549¬±0.0 Cosine Similarity:  0.986¬±0.0
scGPT - INFO - By feature: MSE: 0.5468000173568726¬±0.0 MAE: 0.48010000586509705¬±0.0 R2: 0.2069¬±0.0 PCC: 0.8151¬±0.0 Cosine Similarity: 0.9903¬±0.0
scGPT - INFO - By sample: MSE: 0.5468000173568726¬±0.0 MAE: 0.48010000586509705¬±0.0 R2: 0.3121¬±0.0 PCC: 0.5549¬±0.0 Cosine Similarity: 0.986¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  50
Training model
scGPT - INFO - | epoch  50 | 100/3602 batches | lr 0.0000 | ms/batch 92.42 | loss 10.78 | mse 10.78 | mre  0.00 |
scGPT - INFO - | epoch  50 | 200/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 11.97 | mse 11.97 | mre  0.00 |
scGPT - INFO - | epoch  50 | 300/3602 batches | lr 0.0000 | ms/batch 90.80 | loss  9.90 | mse  9.90 | mre  0.00 |
scGPT - INFO - | epoch  50 | 400/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 10.57 | mse 10.57 | mre  0.00 |
scGPT - INFO - | epoch  50 | 500/3602 batches | lr 0.0000 | ms/batch 90.84 | loss  8.84 | mse  8.84 | mre  0.00 |
scGPT - INFO - | epoch  50 | 600/3602 batches | lr 0.0000 | ms/batch 91.49 | loss 10.39 | mse 10.39 | mre  0.00 |
scGPT - INFO - | epoch  50 | 700/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 11.28 | mse 11.28 | mre  0.00 |
scGPT - INFO - | epoch  50 | 800/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 13.58 | mse 13.58 | mre  0.00 |
scGPT - INFO - | epoch  50 | 900/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 10.91 | mse 10.91 | mre  0.00 |
scGPT - INFO - | epoch  50 | 1000/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 12.56 | mse 12.56 | mre  0.00 |
scGPT - INFO - | epoch  50 | 1100/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 10.76 | mse 10.76 | mre  0.00 |
scGPT - INFO - | epoch  50 | 1200/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 11.15 | mse 11.15 | mre  0.00 |
scGPT - INFO - | epoch  50 | 1300/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 11.11 | mse 11.11 | mre  0.00 |
scGPT - INFO - | epoch  50 | 1400/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 12.22 | mse 12.22 | mre  0.00 |
scGPT - INFO - | epoch  50 | 1500/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 14.10 | mse 14.10 | mre  0.00 |
scGPT - INFO - | epoch  50 | 1600/3602 batches | lr 0.0000 | ms/batch 91.39 | loss  9.65 | mse  9.65 | mre  0.00 |
scGPT - INFO - | epoch  50 | 1700/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 10.41 | mse 10.41 | mre  0.00 |
scGPT - INFO - | epoch  50 | 1800/3602 batches | lr 0.0000 | ms/batch 90.85 | loss  9.91 | mse  9.91 | mre  0.00 |
scGPT - INFO - | epoch  50 | 1900/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 11.65 | mse 11.65 | mre  0.00 |
scGPT - INFO - | epoch  50 | 2000/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 11.72 | mse 11.72 | mre  0.00 |
scGPT - INFO - | epoch  50 | 2100/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 12.91 | mse 12.91 | mre  0.00 |
scGPT - INFO - | epoch  50 | 2200/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 11.47 | mse 11.47 | mre  0.00 |
scGPT - INFO - | epoch  50 | 2300/3602 batches | lr 0.0000 | ms/batch 90.97 | loss 10.09 | mse 10.09 | mre  0.00 |
scGPT - INFO - | epoch  50 | 2400/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 11.20 | mse 11.20 | mre  0.00 |
scGPT - INFO - | epoch  50 | 2500/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 10.29 | mse 10.29 | mre  0.00 |
scGPT - INFO - | epoch  50 | 2600/3602 batches | lr 0.0000 | ms/batch 91.36 | loss 13.30 | mse 13.30 | mre  0.00 |
scGPT - INFO - | epoch  50 | 2700/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 10.61 | mse 10.61 | mre  0.00 |
scGPT - INFO - | epoch  50 | 2800/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 11.28 | mse 11.28 | mre  0.00 |
scGPT - INFO - | epoch  50 | 2900/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 11.46 | mse 11.46 | mre  0.00 |
scGPT - INFO - | epoch  50 | 3000/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 11.50 | mse 11.50 | mre  0.00 |
scGPT - INFO - | epoch  50 | 3100/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 10.94 | mse 10.94 | mre  0.00 |
scGPT - INFO - | epoch  50 | 3200/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 11.23 | mse 11.23 | mre  0.00 |
scGPT - INFO - | epoch  50 | 3300/3602 batches | lr 0.0000 | ms/batch 90.81 | loss  9.25 | mse  9.25 | mre  0.00 |
scGPT - INFO - | epoch  50 | 3400/3602 batches | lr 0.0000 | ms/batch 91.13 | loss 11.25 | mse 11.25 | mre  0.00 |
scGPT - INFO - | epoch  50 | 3500/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 10.60 | mse 10.60 | mre  0.00 |
scGPT - INFO - | epoch  50 | 3600/3602 batches | lr 0.0000 | ms/batch 91.37 | loss 11.52 | mse 11.52 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  50 | time: 339.94s | valid loss/mse 11.9973 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 11.977011192157473, min_delta 0.0001, val_loss 11.997282024924079
Loss error: -0.020270832766605906
scGPT - INFO - INFO: Early stopping counter 7 of 10
epoch:  50
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.965 3.488 3.209 ... 3.305 3.553 3.844]
 [7.016 3.564 3.264 ... 3.385 3.615 3.887]
 [4.49  4.81  4.746 ... 3.986 4.34  3.764]
 ...
 [4.707 3.791 3.822 ... 3.984 3.684 3.523]
 [4.863 3.861 3.916 ... 4.016 3.842 3.56 ]
 [6.89  3.639 3.305 ... 3.47  3.664 3.902]]
(801, 11) (801, 11)
By feature:  MSE:  0.5454¬±0.0 MAE:  0.4784¬±0.0 R2:  0.2126¬±0.0 PCC:  0.8161¬±0.0 Cosine Similarity:  0.9903¬±0.0
By sample:  MSE:  0.5454¬±0.0 MAE:  0.4784¬±0.0 R2:  0.3137¬±0.0 PCC:  0.5566¬±0.0 Cosine Similarity:  0.9861¬±0.0
scGPT - INFO - By feature: MSE: 0.5454000234603882¬±0.0 MAE: 0.47839999198913574¬±0.0 R2: 0.2126¬±0.0 PCC: 0.8161¬±0.0 Cosine Similarity: 0.9903¬±0.0
scGPT - INFO - By sample: MSE: 0.5454000234603882¬±0.0 MAE: 0.47839999198913574¬±0.0 R2: 0.3137¬±0.0 PCC: 0.5566¬±0.0 Cosine Similarity: 0.9861¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  51
Training model
scGPT - INFO - | epoch  51 | 100/3602 batches | lr 0.0000 | ms/batch 92.36 | loss 10.93 | mse 10.93 | mre  0.00 |
scGPT - INFO - | epoch  51 | 200/3602 batches | lr 0.0000 | ms/batch 90.98 | loss 12.12 | mse 12.12 | mre  0.00 |
scGPT - INFO - | epoch  51 | 300/3602 batches | lr 0.0000 | ms/batch 92.38 | loss  9.94 | mse  9.94 | mre  0.00 |
scGPT - INFO - | epoch  51 | 400/3602 batches | lr 0.0000 | ms/batch 92.31 | loss 10.37 | mse 10.37 | mre  0.00 |
scGPT - INFO - | epoch  51 | 500/3602 batches | lr 0.0000 | ms/batch 91.84 | loss  8.67 | mse  8.67 | mre  0.00 |
scGPT - INFO - | epoch  51 | 600/3602 batches | lr 0.0000 | ms/batch 91.68 | loss 10.43 | mse 10.43 | mre  0.00 |
scGPT - INFO - | epoch  51 | 700/3602 batches | lr 0.0000 | ms/batch 91.71 | loss 11.44 | mse 11.44 | mre  0.00 |
scGPT - INFO - | epoch  51 | 800/3602 batches | lr 0.0000 | ms/batch 91.94 | loss 13.74 | mse 13.74 | mre  0.00 |
scGPT - INFO - | epoch  51 | 900/3602 batches | lr 0.0000 | ms/batch 91.49 | loss 10.96 | mse 10.96 | mre  0.00 |
scGPT - INFO - | epoch  51 | 1000/3602 batches | lr 0.0000 | ms/batch 90.98 | loss 12.03 | mse 12.03 | mre  0.00 |
scGPT - INFO - | epoch  51 | 1100/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 10.79 | mse 10.79 | mre  0.00 |
scGPT - INFO - | epoch  51 | 1200/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 10.95 | mse 10.95 | mre  0.00 |
scGPT - INFO - | epoch  51 | 1300/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 10.94 | mse 10.94 | mre  0.00 |
scGPT - INFO - | epoch  51 | 1400/3602 batches | lr 0.0000 | ms/batch 91.04 | loss 12.58 | mse 12.58 | mre  0.00 |
scGPT - INFO - | epoch  51 | 1500/3602 batches | lr 0.0000 | ms/batch 91.03 | loss 14.30 | mse 14.30 | mre  0.00 |
scGPT - INFO - | epoch  51 | 1600/3602 batches | lr 0.0000 | ms/batch 90.94 | loss  9.81 | mse  9.81 | mre  0.00 |
scGPT - INFO - | epoch  51 | 1700/3602 batches | lr 0.0000 | ms/batch 91.01 | loss 10.26 | mse 10.26 | mre  0.00 |
scGPT - INFO - | epoch  51 | 1800/3602 batches | lr 0.0000 | ms/batch 90.98 | loss 10.03 | mse 10.03 | mre  0.00 |
scGPT - INFO - | epoch  51 | 1900/3602 batches | lr 0.0000 | ms/batch 91.74 | loss 11.72 | mse 11.72 | mre  0.00 |
scGPT - INFO - | epoch  51 | 2000/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 11.38 | mse 11.38 | mre  0.00 |
scGPT - INFO - | epoch  51 | 2100/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 12.59 | mse 12.59 | mre  0.00 |
scGPT - INFO - | epoch  51 | 2200/3602 batches | lr 0.0000 | ms/batch 90.91 | loss 11.98 | mse 11.98 | mre  0.00 |
scGPT - INFO - | epoch  51 | 2300/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 10.19 | mse 10.19 | mre  0.00 |
scGPT - INFO - | epoch  51 | 2400/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 11.63 | mse 11.63 | mre  0.00 |
scGPT - INFO - | epoch  51 | 2500/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 10.83 | mse 10.83 | mre  0.00 |
scGPT - INFO - | epoch  51 | 2600/3602 batches | lr 0.0000 | ms/batch 91.00 | loss 13.69 | mse 13.69 | mre  0.00 |
scGPT - INFO - | epoch  51 | 2700/3602 batches | lr 0.0000 | ms/batch 91.10 | loss 10.50 | mse 10.50 | mre  0.00 |
scGPT - INFO - | epoch  51 | 2800/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 11.13 | mse 11.13 | mre  0.00 |
scGPT - INFO - | epoch  51 | 2900/3602 batches | lr 0.0000 | ms/batch 91.49 | loss 11.49 | mse 11.49 | mre  0.00 |
scGPT - INFO - | epoch  51 | 3000/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 11.62 | mse 11.62 | mre  0.00 |
scGPT - INFO - | epoch  51 | 3100/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 10.70 | mse 10.70 | mre  0.00 |
scGPT - INFO - | epoch  51 | 3200/3602 batches | lr 0.0000 | ms/batch 90.98 | loss 10.65 | mse 10.65 | mre  0.00 |
scGPT - INFO - | epoch  51 | 3300/3602 batches | lr 0.0000 | ms/batch 90.91 | loss  9.33 | mse  9.33 | mre  0.00 |
scGPT - INFO - | epoch  51 | 3400/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 11.18 | mse 11.18 | mre  0.00 |
scGPT - INFO - | epoch  51 | 3500/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 10.79 | mse 10.79 | mre  0.00 |
scGPT - INFO - | epoch  51 | 3600/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 11.57 | mse 11.57 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  51 | time: 341.04s | valid loss/mse 12.0024 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 11.977011192157473, min_delta 0.0001, val_loss 12.002355680334732
Loss error: -0.025344488177259095
scGPT - INFO - INFO: Early stopping counter 8 of 10
epoch:  51
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.953 3.492 3.213 ... 3.309 3.555 3.842]
 [7.004 3.568 3.268 ... 3.387 3.617 3.887]
 [4.504 4.863 4.785 ... 3.996 4.355 3.771]
 ...
 [4.703 3.791 3.822 ... 3.984 3.684 3.521]
 [4.863 3.865 3.92  ... 4.02  3.844 3.56 ]
 [6.88  3.646 3.307 ... 3.475 3.666 3.9  ]]
(801, 11) (801, 11)
By feature:  MSE:  0.5456¬±0.0 MAE:  0.4789¬±0.0 R2:  0.2102¬±0.0 PCC:  0.8159¬±0.0 Cosine Similarity:  0.9903¬±0.0
By sample:  MSE:  0.5456¬±0.0 MAE:  0.4789¬±0.0 R2:  0.3133¬±0.0 PCC:  0.556¬±0.0 Cosine Similarity:  0.9861¬±0.0
scGPT - INFO - By feature: MSE: 0.5455999970436096¬±0.0 MAE: 0.4788999855518341¬±0.0 R2: 0.2102¬±0.0 PCC: 0.8159¬±0.0 Cosine Similarity: 0.9903¬±0.0
scGPT - INFO - By sample: MSE: 0.5455999970436096¬±0.0 MAE: 0.4788999855518341¬±0.0 R2: 0.3133¬±0.0 PCC: 0.556¬±0.0 Cosine Similarity: 0.9861¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  52
Training model
scGPT - INFO - | epoch  52 | 100/3602 batches | lr 0.0000 | ms/batch 92.74 | loss 10.90 | mse 10.90 | mre  0.00 |
scGPT - INFO - | epoch  52 | 200/3602 batches | lr 0.0000 | ms/batch 91.47 | loss 11.59 | mse 11.59 | mre  0.00 |
scGPT - INFO - | epoch  52 | 300/3602 batches | lr 0.0000 | ms/batch 92.46 | loss  9.98 | mse  9.98 | mre  0.00 |
scGPT - INFO - | epoch  52 | 400/3602 batches | lr 0.0000 | ms/batch 91.79 | loss 10.01 | mse 10.01 | mre  0.00 |
scGPT - INFO - | epoch  52 | 500/3602 batches | lr 0.0000 | ms/batch 91.32 | loss  8.75 | mse  8.75 | mre  0.00 |
scGPT - INFO - | epoch  52 | 600/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 10.71 | mse 10.71 | mre  0.00 |
scGPT - INFO - | epoch  52 | 700/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 11.19 | mse 11.19 | mre  0.00 |
scGPT - INFO - | epoch  52 | 800/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 13.34 | mse 13.34 | mre  0.00 |
scGPT - INFO - | epoch  52 | 900/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.96 | mse 10.96 | mre  0.00 |
scGPT - INFO - | epoch  52 | 1000/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 12.52 | mse 12.52 | mre  0.00 |
scGPT - INFO - | epoch  52 | 1100/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 11.08 | mse 11.08 | mre  0.00 |
scGPT - INFO - | epoch  52 | 1200/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 11.09 | mse 11.09 | mre  0.00 |
scGPT - INFO - | epoch  52 | 1300/3602 batches | lr 0.0000 | ms/batch 91.33 | loss 10.86 | mse 10.86 | mre  0.00 |
scGPT - INFO - | epoch  52 | 1400/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 12.70 | mse 12.70 | mre  0.00 |
scGPT - INFO - | epoch  52 | 1500/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 14.15 | mse 14.15 | mre  0.00 |
scGPT - INFO - | epoch  52 | 1600/3602 batches | lr 0.0000 | ms/batch 90.74 | loss  9.91 | mse  9.91 | mre  0.00 |
scGPT - INFO - | epoch  52 | 1700/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.43 | mse 10.43 | mre  0.00 |
scGPT - INFO - | epoch  52 | 1800/3602 batches | lr 0.0000 | ms/batch 91.38 | loss 10.01 | mse 10.01 | mre  0.00 |
scGPT - INFO - | epoch  52 | 1900/3602 batches | lr 0.0000 | ms/batch 91.34 | loss 11.38 | mse 11.38 | mre  0.00 |
scGPT - INFO - | epoch  52 | 2000/3602 batches | lr 0.0000 | ms/batch 91.76 | loss 11.15 | mse 11.15 | mre  0.00 |
scGPT - INFO - | epoch  52 | 2100/3602 batches | lr 0.0000 | ms/batch 91.99 | loss 12.54 | mse 12.54 | mre  0.00 |
scGPT - INFO - | epoch  52 | 2200/3602 batches | lr 0.0000 | ms/batch 91.56 | loss 12.29 | mse 12.29 | mre  0.00 |
scGPT - INFO - | epoch  52 | 2300/3602 batches | lr 0.0000 | ms/batch 91.93 | loss 10.50 | mse 10.50 | mre  0.00 |
scGPT - INFO - | epoch  52 | 2400/3602 batches | lr 0.0000 | ms/batch 92.18 | loss 11.48 | mse 11.48 | mre  0.00 |
scGPT - INFO - | epoch  52 | 2500/3602 batches | lr 0.0000 | ms/batch 91.82 | loss 10.37 | mse 10.37 | mre  0.00 |
scGPT - INFO - | epoch  52 | 2600/3602 batches | lr 0.0000 | ms/batch 91.60 | loss 13.48 | mse 13.48 | mre  0.00 |
scGPT - INFO - | epoch  52 | 2700/3602 batches | lr 0.0000 | ms/batch 91.41 | loss 10.65 | mse 10.65 | mre  0.00 |
scGPT - INFO - | epoch  52 | 2800/3602 batches | lr 0.0000 | ms/batch 91.48 | loss 11.15 | mse 11.15 | mre  0.00 |
scGPT - INFO - | epoch  52 | 2900/3602 batches | lr 0.0000 | ms/batch 91.32 | loss 11.41 | mse 11.41 | mre  0.00 |
scGPT - INFO - | epoch  52 | 3000/3602 batches | lr 0.0000 | ms/batch 91.05 | loss 11.53 | mse 11.53 | mre  0.00 |
scGPT - INFO - | epoch  52 | 3100/3602 batches | lr 0.0000 | ms/batch 91.21 | loss 10.63 | mse 10.63 | mre  0.00 |
scGPT - INFO - | epoch  52 | 3200/3602 batches | lr 0.0000 | ms/batch 91.43 | loss 11.08 | mse 11.08 | mre  0.00 |
scGPT - INFO - | epoch  52 | 3300/3602 batches | lr 0.0000 | ms/batch 91.37 | loss  9.28 | mse  9.28 | mre  0.00 |
scGPT - INFO - | epoch  52 | 3400/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 11.36 | mse 11.36 | mre  0.00 |
scGPT - INFO - | epoch  52 | 3500/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 10.57 | mse 10.57 | mre  0.00 |
scGPT - INFO - | epoch  52 | 3600/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 11.56 | mse 11.56 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  52 | time: 341.17s | valid loss/mse 11.9842 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 11.977011192157473, min_delta 0.0001, val_loss 11.98419143391608
Loss error: -0.007180241758607764
scGPT - INFO - INFO: Early stopping counter 9 of 10
epoch:  52
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.957 3.49  3.209 ... 3.307 3.55  3.84 ]
 [7.016 3.568 3.264 ... 3.385 3.615 3.885]
 [4.586 4.703 4.656 ... 4.06  4.324 3.785]
 ...
 [4.715 3.797 3.824 ... 3.986 3.69  3.523]
 [4.875 3.87  3.92  ... 4.02  3.848 3.562]
 [6.887 3.65  3.307 ... 3.477 3.666 3.902]]
(801, 11) (801, 11)
By feature:  MSE:  0.5448¬±0.0 MAE:  0.4786¬±0.0 R2:  0.2103¬±0.0 PCC:  0.8166¬±0.0 Cosine Similarity:  0.9903¬±0.0
By sample:  MSE:  0.5448¬±0.0 MAE:  0.4786¬±0.0 R2:  0.3144¬±0.0 PCC:  0.5565¬±0.0 Cosine Similarity:  0.9861¬±0.0
scGPT - INFO - By feature: MSE: 0.5447999835014343¬±0.0 MAE: 0.47859999537467957¬±0.0 R2: 0.2103¬±0.0 PCC: 0.8166¬±0.0 Cosine Similarity: 0.9903¬±0.0
scGPT - INFO - By sample: MSE: 0.5447999835014343¬±0.0 MAE: 0.47859999537467957¬±0.0 R2: 0.3144¬±0.0 PCC: 0.5565¬±0.0 Cosine Similarity: 0.9861¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  53
Training model
scGPT - INFO - | epoch  53 | 100/3602 batches | lr 0.0000 | ms/batch 92.19 | loss 11.07 | mse 11.07 | mre  0.00 |
scGPT - INFO - | epoch  53 | 200/3602 batches | lr 0.0000 | ms/batch 91.04 | loss 11.95 | mse 11.95 | mre  0.00 |
scGPT - INFO - | epoch  53 | 300/3602 batches | lr 0.0000 | ms/batch 91.14 | loss 10.10 | mse 10.10 | mre  0.00 |
scGPT - INFO - | epoch  53 | 400/3602 batches | lr 0.0000 | ms/batch 91.19 | loss 10.43 | mse 10.43 | mre  0.00 |
scGPT - INFO - | epoch  53 | 500/3602 batches | lr 0.0000 | ms/batch 91.21 | loss  9.31 | mse  9.31 | mre  0.00 |
scGPT - INFO - | epoch  53 | 600/3602 batches | lr 0.0000 | ms/batch 91.13 | loss 10.26 | mse 10.26 | mre  0.00 |
scGPT - INFO - | epoch  53 | 700/3602 batches | lr 0.0000 | ms/batch 91.95 | loss 11.41 | mse 11.41 | mre  0.00 |
scGPT - INFO - | epoch  53 | 800/3602 batches | lr 0.0000 | ms/batch 91.09 | loss 13.42 | mse 13.42 | mre  0.00 |
scGPT - INFO - | epoch  53 | 900/3602 batches | lr 0.0000 | ms/batch 91.12 | loss 10.67 | mse 10.67 | mre  0.00 |
scGPT - INFO - | epoch  53 | 1000/3602 batches | lr 0.0000 | ms/batch 91.14 | loss 12.61 | mse 12.61 | mre  0.00 |
scGPT - INFO - | epoch  53 | 1100/3602 batches | lr 0.0000 | ms/batch 91.15 | loss 10.47 | mse 10.47 | mre  0.00 |
scGPT - INFO - | epoch  53 | 1200/3602 batches | lr 0.0000 | ms/batch 91.06 | loss 11.13 | mse 11.13 | mre  0.00 |
scGPT - INFO - | epoch  53 | 1300/3602 batches | lr 0.0000 | ms/batch 91.15 | loss 11.32 | mse 11.32 | mre  0.00 |
scGPT - INFO - | epoch  53 | 1400/3602 batches | lr 0.0000 | ms/batch 91.07 | loss 12.88 | mse 12.88 | mre  0.00 |
scGPT - INFO - | epoch  53 | 1500/3602 batches | lr 0.0000 | ms/batch 91.10 | loss 14.10 | mse 14.10 | mre  0.00 |
scGPT - INFO - | epoch  53 | 1600/3602 batches | lr 0.0000 | ms/batch 91.11 | loss  9.85 | mse  9.85 | mre  0.00 |
scGPT - INFO - | epoch  53 | 1700/3602 batches | lr 0.0000 | ms/batch 91.69 | loss 10.79 | mse 10.79 | mre  0.00 |
scGPT - INFO - | epoch  53 | 1800/3602 batches | lr 0.0000 | ms/batch 91.10 | loss  9.71 | mse  9.71 | mre  0.00 |
scGPT - INFO - | epoch  53 | 1900/3602 batches | lr 0.0000 | ms/batch 91.09 | loss 11.70 | mse 11.70 | mre  0.00 |
scGPT - INFO - | epoch  53 | 2000/3602 batches | lr 0.0000 | ms/batch 91.33 | loss 11.26 | mse 11.26 | mre  0.00 |
scGPT - INFO - | epoch  53 | 2100/3602 batches | lr 0.0000 | ms/batch 91.12 | loss 12.87 | mse 12.87 | mre  0.00 |
scGPT - INFO - | epoch  53 | 2200/3602 batches | lr 0.0000 | ms/batch 91.12 | loss 11.79 | mse 11.79 | mre  0.00 |
scGPT - INFO - | epoch  53 | 2300/3602 batches | lr 0.0000 | ms/batch 91.21 | loss 10.14 | mse 10.14 | mre  0.00 |
scGPT - INFO - | epoch  53 | 2400/3602 batches | lr 0.0000 | ms/batch 91.17 | loss 11.66 | mse 11.66 | mre  0.00 |
scGPT - INFO - | epoch  53 | 2500/3602 batches | lr 0.0000 | ms/batch 91.18 | loss 10.04 | mse 10.04 | mre  0.00 |
scGPT - INFO - | epoch  53 | 2600/3602 batches | lr 0.0000 | ms/batch 91.21 | loss 13.54 | mse 13.54 | mre  0.00 |
scGPT - INFO - | epoch  53 | 2700/3602 batches | lr 0.0000 | ms/batch 91.66 | loss 10.89 | mse 10.89 | mre  0.00 |
scGPT - INFO - | epoch  53 | 2800/3602 batches | lr 0.0000 | ms/batch 92.22 | loss 10.94 | mse 10.94 | mre  0.00 |
scGPT - INFO - | epoch  53 | 2900/3602 batches | lr 0.0000 | ms/batch 96.15 | loss 11.38 | mse 11.38 | mre  0.00 |
scGPT - INFO - | epoch  53 | 3000/3602 batches | lr 0.0000 | ms/batch 96.15 | loss 11.97 | mse 11.97 | mre  0.00 |
scGPT - INFO - | epoch  53 | 3100/3602 batches | lr 0.0000 | ms/batch 96.17 | loss 10.78 | mse 10.78 | mre  0.00 |
scGPT - INFO - | epoch  53 | 3200/3602 batches | lr 0.0000 | ms/batch 96.08 | loss 11.03 | mse 11.03 | mre  0.00 |
scGPT - INFO - | epoch  53 | 3300/3602 batches | lr 0.0000 | ms/batch 96.17 | loss  9.28 | mse  9.28 | mre  0.00 |
scGPT - INFO - | epoch  53 | 3400/3602 batches | lr 0.0000 | ms/batch 96.17 | loss 10.99 | mse 10.99 | mre  0.00 |
scGPT - INFO - | epoch  53 | 3500/3602 batches | lr 0.0000 | ms/batch 93.85 | loss 10.41 | mse 10.41 | mre  0.00 |
scGPT - INFO - | epoch  53 | 3600/3602 batches | lr 0.0000 | ms/batch 93.31 | loss 11.57 | mse 11.57 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  53 | time: 346.17s | valid loss/mse 11.9961 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 11.977011192157473, min_delta 0.0001, val_loss 11.9960737504465
Loss error: -0.019062558289027365
scGPT - INFO - INFO: Early stopping counter 10 of 10
scGPT - INFO - INFO: Early stopping
scGPT - INFO - Best model saved successfully!
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | End test time: 346.28s | 
scGPT - INFO - -----------------------------------------------------------------------------------------
wandb: - 0.003 MB of 0.003 MB uploadedwandb: \ 0.003 MB of 0.003 MB uploadedwandb: | 0.003 MB of 0.003 MB uploadedwandb: / 0.003 MB of 0.003 MB uploadedwandb: - 0.073 MB of 0.073 MB uploaded (0.003 MB deduped)wandb: \ 0.073 MB of 0.514 MB uploaded (0.003 MB deduped)wandb: | 0.149 MB of 0.514 MB uploaded (0.003 MB deduped)wandb: / 0.514 MB of 0.514 MB uploaded (0.003 MB deduped)wandb: - 0.514 MB of 0.514 MB uploaded (0.003 MB deduped)wandb: 
wandb: Run history:
wandb:                        epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb: info/post_freeze_param_count ‚ñÅ
wandb:  info/pre_freeze_param_count ‚ñÅ
wandb:                    train/mse ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb:                    valid/dab ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                    valid/err ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                    valid/mse ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:            valid/sum_mse_dab ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                        epoch 53
wandb: info/post_freeze_param_count 2639372
wandb:  info/pre_freeze_param_count 29650444
wandb:                    train/mse 6.89204
wandb:                    valid/err 0.0
wandb: 
wandb: üöÄ View run crimson-pyramid-350 at: https://wandb.ai/wang_wandb/scGPT/runs/s6f1yc9c
wandb: Ô∏è‚ö° View job at https://wandb.ai/wang_wandb/scGPT/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE1NTQ0MTQ2Nw==/version_details/v33
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240423_163207-s6f1yc9c/logs
