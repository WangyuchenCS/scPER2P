nohup: ignoring input
Global seed set to 0
wandb: Currently logged in as: ywang2542-c (wang_wandb). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/wandb/run-20240423_213439-e6cgo4q7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-tree-351
wandb: ‚≠êÔ∏è View project at https://wandb.ai/wang_wandb/scGPT
wandb: üöÄ View run at https://wandb.ai/wang_wandb/scGPT/runs/e6cgo4q7
scPEFT_scGPT
Namespace(dataset='dataset3', lr=5e-05, use_prompt=True, prompt_type='Gene_encoder_prompt', data_name='ms', data_path='/home/wsl20/Projects/RNA2prot/scGPT/data/', space_conf=[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], mlp_conf=[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], epoch=100)
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]}
save to save/dataset3/Gene_encoder_prompt/5e-05
adata_gene (8005, 16508)
adata_protein (8005, 11)
celltype num_types:1
scGPT - INFO - match 15367/16508 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human/best_model.pt, the model args will override the config /home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human/args.json.
scGPT - INFO - Filtering genes by counts ...
scGPT - INFO - Filtering cells by counts ...
scGPT - INFO - Normalizing total counts ...
scGPT - INFO - Binning data ...
scGPT - INFO - train set number of samples: 7204, 
	 feature length: 2001
scGPT - INFO - valid set number of samples: 801, 
	 feature length: 2001
except!!!! only load params that are in the model and match the size!!!!!
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params decoder.fc.0.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params decoder.fc.0.bias with shape torch.Size([512])
scGPT - INFO - Loading params decoder.fc.2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params decoder.fc.2.bias with shape torch.Size([512])
Total Pre freeze Params 29650444
All para.requires_grad:  False, Freeze!
Adapter in name: transformer_encoder.layers.0.Space_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.0.Space_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.0.Space_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.0.Space_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.0.Space_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.0.Space_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.0.Space_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.0.Space_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.0.MLP_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.0.MLP_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.0.MLP_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.0.MLP_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.0.MLP_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.0.MLP_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.0.MLP_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.0.MLP_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.1.Space_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.1.Space_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.1.Space_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.1.Space_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.1.Space_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.1.Space_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.1.Space_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.1.Space_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.1.MLP_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.1.MLP_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.1.MLP_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.1.MLP_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.1.MLP_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.1.MLP_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.1.MLP_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.1.MLP_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.2.Space_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.2.Space_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.2.Space_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.2.Space_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.2.Space_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.2.Space_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.2.Space_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.2.Space_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.2.MLP_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.2.MLP_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.2.MLP_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.2.MLP_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.2.MLP_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.2.MLP_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.2.MLP_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.2.MLP_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.3.Space_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.3.Space_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.3.Space_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.3.Space_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.3.Space_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.3.Space_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.3.Space_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.3.Space_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.3.MLP_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.3.MLP_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.3.MLP_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.3.MLP_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.3.MLP_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.3.MLP_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.3.MLP_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.3.MLP_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.4.Space_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.4.Space_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.4.Space_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.4.Space_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.4.Space_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.4.Space_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.4.Space_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.4.Space_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.4.MLP_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.4.MLP_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.4.MLP_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.4.MLP_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.4.MLP_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.4.MLP_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.4.MLP_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.4.MLP_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.5.Space_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.5.Space_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.5.Space_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.5.Space_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.5.Space_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.5.Space_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.5.Space_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.5.Space_Adapter.D_fc2.bias
Adapter in name: transformer_encoder.layers.5.MLP_Adapter.D_fc1.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.5.MLP_Adapter.D_fc1.weight
Adapter in name: transformer_encoder.layers.5.MLP_Adapter.D_fc1.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.5.MLP_Adapter.D_fc1.bias
Adapter in name: transformer_encoder.layers.5.MLP_Adapter.D_fc2.weight
scGPT - INFO - Adapter in name: transformer_encoder.layers.5.MLP_Adapter.D_fc2.weight
Adapter in name: transformer_encoder.layers.5.MLP_Adapter.D_fc2.bias
scGPT - INFO - Adapter in name: transformer_encoder.layers.5.MLP_Adapter.D_fc2.bias
cls in name: cls_decoder._decoder.0.weight
scGPT - INFO - cls in name: cls_decoder._decoder.0.weight
cls in name: cls_decoder._decoder.0.bias
scGPT - INFO - cls in name: cls_decoder._decoder.0.bias
cls in name: cls_decoder._decoder.2.weight
scGPT - INFO - cls in name: cls_decoder._decoder.2.weight
cls in name: cls_decoder._decoder.2.bias
scGPT - INFO - cls in name: cls_decoder._decoder.2.bias
cls in name: cls_decoder._decoder.3.weight
scGPT - INFO - cls in name: cls_decoder._decoder.3.weight
cls in name: cls_decoder._decoder.3.bias
scGPT - INFO - cls in name: cls_decoder._decoder.3.bias
cls in name: cls_decoder._decoder.5.weight
scGPT - INFO - cls in name: cls_decoder._decoder.5.weight
cls in name: cls_decoder._decoder.5.bias
scGPT - INFO - cls in name: cls_decoder._decoder.5.bias
cls in name: cls_decoder.out_layer.weight
scGPT - INFO - cls in name: cls_decoder.out_layer.weight
cls in name: cls_decoder.out_layer.bias
scGPT - INFO - cls in name: cls_decoder.out_layer.bias
Total Post0 freeze Params 2639372
total:29650444
trainable:2639372
Total Post freeze Params 2639372
scGPT - INFO - Total Pre freeze Params 29650444
scGPT - INFO - Total Post freeze Params 2639372
Epoch:  1
Training model
scGPT - INFO - | epoch   1 | 100/3602 batches | lr 0.0001 | ms/batch 99.69 | loss 157.64 | mse 157.64 | mre  0.00 |
scGPT - INFO - | epoch   1 | 200/3602 batches | lr 0.0001 | ms/batch 90.46 | loss 22.69 | mse 22.69 | mre  0.00 |
scGPT - INFO - | epoch   1 | 300/3602 batches | lr 0.0001 | ms/batch 92.50 | loss 19.63 | mse 19.63 | mre  0.00 |
scGPT - INFO - | epoch   1 | 400/3602 batches | lr 0.0001 | ms/batch 95.72 | loss 20.72 | mse 20.72 | mre  0.00 |
scGPT - INFO - | epoch   1 | 500/3602 batches | lr 0.0001 | ms/batch 96.12 | loss 19.61 | mse 19.61 | mre  0.00 |
scGPT - INFO - | epoch   1 | 600/3602 batches | lr 0.0001 | ms/batch 96.12 | loss 20.17 | mse 20.17 | mre  0.00 |
scGPT - INFO - | epoch   1 | 700/3602 batches | lr 0.0001 | ms/batch 96.19 | loss 21.77 | mse 21.77 | mre  0.00 |
scGPT - INFO - | epoch   1 | 800/3602 batches | lr 0.0001 | ms/batch 96.05 | loss 21.63 | mse 21.63 | mre  0.00 |
scGPT - INFO - | epoch   1 | 900/3602 batches | lr 0.0001 | ms/batch 96.04 | loss 21.07 | mse 21.07 | mre  0.00 |
scGPT - INFO - | epoch   1 | 1000/3602 batches | lr 0.0001 | ms/batch 96.63 | loss 21.27 | mse 21.27 | mre  0.00 |
scGPT - INFO - | epoch   1 | 1100/3602 batches | lr 0.0001 | ms/batch 96.04 | loss 21.73 | mse 21.73 | mre  0.00 |
scGPT - INFO - | epoch   1 | 1200/3602 batches | lr 0.0001 | ms/batch 96.02 | loss 21.37 | mse 21.37 | mre  0.00 |
scGPT - INFO - | epoch   1 | 1300/3602 batches | lr 0.0001 | ms/batch 95.99 | loss 21.67 | mse 21.67 | mre  0.00 |
scGPT - INFO - | epoch   1 | 1400/3602 batches | lr 0.0001 | ms/batch 96.06 | loss 24.04 | mse 24.04 | mre  0.00 |
scGPT - INFO - | epoch   1 | 1500/3602 batches | lr 0.0001 | ms/batch 92.93 | loss 21.11 | mse 21.11 | mre  0.00 |
scGPT - INFO - | epoch   1 | 1600/3602 batches | lr 0.0001 | ms/batch 90.88 | loss 19.61 | mse 19.61 | mre  0.00 |
scGPT - INFO - | epoch   1 | 1700/3602 batches | lr 0.0001 | ms/batch 90.91 | loss 22.14 | mse 22.14 | mre  0.00 |
scGPT - INFO - | epoch   1 | 1800/3602 batches | lr 0.0001 | ms/batch 90.93 | loss 19.50 | mse 19.50 | mre  0.00 |
scGPT - INFO - | epoch   1 | 1900/3602 batches | lr 0.0001 | ms/batch 90.84 | loss 23.28 | mse 23.28 | mre  0.00 |
scGPT - INFO - | epoch   1 | 2000/3602 batches | lr 0.0001 | ms/batch 92.10 | loss 21.08 | mse 21.08 | mre  0.00 |
scGPT - INFO - | epoch   1 | 2100/3602 batches | lr 0.0001 | ms/batch 90.96 | loss 21.90 | mse 21.90 | mre  0.00 |
scGPT - INFO - | epoch   1 | 2200/3602 batches | lr 0.0001 | ms/batch 90.88 | loss 21.21 | mse 21.21 | mre  0.00 |
scGPT - INFO - | epoch   1 | 2300/3602 batches | lr 0.0001 | ms/batch 90.81 | loss 20.73 | mse 20.73 | mre  0.00 |
scGPT - INFO - | epoch   1 | 2400/3602 batches | lr 0.0001 | ms/batch 90.77 | loss 20.74 | mse 20.74 | mre  0.00 |
scGPT - INFO - | epoch   1 | 2500/3602 batches | lr 0.0001 | ms/batch 92.39 | loss 20.55 | mse 20.55 | mre  0.00 |
scGPT - INFO - | epoch   1 | 2600/3602 batches | lr 0.0001 | ms/batch 92.72 | loss 21.97 | mse 21.97 | mre  0.00 |
scGPT - INFO - | epoch   1 | 2700/3602 batches | lr 0.0001 | ms/batch 91.24 | loss 21.69 | mse 21.69 | mre  0.00 |
scGPT - INFO - | epoch   1 | 2800/3602 batches | lr 0.0001 | ms/batch 90.74 | loss 20.46 | mse 20.46 | mre  0.00 |
scGPT - INFO - | epoch   1 | 2900/3602 batches | lr 0.0001 | ms/batch 91.91 | loss 20.36 | mse 20.36 | mre  0.00 |
scGPT - INFO - | epoch   1 | 3000/3602 batches | lr 0.0001 | ms/batch 92.44 | loss 22.85 | mse 22.85 | mre  0.00 |
scGPT - INFO - | epoch   1 | 3100/3602 batches | lr 0.0001 | ms/batch 92.24 | loss 20.45 | mse 20.45 | mre  0.00 |
scGPT - INFO - | epoch   1 | 3200/3602 batches | lr 0.0001 | ms/batch 92.70 | loss 21.03 | mse 21.03 | mre  0.00 |
scGPT - INFO - | epoch   1 | 3300/3602 batches | lr 0.0001 | ms/batch 92.83 | loss 20.22 | mse 20.22 | mre  0.00 |
scGPT - INFO - | epoch   1 | 3400/3602 batches | lr 0.0001 | ms/batch 92.75 | loss 20.71 | mse 20.71 | mre  0.00 |
scGPT - INFO - | epoch   1 | 3500/3602 batches | lr 0.0001 | ms/batch 92.72 | loss 18.55 | mse 18.55 | mre  0.00 |
scGPT - INFO - | epoch   1 | 3600/3602 batches | lr 0.0001 | ms/batch 92.80 | loss 23.77 | mse 23.77 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 349.04s | valid loss/mse 20.3959 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 20.3959
epoch:  1
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[5.59  3.836 3.73  ... 3.729 3.89  3.77 ]
 [5.61  3.85  3.758 ... 3.738 3.896 3.773]
 [5.57  3.828 3.715 ... 3.72  3.889 3.77 ]
 ...
 [5.61  3.848 3.758 ... 3.736 3.893 3.768]
 [5.61  3.85  3.75  ... 3.738 3.9   3.773]
 [5.633 3.867 3.793 ... 3.752 3.926 3.791]]
(801, 11) (801, 11)
By feature:  MSE:  0.9273¬±0.0 MAE:  0.6982¬±0.0 R2:  -0.1895¬±0.0 PCC:  0.5764¬±0.0 Cosine Similarity:  0.9809¬±0.0
By sample:  MSE:  0.9273¬±0.0 MAE:  0.6982¬±0.0 R2:  -0.0145¬±0.0 PCC:  0.0829¬±0.0 Cosine Similarity:  0.9776¬±0.0
scGPT - INFO - By feature: MSE: 0.927299976348877¬±0.0 MAE: 0.698199987411499¬±0.0 R2: -0.1895¬±0.0 PCC: 0.5764¬±0.0 Cosine Similarity: 0.9809¬±0.0
scGPT - INFO - By sample: MSE: 0.927299976348877¬±0.0 MAE: 0.698199987411499¬±0.0 R2: -0.0145¬±0.0 PCC: 0.0829¬±0.0 Cosine Similarity: 0.9776¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  2
Training model
scGPT - INFO - | epoch   2 | 100/3602 batches | lr 0.0000 | ms/batch 92.01 | loss 21.95 | mse 21.95 | mre  0.00 |
scGPT - INFO - | epoch   2 | 200/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 21.97 | mse 21.97 | mre  0.00 |
scGPT - INFO - | epoch   2 | 300/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 19.00 | mse 19.00 | mre  0.00 |
scGPT - INFO - | epoch   2 | 400/3602 batches | lr 0.0000 | ms/batch 91.12 | loss 20.31 | mse 20.31 | mre  0.00 |
scGPT - INFO - | epoch   2 | 500/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 19.27 | mse 19.27 | mre  0.00 |
scGPT - INFO - | epoch   2 | 600/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 19.60 | mse 19.60 | mre  0.00 |
scGPT - INFO - | epoch   2 | 700/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 21.45 | mse 21.45 | mre  0.00 |
scGPT - INFO - | epoch   2 | 800/3602 batches | lr 0.0000 | ms/batch 91.91 | loss 21.36 | mse 21.36 | mre  0.00 |
scGPT - INFO - | epoch   2 | 900/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 20.71 | mse 20.71 | mre  0.00 |
scGPT - INFO - | epoch   2 | 1000/3602 batches | lr 0.0000 | ms/batch 90.56 | loss 20.80 | mse 20.80 | mre  0.00 |
scGPT - INFO - | epoch   2 | 1100/3602 batches | lr 0.0000 | ms/batch 90.53 | loss 21.42 | mse 21.42 | mre  0.00 |
scGPT - INFO - | epoch   2 | 1200/3602 batches | lr 0.0000 | ms/batch 90.56 | loss 21.35 | mse 21.35 | mre  0.00 |
scGPT - INFO - | epoch   2 | 1300/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 21.34 | mse 21.34 | mre  0.00 |
scGPT - INFO - | epoch   2 | 1400/3602 batches | lr 0.0000 | ms/batch 91.09 | loss 23.70 | mse 23.70 | mre  0.00 |
scGPT - INFO - | epoch   2 | 1500/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 20.84 | mse 20.84 | mre  0.00 |
scGPT - INFO - | epoch   2 | 1600/3602 batches | lr 0.0000 | ms/batch 90.56 | loss 19.49 | mse 19.49 | mre  0.00 |
scGPT - INFO - | epoch   2 | 1700/3602 batches | lr 0.0000 | ms/batch 90.56 | loss 21.86 | mse 21.86 | mre  0.00 |
scGPT - INFO - | epoch   2 | 1800/3602 batches | lr 0.0000 | ms/batch 90.53 | loss 19.22 | mse 19.22 | mre  0.00 |
scGPT - INFO - | epoch   2 | 1900/3602 batches | lr 0.0000 | ms/batch 90.59 | loss 22.97 | mse 22.97 | mre  0.00 |
scGPT - INFO - | epoch   2 | 2000/3602 batches | lr 0.0000 | ms/batch 90.56 | loss 20.94 | mse 20.94 | mre  0.00 |
scGPT - INFO - | epoch   2 | 2100/3602 batches | lr 0.0000 | ms/batch 90.58 | loss 21.68 | mse 21.68 | mre  0.00 |
scGPT - INFO - | epoch   2 | 2200/3602 batches | lr 0.0000 | ms/batch 91.15 | loss 21.12 | mse 21.12 | mre  0.00 |
scGPT - INFO - | epoch   2 | 2300/3602 batches | lr 0.0000 | ms/batch 92.72 | loss 20.50 | mse 20.50 | mre  0.00 |
scGPT - INFO - | epoch   2 | 2400/3602 batches | lr 0.0000 | ms/batch 93.10 | loss 20.66 | mse 20.66 | mre  0.00 |
scGPT - INFO - | epoch   2 | 2500/3602 batches | lr 0.0000 | ms/batch 92.59 | loss 20.12 | mse 20.12 | mre  0.00 |
scGPT - INFO - | epoch   2 | 2600/3602 batches | lr 0.0000 | ms/batch 92.59 | loss 21.86 | mse 21.86 | mre  0.00 |
scGPT - INFO - | epoch   2 | 2700/3602 batches | lr 0.0000 | ms/batch 92.56 | loss 21.37 | mse 21.37 | mre  0.00 |
scGPT - INFO - | epoch   2 | 2800/3602 batches | lr 0.0000 | ms/batch 92.62 | loss 20.38 | mse 20.38 | mre  0.00 |
scGPT - INFO - | epoch   2 | 2900/3602 batches | lr 0.0000 | ms/batch 92.63 | loss 20.21 | mse 20.21 | mre  0.00 |
scGPT - INFO - | epoch   2 | 3000/3602 batches | lr 0.0000 | ms/batch 92.54 | loss 22.52 | mse 22.52 | mre  0.00 |
scGPT - INFO - | epoch   2 | 3100/3602 batches | lr 0.0000 | ms/batch 92.64 | loss 20.15 | mse 20.15 | mre  0.00 |
scGPT - INFO - | epoch   2 | 3200/3602 batches | lr 0.0000 | ms/batch 92.53 | loss 20.94 | mse 20.94 | mre  0.00 |
scGPT - INFO - | epoch   2 | 3300/3602 batches | lr 0.0000 | ms/batch 92.56 | loss 20.11 | mse 20.11 | mre  0.00 |
scGPT - INFO - | epoch   2 | 3400/3602 batches | lr 0.0000 | ms/batch 93.67 | loss 20.49 | mse 20.49 | mre  0.00 |
scGPT - INFO - | epoch   2 | 3500/3602 batches | lr 0.0000 | ms/batch 92.61 | loss 18.50 | mse 18.50 | mre  0.00 |
scGPT - INFO - | epoch   2 | 3600/3602 batches | lr 0.0000 | ms/batch 92.58 | loss 23.35 | mse 23.35 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 342.67s | valid loss/mse 20.3077 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 20.3077
best_loss: 20.39594800195444, min_delta 0.0001, val_loss 20.307732212111894
Loss error: 0.08821578984254685
epoch:  2
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[5.54  3.828 3.76  ... 3.729 3.86  3.734]
 [5.566 3.854 3.781 ... 3.748 3.885 3.744]
 [5.535 3.818 3.754 ... 3.725 3.852 3.736]
 ...
 [5.555 3.848 3.773 ... 3.742 3.879 3.734]
 [5.562 3.852 3.777 ... 3.746 3.883 3.744]
 [5.61  3.898 3.812 ... 3.78  3.934 3.756]]
(801, 11) (801, 11)
By feature:  MSE:  0.9233¬±0.0 MAE:  0.6947¬±0.0 R2:  -0.1834¬±0.0 PCC:  0.5795¬±0.0 Cosine Similarity:  0.981¬±0.0
By sample:  MSE:  0.9233¬±0.0 MAE:  0.6947¬±0.0 R2:  -0.0085¬±0.0 PCC:  0.0923¬±0.0 Cosine Similarity:  0.9776¬±0.0
scGPT - INFO - By feature: MSE: 0.92330002784729¬±0.0 MAE: 0.6947000026702881¬±0.0 R2: -0.1834¬±0.0 PCC: 0.5795¬±0.0 Cosine Similarity: 0.981¬±0.0
scGPT - INFO - By sample: MSE: 0.92330002784729¬±0.0 MAE: 0.6947000026702881¬±0.0 R2: -0.0085¬±0.0 PCC: 0.0923¬±0.0 Cosine Similarity: 0.9776¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  3
Training model
scGPT - INFO - | epoch   3 | 100/3602 batches | lr 0.0000 | ms/batch 94.13 | loss 21.71 | mse 21.71 | mre  0.00 |
scGPT - INFO - | epoch   3 | 200/3602 batches | lr 0.0000 | ms/batch 92.82 | loss 21.81 | mse 21.81 | mre  0.00 |
scGPT - INFO - | epoch   3 | 300/3602 batches | lr 0.0000 | ms/batch 92.77 | loss 18.88 | mse 18.88 | mre  0.00 |
scGPT - INFO - | epoch   3 | 400/3602 batches | lr 0.0000 | ms/batch 92.74 | loss 20.15 | mse 20.15 | mre  0.00 |
scGPT - INFO - | epoch   3 | 500/3602 batches | lr 0.0000 | ms/batch 92.67 | loss 19.05 | mse 19.05 | mre  0.00 |
scGPT - INFO - | epoch   3 | 600/3602 batches | lr 0.0000 | ms/batch 92.60 | loss 19.34 | mse 19.34 | mre  0.00 |
scGPT - INFO - | epoch   3 | 700/3602 batches | lr 0.0000 | ms/batch 92.60 | loss 21.22 | mse 21.22 | mre  0.00 |
scGPT - INFO - | epoch   3 | 800/3602 batches | lr 0.0000 | ms/batch 93.65 | loss 21.39 | mse 21.39 | mre  0.00 |
scGPT - INFO - | epoch   3 | 900/3602 batches | lr 0.0000 | ms/batch 92.65 | loss 20.65 | mse 20.65 | mre  0.00 |
scGPT - INFO - | epoch   3 | 1000/3602 batches | lr 0.0000 | ms/batch 92.72 | loss 20.74 | mse 20.74 | mre  0.00 |
scGPT - INFO - | epoch   3 | 1100/3602 batches | lr 0.0000 | ms/batch 92.68 | loss 21.12 | mse 21.12 | mre  0.00 |
scGPT - INFO - | epoch   3 | 1200/3602 batches | lr 0.0000 | ms/batch 92.62 | loss 21.09 | mse 21.09 | mre  0.00 |
scGPT - INFO - | epoch   3 | 1300/3602 batches | lr 0.0000 | ms/batch 92.70 | loss 21.28 | mse 21.28 | mre  0.00 |
scGPT - INFO - | epoch   3 | 1400/3602 batches | lr 0.0000 | ms/batch 92.68 | loss 23.53 | mse 23.53 | mre  0.00 |
scGPT - INFO - | epoch   3 | 1500/3602 batches | lr 0.0000 | ms/batch 92.65 | loss 20.71 | mse 20.71 | mre  0.00 |
scGPT - INFO - | epoch   3 | 1600/3602 batches | lr 0.0000 | ms/batch 92.60 | loss 19.27 | mse 19.27 | mre  0.00 |
scGPT - INFO - | epoch   3 | 1700/3602 batches | lr 0.0000 | ms/batch 92.77 | loss 21.79 | mse 21.79 | mre  0.00 |
scGPT - INFO - | epoch   3 | 1800/3602 batches | lr 0.0000 | ms/batch 93.69 | loss 19.18 | mse 19.18 | mre  0.00 |
scGPT - INFO - | epoch   3 | 1900/3602 batches | lr 0.0000 | ms/batch 92.69 | loss 22.85 | mse 22.85 | mre  0.00 |
scGPT - INFO - | epoch   3 | 2000/3602 batches | lr 0.0000 | ms/batch 92.68 | loss 20.89 | mse 20.89 | mre  0.00 |
scGPT - INFO - | epoch   3 | 2100/3602 batches | lr 0.0000 | ms/batch 92.76 | loss 21.57 | mse 21.57 | mre  0.00 |
scGPT - INFO - | epoch   3 | 2200/3602 batches | lr 0.0000 | ms/batch 92.61 | loss 20.89 | mse 20.89 | mre  0.00 |
scGPT - INFO - | epoch   3 | 2300/3602 batches | lr 0.0000 | ms/batch 92.66 | loss 20.37 | mse 20.37 | mre  0.00 |
scGPT - INFO - | epoch   3 | 2400/3602 batches | lr 0.0000 | ms/batch 92.64 | loss 20.56 | mse 20.56 | mre  0.00 |
scGPT - INFO - | epoch   3 | 2500/3602 batches | lr 0.0000 | ms/batch 92.53 | loss 20.12 | mse 20.12 | mre  0.00 |
scGPT - INFO - | epoch   3 | 2600/3602 batches | lr 0.0000 | ms/batch 92.48 | loss 21.68 | mse 21.68 | mre  0.00 |
scGPT - INFO - | epoch   3 | 2700/3602 batches | lr 0.0000 | ms/batch 92.52 | loss 21.26 | mse 21.26 | mre  0.00 |
scGPT - INFO - | epoch   3 | 2800/3602 batches | lr 0.0000 | ms/batch 93.58 | loss 20.26 | mse 20.26 | mre  0.00 |
scGPT - INFO - | epoch   3 | 2900/3602 batches | lr 0.0000 | ms/batch 92.49 | loss 20.12 | mse 20.12 | mre  0.00 |
scGPT - INFO - | epoch   3 | 3000/3602 batches | lr 0.0000 | ms/batch 92.48 | loss 22.43 | mse 22.43 | mre  0.00 |
scGPT - INFO - | epoch   3 | 3100/3602 batches | lr 0.0000 | ms/batch 92.46 | loss 19.91 | mse 19.91 | mre  0.00 |
scGPT - INFO - | epoch   3 | 3200/3602 batches | lr 0.0000 | ms/batch 92.47 | loss 20.84 | mse 20.84 | mre  0.00 |
scGPT - INFO - | epoch   3 | 3300/3602 batches | lr 0.0000 | ms/batch 92.48 | loss 20.01 | mse 20.01 | mre  0.00 |
scGPT - INFO - | epoch   3 | 3400/3602 batches | lr 0.0000 | ms/batch 92.50 | loss 20.32 | mse 20.32 | mre  0.00 |
scGPT - INFO - | epoch   3 | 3500/3602 batches | lr 0.0000 | ms/batch 92.52 | loss 18.45 | mse 18.45 | mre  0.00 |
scGPT - INFO - | epoch   3 | 3600/3602 batches | lr 0.0000 | ms/batch 92.52 | loss 23.12 | mse 23.12 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 346.39s | valid loss/mse 20.2683 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 20.2683
best_loss: 20.307732212111894, min_delta 0.0001, val_loss 20.268335364135762
Loss error: 0.039396847976131966
epoch:  3
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[5.508 3.855 3.756 ... 3.729 3.857 3.713]
 [5.53  3.867 3.787 ... 3.754 3.877 3.732]
 [5.508 3.857 3.75  ... 3.727 3.857 3.713]
 ...
 [5.516 3.855 3.773 ... 3.742 3.865 3.719]
 [5.535 3.87  3.783 ... 3.754 3.879 3.732]
 [5.574 3.896 3.83  ... 3.795 3.914 3.758]]
(801, 11) (801, 11)
By feature:  MSE:  0.9215¬±0.0 MAE:  0.6927¬±0.0 R2:  -0.1809¬±0.0 PCC:  0.5809¬±0.0 Cosine Similarity:  0.981¬±0.0
By sample:  MSE:  0.9215¬±0.0 MAE:  0.6927¬±0.0 R2:  -0.0065¬±0.0 PCC:  0.0978¬±0.0 Cosine Similarity:  0.9776¬±0.0
scGPT - INFO - By feature: MSE: 0.921500027179718¬±0.0 MAE: 0.6927000284194946¬±0.0 R2: -0.1809¬±0.0 PCC: 0.5809¬±0.0 Cosine Similarity: 0.981¬±0.0
scGPT - INFO - By sample: MSE: 0.921500027179718¬±0.0 MAE: 0.6927000284194946¬±0.0 R2: -0.0065¬±0.0 PCC: 0.0978¬±0.0 Cosine Similarity: 0.9776¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  4
Training model
scGPT - INFO - | epoch   4 | 100/3602 batches | lr 0.0000 | ms/batch 93.77 | loss 21.72 | mse 21.72 | mre  0.00 |
scGPT - INFO - | epoch   4 | 200/3602 batches | lr 0.0000 | ms/batch 93.56 | loss 21.62 | mse 21.62 | mre  0.00 |
scGPT - INFO - | epoch   4 | 300/3602 batches | lr 0.0000 | ms/batch 92.57 | loss 18.76 | mse 18.76 | mre  0.00 |
scGPT - INFO - | epoch   4 | 400/3602 batches | lr 0.0000 | ms/batch 92.57 | loss 19.99 | mse 19.99 | mre  0.00 |
scGPT - INFO - | epoch   4 | 500/3602 batches | lr 0.0000 | ms/batch 92.60 | loss 19.01 | mse 19.01 | mre  0.00 |
scGPT - INFO - | epoch   4 | 600/3602 batches | lr 0.0000 | ms/batch 92.56 | loss 19.25 | mse 19.25 | mre  0.00 |
scGPT - INFO - | epoch   4 | 700/3602 batches | lr 0.0000 | ms/batch 92.55 | loss 21.10 | mse 21.10 | mre  0.00 |
scGPT - INFO - | epoch   4 | 800/3602 batches | lr 0.0000 | ms/batch 92.81 | loss 21.27 | mse 21.27 | mre  0.00 |
scGPT - INFO - | epoch   4 | 900/3602 batches | lr 0.0000 | ms/batch 92.58 | loss 20.50 | mse 20.50 | mre  0.00 |
scGPT - INFO - | epoch   4 | 1000/3602 batches | lr 0.0000 | ms/batch 92.58 | loss 20.79 | mse 20.79 | mre  0.00 |
scGPT - INFO - | epoch   4 | 1100/3602 batches | lr 0.0000 | ms/batch 92.53 | loss 21.15 | mse 21.15 | mre  0.00 |
scGPT - INFO - | epoch   4 | 1200/3602 batches | lr 0.0000 | ms/batch 93.57 | loss 21.03 | mse 21.03 | mre  0.00 |
scGPT - INFO - | epoch   4 | 1300/3602 batches | lr 0.0000 | ms/batch 92.52 | loss 21.27 | mse 21.27 | mre  0.00 |
scGPT - INFO - | epoch   4 | 1400/3602 batches | lr 0.0000 | ms/batch 92.55 | loss 23.29 | mse 23.29 | mre  0.00 |
scGPT - INFO - | epoch   4 | 1500/3602 batches | lr 0.0000 | ms/batch 92.57 | loss 20.59 | mse 20.59 | mre  0.00 |
scGPT - INFO - | epoch   4 | 1600/3602 batches | lr 0.0000 | ms/batch 92.26 | loss 19.22 | mse 19.22 | mre  0.00 |
scGPT - INFO - | epoch   4 | 1700/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 21.63 | mse 21.63 | mre  0.00 |
scGPT - INFO - | epoch   4 | 1800/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 19.03 | mse 19.03 | mre  0.00 |
scGPT - INFO - | epoch   4 | 1900/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 22.75 | mse 22.75 | mre  0.00 |
scGPT - INFO - | epoch   4 | 2000/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 20.72 | mse 20.72 | mre  0.00 |
scGPT - INFO - | epoch   4 | 2100/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 21.49 | mse 21.49 | mre  0.00 |
scGPT - INFO - | epoch   4 | 2200/3602 batches | lr 0.0000 | ms/batch 91.15 | loss 20.74 | mse 20.74 | mre  0.00 |
scGPT - INFO - | epoch   4 | 2300/3602 batches | lr 0.0000 | ms/batch 90.62 | loss 20.34 | mse 20.34 | mre  0.00 |
scGPT - INFO - | epoch   4 | 2400/3602 batches | lr 0.0000 | ms/batch 91.08 | loss 20.46 | mse 20.46 | mre  0.00 |
scGPT - INFO - | epoch   4 | 2500/3602 batches | lr 0.0000 | ms/batch 92.69 | loss 20.08 | mse 20.08 | mre  0.00 |
scGPT - INFO - | epoch   4 | 2600/3602 batches | lr 0.0000 | ms/batch 92.72 | loss 21.53 | mse 21.53 | mre  0.00 |
scGPT - INFO - | epoch   4 | 2700/3602 batches | lr 0.0000 | ms/batch 92.65 | loss 21.20 | mse 21.20 | mre  0.00 |
scGPT - INFO - | epoch   4 | 2800/3602 batches | lr 0.0000 | ms/batch 92.83 | loss 20.14 | mse 20.14 | mre  0.00 |
scGPT - INFO - | epoch   4 | 2900/3602 batches | lr 0.0000 | ms/batch 92.81 | loss 20.06 | mse 20.06 | mre  0.00 |
scGPT - INFO - | epoch   4 | 3000/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 22.21 | mse 22.21 | mre  0.00 |
scGPT - INFO - | epoch   4 | 3100/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 19.84 | mse 19.84 | mre  0.00 |
scGPT - INFO - | epoch   4 | 3200/3602 batches | lr 0.0000 | ms/batch 91.24 | loss 20.78 | mse 20.78 | mre  0.00 |
scGPT - INFO - | epoch   4 | 3300/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 19.83 | mse 19.83 | mre  0.00 |
scGPT - INFO - | epoch   4 | 3400/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 20.26 | mse 20.26 | mre  0.00 |
scGPT - INFO - | epoch   4 | 3500/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 18.40 | mse 18.40 | mre  0.00 |
scGPT - INFO - | epoch   4 | 3600/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 23.05 | mse 23.05 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 343.47s | valid loss/mse 20.1374 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 20.1374
best_loss: 20.268335364135762, min_delta 0.0001, val_loss 20.13738931252269
Loss error: 0.1309460516130727
epoch:  4
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[5.46  3.852 3.748 ... 3.71  3.828 3.7  ]
 [5.516 3.898 3.8   ... 3.762 3.871 3.73 ]
 [5.46  3.852 3.746 ... 3.715 3.828 3.701]
 ...
 [5.477 3.865 3.768 ... 3.73  3.844 3.703]
 [5.527 3.906 3.805 ... 3.768 3.877 3.736]
 [5.598 3.977 3.877 ... 3.834 3.943 3.77 ]]
(801, 11) (801, 11)
By feature:  MSE:  0.9156¬±0.0 MAE:  0.6926¬±0.0 R2:  -0.1743¬±0.0 PCC:  0.583¬±0.0 Cosine Similarity:  0.9811¬±0.0
By sample:  MSE:  0.9156¬±0.0 MAE:  0.6926¬±0.0 R2:  0.0007¬±0.0 PCC:  0.1071¬±0.0 Cosine Similarity:  0.9777¬±0.0
scGPT - INFO - By feature: MSE: 0.9156000018119812¬±0.0 MAE: 0.6926000118255615¬±0.0 R2: -0.1743¬±0.0 PCC: 0.583¬±0.0 Cosine Similarity: 0.9811¬±0.0
scGPT - INFO - By sample: MSE: 0.9156000018119812¬±0.0 MAE: 0.6926000118255615¬±0.0 R2: 0.0007¬±0.0 PCC: 0.1071¬±0.0 Cosine Similarity: 0.9777¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  5
Training model
scGPT - INFO - | epoch   5 | 100/3602 batches | lr 0.0000 | ms/batch 91.93 | loss 21.62 | mse 21.62 | mre  0.00 |
scGPT - INFO - | epoch   5 | 200/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 21.55 | mse 21.55 | mre  0.00 |
scGPT - INFO - | epoch   5 | 300/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 18.76 | mse 18.76 | mre  0.00 |
scGPT - INFO - | epoch   5 | 400/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 19.86 | mse 19.86 | mre  0.00 |
scGPT - INFO - | epoch   5 | 500/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 18.91 | mse 18.91 | mre  0.00 |
scGPT - INFO - | epoch   5 | 600/3602 batches | lr 0.0000 | ms/batch 91.27 | loss 19.11 | mse 19.11 | mre  0.00 |
scGPT - INFO - | epoch   5 | 700/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 20.91 | mse 20.91 | mre  0.00 |
scGPT - INFO - | epoch   5 | 800/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 21.19 | mse 21.19 | mre  0.00 |
scGPT - INFO - | epoch   5 | 900/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 20.40 | mse 20.40 | mre  0.00 |
scGPT - INFO - | epoch   5 | 1000/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 20.73 | mse 20.73 | mre  0.00 |
scGPT - INFO - | epoch   5 | 1100/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 20.98 | mse 20.98 | mre  0.00 |
scGPT - INFO - | epoch   5 | 1200/3602 batches | lr 0.0000 | ms/batch 92.29 | loss 20.92 | mse 20.92 | mre  0.00 |
scGPT - INFO - | epoch   5 | 1300/3602 batches | lr 0.0000 | ms/batch 92.89 | loss 21.15 | mse 21.15 | mre  0.00 |
scGPT - INFO - | epoch   5 | 1400/3602 batches | lr 0.0000 | ms/batch 92.86 | loss 23.04 | mse 23.04 | mre  0.00 |
scGPT - INFO - | epoch   5 | 1500/3602 batches | lr 0.0000 | ms/batch 91.99 | loss 20.42 | mse 20.42 | mre  0.00 |
scGPT - INFO - | epoch   5 | 1600/3602 batches | lr 0.0000 | ms/batch 91.21 | loss 19.09 | mse 19.09 | mre  0.00 |
scGPT - INFO - | epoch   5 | 1700/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 21.36 | mse 21.36 | mre  0.00 |
scGPT - INFO - | epoch   5 | 1800/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 18.85 | mse 18.85 | mre  0.00 |
scGPT - INFO - | epoch   5 | 1900/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 22.50 | mse 22.50 | mre  0.00 |
scGPT - INFO - | epoch   5 | 2000/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 20.74 | mse 20.74 | mre  0.00 |
scGPT - INFO - | epoch   5 | 2100/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 21.17 | mse 21.17 | mre  0.00 |
scGPT - INFO - | epoch   5 | 2200/3602 batches | lr 0.0000 | ms/batch 90.60 | loss 20.39 | mse 20.39 | mre  0.00 |
scGPT - INFO - | epoch   5 | 2300/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 20.28 | mse 20.28 | mre  0.00 |
scGPT - INFO - | epoch   5 | 2400/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 20.28 | mse 20.28 | mre  0.00 |
scGPT - INFO - | epoch   5 | 2500/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 20.07 | mse 20.07 | mre  0.00 |
scGPT - INFO - | epoch   5 | 2600/3602 batches | lr 0.0000 | ms/batch 91.21 | loss 21.44 | mse 21.44 | mre  0.00 |
scGPT - INFO - | epoch   5 | 2700/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 21.18 | mse 21.18 | mre  0.00 |
scGPT - INFO - | epoch   5 | 2800/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 19.93 | mse 19.93 | mre  0.00 |
scGPT - INFO - | epoch   5 | 2900/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 20.01 | mse 20.01 | mre  0.00 |
scGPT - INFO - | epoch   5 | 3000/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 21.75 | mse 21.75 | mre  0.00 |
scGPT - INFO - | epoch   5 | 3100/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 19.71 | mse 19.71 | mre  0.00 |
scGPT - INFO - | epoch   5 | 3200/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 20.64 | mse 20.64 | mre  0.00 |
scGPT - INFO - | epoch   5 | 3300/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 19.63 | mse 19.63 | mre  0.00 |
scGPT - INFO - | epoch   5 | 3400/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 20.16 | mse 20.16 | mre  0.00 |
scGPT - INFO - | epoch   5 | 3500/3602 batches | lr 0.0000 | ms/batch 90.62 | loss 18.28 | mse 18.28 | mre  0.00 |
scGPT - INFO - | epoch   5 | 3600/3602 batches | lr 0.0000 | ms/batch 91.15 | loss 22.84 | mse 22.84 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 340.07s | valid loss/mse 19.9506 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 19.9506
best_loss: 20.13738931252269, min_delta 0.0001, val_loss 19.950564563051145
Loss error: 0.18682474947154404
epoch:  5
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[5.457 3.842 3.715 ... 3.691 3.818 3.705]
 [5.484 3.883 3.783 ... 3.742 3.842 3.717]
 [5.473 3.857 3.732 ... 3.707 3.822 3.71 ]
 ...
 [5.473 3.857 3.742 ... 3.707 3.82  3.707]
 [5.52  3.943 3.836 ... 3.809 3.883 3.734]
 [5.504 3.945 3.832 ... 3.807 3.89  3.717]]
(801, 11) (801, 11)
By feature:  MSE:  0.9071¬±0.0 MAE:  0.6889¬±0.0 R2:  -0.1659¬±0.0 PCC:  0.5868¬±0.0 Cosine Similarity:  0.9813¬±0.0
By sample:  MSE:  0.9071¬±0.0 MAE:  0.6889¬±0.0 R2:  0.0096¬±0.0 PCC:  0.1249¬±0.0 Cosine Similarity:  0.9778¬±0.0
scGPT - INFO - By feature: MSE: 0.9071000218391418¬±0.0 MAE: 0.6888999938964844¬±0.0 R2: -0.1659¬±0.0 PCC: 0.5868¬±0.0 Cosine Similarity: 0.9813¬±0.0
scGPT - INFO - By sample: MSE: 0.9071000218391418¬±0.0 MAE: 0.6888999938964844¬±0.0 R2: 0.0096¬±0.0 PCC: 0.1249¬±0.0 Cosine Similarity: 0.9778¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  6
Training model
scGPT - INFO - | epoch   6 | 100/3602 batches | lr 0.0000 | ms/batch 91.93 | loss 21.22 | mse 21.22 | mre  0.00 |
scGPT - INFO - | epoch   6 | 200/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 21.42 | mse 21.42 | mre  0.00 |
scGPT - INFO - | epoch   6 | 300/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 18.67 | mse 18.67 | mre  0.00 |
scGPT - INFO - | epoch   6 | 400/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 19.71 | mse 19.71 | mre  0.00 |
scGPT - INFO - | epoch   6 | 500/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 18.74 | mse 18.74 | mre  0.00 |
scGPT - INFO - | epoch   6 | 600/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 18.95 | mse 18.95 | mre  0.00 |
scGPT - INFO - | epoch   6 | 700/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 20.61 | mse 20.61 | mre  0.00 |
scGPT - INFO - | epoch   6 | 800/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 21.14 | mse 21.14 | mre  0.00 |
scGPT - INFO - | epoch   6 | 900/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 20.07 | mse 20.07 | mre  0.00 |
scGPT - INFO - | epoch   6 | 1000/3602 batches | lr 0.0000 | ms/batch 91.28 | loss 20.56 | mse 20.56 | mre  0.00 |
scGPT - INFO - | epoch   6 | 1100/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 20.36 | mse 20.36 | mre  0.00 |
scGPT - INFO - | epoch   6 | 1200/3602 batches | lr 0.0000 | ms/batch 90.97 | loss 20.29 | mse 20.29 | mre  0.00 |
scGPT - INFO - | epoch   6 | 1300/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 20.52 | mse 20.52 | mre  0.00 |
scGPT - INFO - | epoch   6 | 1400/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 22.15 | mse 22.15 | mre  0.00 |
scGPT - INFO - | epoch   6 | 1500/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 19.70 | mse 19.70 | mre  0.00 |
scGPT - INFO - | epoch   6 | 1600/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 17.30 | mse 17.30 | mre  0.00 |
scGPT - INFO - | epoch   6 | 1700/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 19.36 | mse 19.36 | mre  0.00 |
scGPT - INFO - | epoch   6 | 1800/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 16.10 | mse 16.10 | mre  0.00 |
scGPT - INFO - | epoch   6 | 1900/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 20.62 | mse 20.62 | mre  0.00 |
scGPT - INFO - | epoch   6 | 2000/3602 batches | lr 0.0000 | ms/batch 91.15 | loss 17.98 | mse 17.98 | mre  0.00 |
scGPT - INFO - | epoch   6 | 2100/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 18.75 | mse 18.75 | mre  0.00 |
scGPT - INFO - | epoch   6 | 2200/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 17.52 | mse 17.52 | mre  0.00 |
scGPT - INFO - | epoch   6 | 2300/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 17.82 | mse 17.82 | mre  0.00 |
scGPT - INFO - | epoch   6 | 2400/3602 batches | lr 0.0000 | ms/batch 91.25 | loss 17.30 | mse 17.30 | mre  0.00 |
scGPT - INFO - | epoch   6 | 2500/3602 batches | lr 0.0000 | ms/batch 92.69 | loss 17.02 | mse 17.02 | mre  0.00 |
scGPT - INFO - | epoch   6 | 2600/3602 batches | lr 0.0000 | ms/batch 91.74 | loss 19.76 | mse 19.76 | mre  0.00 |
scGPT - INFO - | epoch   6 | 2700/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 18.35 | mse 18.35 | mre  0.00 |
scGPT - INFO - | epoch   6 | 2800/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 17.12 | mse 17.12 | mre  0.00 |
scGPT - INFO - | epoch   6 | 2900/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 17.47 | mse 17.47 | mre  0.00 |
scGPT - INFO - | epoch   6 | 3000/3602 batches | lr 0.0000 | ms/batch 91.38 | loss 18.70 | mse 18.70 | mre  0.00 |
scGPT - INFO - | epoch   6 | 3100/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 16.71 | mse 16.71 | mre  0.00 |
scGPT - INFO - | epoch   6 | 3200/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 17.38 | mse 17.38 | mre  0.00 |
scGPT - INFO - | epoch   6 | 3300/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 16.57 | mse 16.57 | mre  0.00 |
scGPT - INFO - | epoch   6 | 3400/3602 batches | lr 0.0000 | ms/batch 90.93 | loss 16.99 | mse 16.99 | mre  0.00 |
scGPT - INFO - | epoch   6 | 3500/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 15.08 | mse 15.08 | mre  0.00 |
scGPT - INFO - | epoch   6 | 3600/3602 batches | lr 0.0000 | ms/batch 90.97 | loss 19.78 | mse 19.78 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 339.83s | valid loss/mse 17.0646 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 17.0646
best_loss: 19.950564563051145, min_delta 0.0001, val_loss 17.06464008073533
Loss error: 2.885924482315815
epoch:  6
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.434 3.77  3.453 ... 3.566 3.746 3.852]
 [6.33  3.777 3.479 ... 3.572 3.736 3.834]
 [4.453 4.11  4.168 ... 4.01  3.904 3.557]
 ...
 [4.47  4.15  4.273 ... 4.117 3.965 3.627]
 [4.523 4.17  4.29  ... 4.15  3.986 3.654]
 [6.305 3.787 3.496 ... 3.57  3.738 3.83 ]]
(801, 11) (801, 11)
By feature:  MSE:  0.7757¬±0.0 MAE:  0.6095¬±0.0 R2:  -0.0526¬±0.0 PCC:  0.6668¬±0.0 Cosine Similarity:  0.9848¬±0.0
By sample:  MSE:  0.7757¬±0.0 MAE:  0.6095¬±0.0 R2:  0.0951¬±0.0 PCC:  0.2942¬±0.0 Cosine Similarity:  0.9811¬±0.0
scGPT - INFO - By feature: MSE: 0.7756999731063843¬±0.0 MAE: 0.609499990940094¬±0.0 R2: -0.0526¬±0.0 PCC: 0.6668¬±0.0 Cosine Similarity: 0.9848¬±0.0
scGPT - INFO - By sample: MSE: 0.7756999731063843¬±0.0 MAE: 0.609499990940094¬±0.0 R2: 0.0951¬±0.0 PCC: 0.2942¬±0.0 Cosine Similarity: 0.9811¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  7
Training model
scGPT - INFO - | epoch   7 | 100/3602 batches | lr 0.0000 | ms/batch 92.14 | loss 17.10 | mse 17.10 | mre  0.00 |
scGPT - INFO - | epoch   7 | 200/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 18.23 | mse 18.23 | mre  0.00 |
scGPT - INFO - | epoch   7 | 300/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 14.43 | mse 14.43 | mre  0.00 |
scGPT - INFO - | epoch   7 | 400/3602 batches | lr 0.0000 | ms/batch 93.22 | loss 15.87 | mse 15.87 | mre  0.00 |
scGPT - INFO - | epoch   7 | 500/3602 batches | lr 0.0000 | ms/batch 92.74 | loss 15.63 | mse 15.63 | mre  0.00 |
scGPT - INFO - | epoch   7 | 600/3602 batches | lr 0.0000 | ms/batch 92.70 | loss 15.69 | mse 15.69 | mre  0.00 |
scGPT - INFO - | epoch   7 | 700/3602 batches | lr 0.0000 | ms/batch 92.82 | loss 17.34 | mse 17.34 | mre  0.00 |
scGPT - INFO - | epoch   7 | 800/3602 batches | lr 0.0000 | ms/batch 92.75 | loss 18.37 | mse 18.37 | mre  0.00 |
scGPT - INFO - | epoch   7 | 900/3602 batches | lr 0.0000 | ms/batch 92.75 | loss 16.49 | mse 16.49 | mre  0.00 |
scGPT - INFO - | epoch   7 | 1000/3602 batches | lr 0.0000 | ms/batch 92.73 | loss 17.97 | mse 17.97 | mre  0.00 |
scGPT - INFO - | epoch   7 | 1100/3602 batches | lr 0.0000 | ms/batch 92.75 | loss 17.08 | mse 17.08 | mre  0.00 |
scGPT - INFO - | epoch   7 | 1200/3602 batches | lr 0.0000 | ms/batch 92.78 | loss 16.75 | mse 16.75 | mre  0.00 |
scGPT - INFO - | epoch   7 | 1300/3602 batches | lr 0.0000 | ms/batch 92.73 | loss 18.27 | mse 18.27 | mre  0.00 |
scGPT - INFO - | epoch   7 | 1400/3602 batches | lr 0.0000 | ms/batch 93.25 | loss 19.19 | mse 19.19 | mre  0.00 |
scGPT - INFO - | epoch   7 | 1500/3602 batches | lr 0.0000 | ms/batch 92.75 | loss 17.64 | mse 17.64 | mre  0.00 |
scGPT - INFO - | epoch   7 | 1600/3602 batches | lr 0.0000 | ms/batch 92.83 | loss 14.76 | mse 14.76 | mre  0.00 |
scGPT - INFO - | epoch   7 | 1700/3602 batches | lr 0.0000 | ms/batch 92.79 | loss 17.23 | mse 17.23 | mre  0.00 |
scGPT - INFO - | epoch   7 | 1800/3602 batches | lr 0.0000 | ms/batch 92.84 | loss 15.10 | mse 15.10 | mre  0.00 |
scGPT - INFO - | epoch   7 | 1900/3602 batches | lr 0.0000 | ms/batch 92.75 | loss 19.66 | mse 19.66 | mre  0.00 |
scGPT - INFO - | epoch   7 | 2000/3602 batches | lr 0.0000 | ms/batch 92.73 | loss 17.38 | mse 17.38 | mre  0.00 |
scGPT - INFO - | epoch   7 | 2100/3602 batches | lr 0.0000 | ms/batch 92.73 | loss 17.95 | mse 17.95 | mre  0.00 |
scGPT - INFO - | epoch   7 | 2200/3602 batches | lr 0.0000 | ms/batch 92.71 | loss 16.02 | mse 16.02 | mre  0.00 |
scGPT - INFO - | epoch   7 | 2300/3602 batches | lr 0.0000 | ms/batch 92.75 | loss 17.09 | mse 17.09 | mre  0.00 |
scGPT - INFO - | epoch   7 | 2400/3602 batches | lr 0.0000 | ms/batch 93.21 | loss 16.62 | mse 16.62 | mre  0.00 |
scGPT - INFO - | epoch   7 | 2500/3602 batches | lr 0.0000 | ms/batch 90.51 | loss 16.26 | mse 16.26 | mre  0.00 |
scGPT - INFO - | epoch   7 | 2600/3602 batches | lr 0.0000 | ms/batch 90.56 | loss 18.52 | mse 18.52 | mre  0.00 |
scGPT - INFO - | epoch   7 | 2700/3602 batches | lr 0.0000 | ms/batch 90.57 | loss 17.47 | mse 17.47 | mre  0.00 |
scGPT - INFO - | epoch   7 | 2800/3602 batches | lr 0.0000 | ms/batch 90.56 | loss 16.27 | mse 16.27 | mre  0.00 |
scGPT - INFO - | epoch   7 | 2900/3602 batches | lr 0.0000 | ms/batch 90.59 | loss 16.49 | mse 16.49 | mre  0.00 |
scGPT - INFO - | epoch   7 | 3000/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 17.52 | mse 17.52 | mre  0.00 |
scGPT - INFO - | epoch   7 | 3100/3602 batches | lr 0.0000 | ms/batch 90.60 | loss 16.46 | mse 16.46 | mre  0.00 |
scGPT - INFO - | epoch   7 | 3200/3602 batches | lr 0.0000 | ms/batch 90.59 | loss 16.70 | mse 16.70 | mre  0.00 |
scGPT - INFO - | epoch   7 | 3300/3602 batches | lr 0.0000 | ms/batch 90.56 | loss 15.95 | mse 15.95 | mre  0.00 |
scGPT - INFO - | epoch   7 | 3400/3602 batches | lr 0.0000 | ms/batch 91.16 | loss 16.51 | mse 16.51 | mre  0.00 |
scGPT - INFO - | epoch   7 | 3500/3602 batches | lr 0.0000 | ms/batch 90.61 | loss 15.12 | mse 15.12 | mre  0.00 |
scGPT - INFO - | epoch   7 | 3600/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 19.02 | mse 19.02 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 343.58s | valid loss/mse 16.5882 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 16.5882
best_loss: 17.06464008073533, min_delta 0.0001, val_loss 16.58820451749547
Loss error: 0.47643556323986047
epoch:  7
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.37  3.738 3.465 ... 3.557 3.736 3.852]
 [6.414 3.746 3.465 ... 3.572 3.744 3.86 ]
 [4.5   4.04  4.11  ... 4.008 3.883 3.543]
 ...
 [4.55  3.994 4.15  ... 4.098 3.91  3.594]
 [4.613 4.05  4.207 ... 4.152 3.967 3.637]
 [6.383 3.768 3.486 ... 3.582 3.754 3.857]]
(801, 11) (801, 11)
By feature:  MSE:  0.754¬±0.0 MAE:  0.595¬±0.0 R2:  -0.0224¬±0.0 PCC:  0.6796¬±0.0 Cosine Similarity:  0.9854¬±0.0
By sample:  MSE:  0.754¬±0.0 MAE:  0.595¬±0.0 R2:  0.1128¬±0.0 PCC:  0.3274¬±0.0 Cosine Similarity:  0.9817¬±0.0
scGPT - INFO - By feature: MSE: 0.7540000081062317¬±0.0 MAE: 0.5950000286102295¬±0.0 R2: -0.0224¬±0.0 PCC: 0.6796¬±0.0 Cosine Similarity: 0.9854¬±0.0
scGPT - INFO - By sample: MSE: 0.7540000081062317¬±0.0 MAE: 0.5950000286102295¬±0.0 R2: 0.1128¬±0.0 PCC: 0.3274¬±0.0 Cosine Similarity: 0.9817¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  8
Training model
scGPT - INFO - | epoch   8 | 100/3602 batches | lr 0.0000 | ms/batch 93.26 | loss 16.63 | mse 16.63 | mre  0.00 |
scGPT - INFO - | epoch   8 | 200/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 17.74 | mse 17.74 | mre  0.00 |
scGPT - INFO - | epoch   8 | 300/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 14.23 | mse 14.23 | mre  0.00 |
scGPT - INFO - | epoch   8 | 400/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 15.38 | mse 15.38 | mre  0.00 |
scGPT - INFO - | epoch   8 | 500/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 14.95 | mse 14.95 | mre  0.00 |
scGPT - INFO - | epoch   8 | 600/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 15.02 | mse 15.02 | mre  0.00 |
scGPT - INFO - | epoch   8 | 700/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 16.26 | mse 16.26 | mre  0.00 |
scGPT - INFO - | epoch   8 | 800/3602 batches | lr 0.0000 | ms/batch 91.26 | loss 18.03 | mse 18.03 | mre  0.00 |
scGPT - INFO - | epoch   8 | 900/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 15.45 | mse 15.45 | mre  0.00 |
scGPT - INFO - | epoch   8 | 1000/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 17.33 | mse 17.33 | mre  0.00 |
scGPT - INFO - | epoch   8 | 1100/3602 batches | lr 0.0000 | ms/batch 90.97 | loss 16.33 | mse 16.33 | mre  0.00 |
scGPT - INFO - | epoch   8 | 1200/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 15.68 | mse 15.68 | mre  0.00 |
scGPT - INFO - | epoch   8 | 1300/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 17.31 | mse 17.31 | mre  0.00 |
scGPT - INFO - | epoch   8 | 1400/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 18.36 | mse 18.36 | mre  0.00 |
scGPT - INFO - | epoch   8 | 1500/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 17.45 | mse 17.45 | mre  0.00 |
scGPT - INFO - | epoch   8 | 1600/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 14.51 | mse 14.51 | mre  0.00 |
scGPT - INFO - | epoch   8 | 1700/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 16.36 | mse 16.36 | mre  0.00 |
scGPT - INFO - | epoch   8 | 1800/3602 batches | lr 0.0000 | ms/batch 91.27 | loss 14.57 | mse 14.57 | mre  0.00 |
scGPT - INFO - | epoch   8 | 1900/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 18.56 | mse 18.56 | mre  0.00 |
scGPT - INFO - | epoch   8 | 2000/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 16.73 | mse 16.73 | mre  0.00 |
scGPT - INFO - | epoch   8 | 2100/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 17.10 | mse 17.10 | mre  0.00 |
scGPT - INFO - | epoch   8 | 2200/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 15.73 | mse 15.73 | mre  0.00 |
scGPT - INFO - | epoch   8 | 2300/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 16.06 | mse 16.06 | mre  0.00 |
scGPT - INFO - | epoch   8 | 2400/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 15.61 | mse 15.61 | mre  0.00 |
scGPT - INFO - | epoch   8 | 2500/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 15.38 | mse 15.38 | mre  0.00 |
scGPT - INFO - | epoch   8 | 2600/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 17.59 | mse 17.59 | mre  0.00 |
scGPT - INFO - | epoch   8 | 2700/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 16.25 | mse 16.25 | mre  0.00 |
scGPT - INFO - | epoch   8 | 2800/3602 batches | lr 0.0000 | ms/batch 91.30 | loss 15.69 | mse 15.69 | mre  0.00 |
scGPT - INFO - | epoch   8 | 2900/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 16.01 | mse 16.01 | mre  0.00 |
scGPT - INFO - | epoch   8 | 3000/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 16.78 | mse 16.78 | mre  0.00 |
scGPT - INFO - | epoch   8 | 3100/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 15.62 | mse 15.62 | mre  0.00 |
scGPT - INFO - | epoch   8 | 3200/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 15.45 | mse 15.45 | mre  0.00 |
scGPT - INFO - | epoch   8 | 3300/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 14.28 | mse 14.28 | mre  0.00 |
scGPT - INFO - | epoch   8 | 3400/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 15.27 | mse 15.27 | mre  0.00 |
scGPT - INFO - | epoch   8 | 3500/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 14.53 | mse 14.53 | mre  0.00 |
scGPT - INFO - | epoch   8 | 3600/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 18.14 | mse 18.14 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   8 | time: 340.14s | valid loss/mse 15.5706 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 15.5706
best_loss: 16.58820451749547, min_delta 0.0001, val_loss 15.57063339697139
Loss error: 1.0175711205240798
epoch:  8
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.438 3.732 3.434 ... 3.527 3.715 3.855]
 [6.49  3.74  3.432 ... 3.547 3.729 3.871]
 [4.66  3.984 4.043 ... 4.035 3.857 3.549]
 ...
 [4.883 3.582 3.734 ... 4.043 3.637 3.553]
 [4.98  3.75  3.91  ... 4.152 3.791 3.635]
 [6.395 3.803 3.488 ... 3.553 3.746 3.857]]
(801, 11) (801, 11)
By feature:  MSE:  0.7078¬±0.0 MAE:  0.572¬±0.0 R2:  0.0546¬±0.0 PCC:  0.6973¬±0.0 Cosine Similarity:  0.9865¬±0.0
By sample:  MSE:  0.7078¬±0.0 MAE:  0.572¬±0.0 R2:  0.1505¬±0.0 PCC:  0.3792¬±0.0 Cosine Similarity:  0.9825¬±0.0
scGPT - INFO - By feature: MSE: 0.7077999711036682¬±0.0 MAE: 0.5720000267028809¬±0.0 R2: 0.0546¬±0.0 PCC: 0.6973¬±0.0 Cosine Similarity: 0.9865¬±0.0
scGPT - INFO - By sample: MSE: 0.7077999711036682¬±0.0 MAE: 0.5720000267028809¬±0.0 R2: 0.1505¬±0.0 PCC: 0.3792¬±0.0 Cosine Similarity: 0.9825¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  9
Training model
scGPT - INFO - | epoch   9 | 100/3602 batches | lr 0.0000 | ms/batch 93.74 | loss 15.54 | mse 15.54 | mre  0.00 |
scGPT - INFO - | epoch   9 | 200/3602 batches | lr 0.0000 | ms/batch 93.30 | loss 16.22 | mse 16.22 | mre  0.00 |
scGPT - INFO - | epoch   9 | 300/3602 batches | lr 0.0000 | ms/batch 92.83 | loss 13.58 | mse 13.58 | mre  0.00 |
scGPT - INFO - | epoch   9 | 400/3602 batches | lr 0.0000 | ms/batch 92.68 | loss 14.63 | mse 14.63 | mre  0.00 |
scGPT - INFO - | epoch   9 | 500/3602 batches | lr 0.0000 | ms/batch 92.76 | loss 14.31 | mse 14.31 | mre  0.00 |
scGPT - INFO - | epoch   9 | 600/3602 batches | lr 0.0000 | ms/batch 92.76 | loss 13.91 | mse 13.91 | mre  0.00 |
scGPT - INFO - | epoch   9 | 700/3602 batches | lr 0.0000 | ms/batch 92.75 | loss 15.38 | mse 15.38 | mre  0.00 |
scGPT - INFO - | epoch   9 | 800/3602 batches | lr 0.0000 | ms/batch 92.69 | loss 16.85 | mse 16.85 | mre  0.00 |
scGPT - INFO - | epoch   9 | 900/3602 batches | lr 0.0000 | ms/batch 92.75 | loss 15.10 | mse 15.10 | mre  0.00 |
scGPT - INFO - | epoch   9 | 1000/3602 batches | lr 0.0000 | ms/batch 92.68 | loss 16.05 | mse 16.05 | mre  0.00 |
scGPT - INFO - | epoch   9 | 1100/3602 batches | lr 0.0000 | ms/batch 92.71 | loss 14.57 | mse 14.57 | mre  0.00 |
scGPT - INFO - | epoch   9 | 1200/3602 batches | lr 0.0000 | ms/batch 92.25 | loss 15.17 | mse 15.17 | mre  0.00 |
scGPT - INFO - | epoch   9 | 1300/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 16.40 | mse 16.40 | mre  0.00 |
scGPT - INFO - | epoch   9 | 1400/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 16.94 | mse 16.94 | mre  0.00 |
scGPT - INFO - | epoch   9 | 1500/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 17.32 | mse 17.32 | mre  0.00 |
scGPT - INFO - | epoch   9 | 1600/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 13.62 | mse 13.62 | mre  0.00 |
scGPT - INFO - | epoch   9 | 1700/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 15.23 | mse 15.23 | mre  0.00 |
scGPT - INFO - | epoch   9 | 1800/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 14.34 | mse 14.34 | mre  0.00 |
scGPT - INFO - | epoch   9 | 1900/3602 batches | lr 0.0000 | ms/batch 92.18 | loss 17.03 | mse 17.03 | mre  0.00 |
scGPT - INFO - | epoch   9 | 2000/3602 batches | lr 0.0000 | ms/batch 92.82 | loss 15.84 | mse 15.84 | mre  0.00 |
scGPT - INFO - | epoch   9 | 2100/3602 batches | lr 0.0000 | ms/batch 92.77 | loss 16.44 | mse 16.44 | mre  0.00 |
scGPT - INFO - | epoch   9 | 2200/3602 batches | lr 0.0000 | ms/batch 93.34 | loss 15.42 | mse 15.42 | mre  0.00 |
scGPT - INFO - | epoch   9 | 2300/3602 batches | lr 0.0000 | ms/batch 92.78 | loss 14.92 | mse 14.92 | mre  0.00 |
scGPT - INFO - | epoch   9 | 2400/3602 batches | lr 0.0000 | ms/batch 92.68 | loss 14.87 | mse 14.87 | mre  0.00 |
scGPT - INFO - | epoch   9 | 2500/3602 batches | lr 0.0000 | ms/batch 92.79 | loss 14.19 | mse 14.19 | mre  0.00 |
scGPT - INFO - | epoch   9 | 2600/3602 batches | lr 0.0000 | ms/batch 92.77 | loss 17.10 | mse 17.10 | mre  0.00 |
scGPT - INFO - | epoch   9 | 2700/3602 batches | lr 0.0000 | ms/batch 92.70 | loss 14.96 | mse 14.96 | mre  0.00 |
scGPT - INFO - | epoch   9 | 2800/3602 batches | lr 0.0000 | ms/batch 92.79 | loss 15.06 | mse 15.06 | mre  0.00 |
scGPT - INFO - | epoch   9 | 2900/3602 batches | lr 0.0000 | ms/batch 92.77 | loss 15.62 | mse 15.62 | mre  0.00 |
scGPT - INFO - | epoch   9 | 3000/3602 batches | lr 0.0000 | ms/batch 92.76 | loss 15.45 | mse 15.45 | mre  0.00 |
scGPT - INFO - | epoch   9 | 3100/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 14.19 | mse 14.19 | mre  0.00 |
scGPT - INFO - | epoch   9 | 3200/3602 batches | lr 0.0000 | ms/batch 91.21 | loss 14.70 | mse 14.70 | mre  0.00 |
scGPT - INFO - | epoch   9 | 3300/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 13.43 | mse 13.43 | mre  0.00 |
scGPT - INFO - | epoch   9 | 3400/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 14.74 | mse 14.74 | mre  0.00 |
scGPT - INFO - | epoch   9 | 3500/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 13.72 | mse 13.72 | mre  0.00 |
scGPT - INFO - | epoch   9 | 3600/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 16.70 | mse 16.70 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   9 | time: 344.11s | valid loss/mse 14.4199 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 14.4199
best_loss: 15.57063339697139, min_delta 0.0001, val_loss 14.41985273495149
Loss error: 1.1507806620198995
epoch:  9
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.562 3.697 3.371 ... 3.533 3.695 3.889]
 [6.76  3.686 3.354 ... 3.562 3.715 3.924]
 [4.246 4.28  4.293 ... 3.93  3.965 3.53 ]
 ...
 [5.188 3.678 3.797 ... 4.234 3.756 3.664]
 [5.21  3.84  3.97  ... 4.3   3.893 3.736]
 [6.793 3.688 3.344 ... 3.559 3.723 3.93 ]]
(801, 11) (801, 11)
By feature:  MSE:  0.6555¬±0.0 MAE:  0.5401¬±0.0 R2:  0.1173¬±0.0 PCC:  0.7365¬±0.0 Cosine Similarity:  0.9879¬±0.0
By sample:  MSE:  0.6555¬±0.0 MAE:  0.5401¬±0.0 R2:  0.198¬±0.0 PCC:  0.4341¬±0.0 Cosine Similarity:  0.9836¬±0.0
scGPT - INFO - By feature: MSE: 0.6554999947547913¬±0.0 MAE: 0.5400999784469604¬±0.0 R2: 0.1173¬±0.0 PCC: 0.7365¬±0.0 Cosine Similarity: 0.9879¬±0.0
scGPT - INFO - By sample: MSE: 0.6554999947547913¬±0.0 MAE: 0.5400999784469604¬±0.0 R2: 0.198¬±0.0 PCC: 0.4341¬±0.0 Cosine Similarity: 0.9836¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  10
Training model
scGPT - INFO - | epoch  10 | 100/3602 batches | lr 0.0000 | ms/batch 91.93 | loss 14.87 | mse 14.87 | mre  0.00 |
scGPT - INFO - | epoch  10 | 200/3602 batches | lr 0.0000 | ms/batch 91.85 | loss 15.50 | mse 15.50 | mre  0.00 |
scGPT - INFO - | epoch  10 | 300/3602 batches | lr 0.0000 | ms/batch 92.93 | loss 13.32 | mse 13.32 | mre  0.00 |
scGPT - INFO - | epoch  10 | 400/3602 batches | lr 0.0000 | ms/batch 92.85 | loss 13.16 | mse 13.16 | mre  0.00 |
scGPT - INFO - | epoch  10 | 500/3602 batches | lr 0.0000 | ms/batch 92.94 | loss 14.07 | mse 14.07 | mre  0.00 |
scGPT - INFO - | epoch  10 | 600/3602 batches | lr 0.0000 | ms/batch 93.46 | loss 13.58 | mse 13.58 | mre  0.00 |
scGPT - INFO - | epoch  10 | 700/3602 batches | lr 0.0000 | ms/batch 92.91 | loss 14.41 | mse 14.41 | mre  0.00 |
scGPT - INFO - | epoch  10 | 800/3602 batches | lr 0.0000 | ms/batch 92.86 | loss 16.47 | mse 16.47 | mre  0.00 |
scGPT - INFO - | epoch  10 | 900/3602 batches | lr 0.0000 | ms/batch 92.87 | loss 13.85 | mse 13.85 | mre  0.00 |
scGPT - INFO - | epoch  10 | 1000/3602 batches | lr 0.0000 | ms/batch 91.41 | loss 15.54 | mse 15.54 | mre  0.00 |
scGPT - INFO - | epoch  10 | 1100/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 13.97 | mse 13.97 | mre  0.00 |
scGPT - INFO - | epoch  10 | 1200/3602 batches | lr 0.0000 | ms/batch 90.99 | loss 14.47 | mse 14.47 | mre  0.00 |
scGPT - INFO - | epoch  10 | 1300/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 15.58 | mse 15.58 | mre  0.00 |
scGPT - INFO - | epoch  10 | 1400/3602 batches | lr 0.0000 | ms/batch 90.95 | loss 16.81 | mse 16.81 | mre  0.00 |
scGPT - INFO - | epoch  10 | 1500/3602 batches | lr 0.0000 | ms/batch 90.97 | loss 16.14 | mse 16.14 | mre  0.00 |
scGPT - INFO - | epoch  10 | 1600/3602 batches | lr 0.0000 | ms/batch 91.55 | loss 13.05 | mse 13.05 | mre  0.00 |
scGPT - INFO - | epoch  10 | 1700/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 14.33 | mse 14.33 | mre  0.00 |
scGPT - INFO - | epoch  10 | 1800/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 13.42 | mse 13.42 | mre  0.00 |
scGPT - INFO - | epoch  10 | 1900/3602 batches | lr 0.0000 | ms/batch 91.08 | loss 16.34 | mse 16.34 | mre  0.00 |
scGPT - INFO - | epoch  10 | 2000/3602 batches | lr 0.0000 | ms/batch 91.05 | loss 15.02 | mse 15.02 | mre  0.00 |
scGPT - INFO - | epoch  10 | 2100/3602 batches | lr 0.0000 | ms/batch 90.98 | loss 15.52 | mse 15.52 | mre  0.00 |
scGPT - INFO - | epoch  10 | 2200/3602 batches | lr 0.0000 | ms/batch 90.99 | loss 14.67 | mse 14.67 | mre  0.00 |
scGPT - INFO - | epoch  10 | 2300/3602 batches | lr 0.0000 | ms/batch 91.13 | loss 14.19 | mse 14.19 | mre  0.00 |
scGPT - INFO - | epoch  10 | 2400/3602 batches | lr 0.0000 | ms/batch 90.99 | loss 14.30 | mse 14.30 | mre  0.00 |
scGPT - INFO - | epoch  10 | 2500/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 13.75 | mse 13.75 | mre  0.00 |
scGPT - INFO - | epoch  10 | 2600/3602 batches | lr 0.0000 | ms/batch 91.38 | loss 17.10 | mse 17.10 | mre  0.00 |
scGPT - INFO - | epoch  10 | 2700/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 14.40 | mse 14.40 | mre  0.00 |
scGPT - INFO - | epoch  10 | 2800/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 14.83 | mse 14.83 | mre  0.00 |
scGPT - INFO - | epoch  10 | 2900/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 14.56 | mse 14.56 | mre  0.00 |
scGPT - INFO - | epoch  10 | 3000/3602 batches | lr 0.0000 | ms/batch 90.99 | loss 14.88 | mse 14.88 | mre  0.00 |
scGPT - INFO - | epoch  10 | 3100/3602 batches | lr 0.0000 | ms/batch 90.95 | loss 13.89 | mse 13.89 | mre  0.00 |
scGPT - INFO - | epoch  10 | 3200/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 14.35 | mse 14.35 | mre  0.00 |
scGPT - INFO - | epoch  10 | 3300/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 12.76 | mse 12.76 | mre  0.00 |
scGPT - INFO - | epoch  10 | 3400/3602 batches | lr 0.0000 | ms/batch 91.08 | loss 14.61 | mse 14.61 | mre  0.00 |
scGPT - INFO - | epoch  10 | 3500/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 12.93 | mse 12.93 | mre  0.00 |
scGPT - INFO - | epoch  10 | 3600/3602 batches | lr 0.0000 | ms/batch 91.41 | loss 16.29 | mse 16.29 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  10 | time: 341.67s | valid loss/mse 13.9024 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 13.9024
best_loss: 14.41985273495149, min_delta 0.0001, val_loss 13.902436010325594
Loss error: 0.517416724625896
epoch:  10
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[5.94  3.693 3.422 ... 3.508 3.617 3.748]
 [6.68  3.678 3.367 ... 3.53  3.695 3.898]
 [4.016 4.324 4.39  ... 3.81  3.936 3.457]
 ...
 [5.02  3.672 3.785 ... 4.164 3.73  3.58 ]
 [5.02  3.795 3.941 ... 4.207 3.848 3.637]
 [6.266 3.795 3.469 ... 3.531 3.697 3.842]]
(801, 11) (801, 11)
By feature:  MSE:  0.632¬±0.0 MAE:  0.5278¬±0.0 R2:  0.1491¬±0.0 PCC:  0.7516¬±0.0 Cosine Similarity:  0.9884¬±0.0
By sample:  MSE:  0.632¬±0.0 MAE:  0.5278¬±0.0 R2:  0.2242¬±0.0 PCC:  0.4599¬±0.0 Cosine Similarity:  0.9841¬±0.0
scGPT - INFO - By feature: MSE: 0.6320000290870667¬±0.0 MAE: 0.5278000235557556¬±0.0 R2: 0.1491¬±0.0 PCC: 0.7516¬±0.0 Cosine Similarity: 0.9884¬±0.0
scGPT - INFO - By sample: MSE: 0.6320000290870667¬±0.0 MAE: 0.5278000235557556¬±0.0 R2: 0.2242¬±0.0 PCC: 0.4599¬±0.0 Cosine Similarity: 0.9841¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  11
Training model
scGPT - INFO - | epoch  11 | 100/3602 batches | lr 0.0000 | ms/batch 92.21 | loss 14.12 | mse 14.12 | mre  0.00 |
scGPT - INFO - | epoch  11 | 200/3602 batches | lr 0.0000 | ms/batch 90.93 | loss 14.43 | mse 14.43 | mre  0.00 |
scGPT - INFO - | epoch  11 | 300/3602 batches | lr 0.0000 | ms/batch 91.08 | loss 12.56 | mse 12.56 | mre  0.00 |
scGPT - INFO - | epoch  11 | 400/3602 batches | lr 0.0000 | ms/batch 91.15 | loss 13.31 | mse 13.31 | mre  0.00 |
scGPT - INFO - | epoch  11 | 500/3602 batches | lr 0.0000 | ms/batch 91.00 | loss 12.94 | mse 12.94 | mre  0.00 |
scGPT - INFO - | epoch  11 | 600/3602 batches | lr 0.0000 | ms/batch 91.26 | loss 13.30 | mse 13.30 | mre  0.00 |
scGPT - INFO - | epoch  11 | 700/3602 batches | lr 0.0000 | ms/batch 90.95 | loss 13.49 | mse 13.49 | mre  0.00 |
scGPT - INFO - | epoch  11 | 800/3602 batches | lr 0.0000 | ms/batch 90.91 | loss 16.54 | mse 16.54 | mre  0.00 |
scGPT - INFO - | epoch  11 | 900/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 13.16 | mse 13.16 | mre  0.00 |
scGPT - INFO - | epoch  11 | 1000/3602 batches | lr 0.0000 | ms/batch 91.63 | loss 14.82 | mse 14.82 | mre  0.00 |
scGPT - INFO - | epoch  11 | 1100/3602 batches | lr 0.0000 | ms/batch 91.02 | loss 13.30 | mse 13.30 | mre  0.00 |
scGPT - INFO - | epoch  11 | 1200/3602 batches | lr 0.0000 | ms/batch 91.08 | loss 14.21 | mse 14.21 | mre  0.00 |
scGPT - INFO - | epoch  11 | 1300/3602 batches | lr 0.0000 | ms/batch 91.01 | loss 14.82 | mse 14.82 | mre  0.00 |
scGPT - INFO - | epoch  11 | 1400/3602 batches | lr 0.0000 | ms/batch 91.09 | loss 16.09 | mse 16.09 | mre  0.00 |
scGPT - INFO - | epoch  11 | 1500/3602 batches | lr 0.0000 | ms/batch 91.01 | loss 16.54 | mse 16.54 | mre  0.00 |
scGPT - INFO - | epoch  11 | 1600/3602 batches | lr 0.0000 | ms/batch 90.99 | loss 12.59 | mse 12.59 | mre  0.00 |
scGPT - INFO - | epoch  11 | 1700/3602 batches | lr 0.0000 | ms/batch 91.07 | loss 14.19 | mse 14.19 | mre  0.00 |
scGPT - INFO - | epoch  11 | 1800/3602 batches | lr 0.0000 | ms/batch 91.04 | loss 13.33 | mse 13.33 | mre  0.00 |
scGPT - INFO - | epoch  11 | 1900/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 16.14 | mse 16.14 | mre  0.00 |
scGPT - INFO - | epoch  11 | 2000/3602 batches | lr 0.0000 | ms/batch 91.51 | loss 14.64 | mse 14.64 | mre  0.00 |
scGPT - INFO - | epoch  11 | 2100/3602 batches | lr 0.0000 | ms/batch 90.93 | loss 15.56 | mse 15.56 | mre  0.00 |
scGPT - INFO - | epoch  11 | 2200/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 14.18 | mse 14.18 | mre  0.00 |
scGPT - INFO - | epoch  11 | 2300/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 13.71 | mse 13.71 | mre  0.00 |
scGPT - INFO - | epoch  11 | 2400/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 14.10 | mse 14.10 | mre  0.00 |
scGPT - INFO - | epoch  11 | 2500/3602 batches | lr 0.0000 | ms/batch 90.97 | loss 12.95 | mse 12.95 | mre  0.00 |
scGPT - INFO - | epoch  11 | 2600/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 16.52 | mse 16.52 | mre  0.00 |
scGPT - INFO - | epoch  11 | 2700/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 14.32 | mse 14.32 | mre  0.00 |
scGPT - INFO - | epoch  11 | 2800/3602 batches | lr 0.0000 | ms/batch 91.04 | loss 14.26 | mse 14.26 | mre  0.00 |
scGPT - INFO - | epoch  11 | 2900/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 13.78 | mse 13.78 | mre  0.00 |
scGPT - INFO - | epoch  11 | 3000/3602 batches | lr 0.0000 | ms/batch 91.44 | loss 14.43 | mse 14.43 | mre  0.00 |
scGPT - INFO - | epoch  11 | 3100/3602 batches | lr 0.0000 | ms/batch 91.11 | loss 13.58 | mse 13.58 | mre  0.00 |
scGPT - INFO - | epoch  11 | 3200/3602 batches | lr 0.0000 | ms/batch 91.04 | loss 14.08 | mse 14.08 | mre  0.00 |
scGPT - INFO - | epoch  11 | 3300/3602 batches | lr 0.0000 | ms/batch 90.93 | loss 12.24 | mse 12.24 | mre  0.00 |
scGPT - INFO - | epoch  11 | 3400/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 13.78 | mse 13.78 | mre  0.00 |
scGPT - INFO - | epoch  11 | 3500/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 12.75 | mse 12.75 | mre  0.00 |
scGPT - INFO - | epoch  11 | 3600/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 15.49 | mse 15.49 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  11 | time: 340.17s | valid loss/mse 13.6737 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 13.6737
best_loss: 13.902436010325594, min_delta 0.0001, val_loss 13.673745089702392
Loss error: 0.22869092062320284
epoch:  11
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[5.426 3.764 3.48  ... 3.635 3.656 3.682]
 [6.83  3.652 3.375 ... 3.557 3.707 3.93 ]
 [4.125 4.043 4.14  ... 3.791 3.826 3.414]
 ...
 [4.96  3.709 3.834 ... 4.15  3.73  3.602]
 [4.996 3.8   3.957 ... 4.195 3.836 3.65 ]
 [6.848 3.65  3.346 ... 3.553 3.7   3.936]]
(801, 11) (801, 11)
By feature:  MSE:  0.6216¬±0.0 MAE:  0.5168¬±0.0 R2:  0.1641¬±0.0 PCC:  0.7627¬±0.0 Cosine Similarity:  0.9887¬±0.0
By sample:  MSE:  0.6216¬±0.0 MAE:  0.5168¬±0.0 R2:  0.2348¬±0.0 PCC:  0.471¬±0.0 Cosine Similarity:  0.9843¬±0.0
scGPT - INFO - By feature: MSE: 0.6215999722480774¬±0.0 MAE: 0.5167999863624573¬±0.0 R2: 0.1641¬±0.0 PCC: 0.7627¬±0.0 Cosine Similarity: 0.9887¬±0.0
scGPT - INFO - By sample: MSE: 0.6215999722480774¬±0.0 MAE: 0.5167999863624573¬±0.0 R2: 0.2348¬±0.0 PCC: 0.471¬±0.0 Cosine Similarity: 0.9843¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  12
Training model
scGPT - INFO - | epoch  12 | 100/3602 batches | lr 0.0000 | ms/batch 92.06 | loss 13.95 | mse 13.95 | mre  0.00 |
scGPT - INFO - | epoch  12 | 200/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 14.38 | mse 14.38 | mre  0.00 |
scGPT - INFO - | epoch  12 | 300/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 12.24 | mse 12.24 | mre  0.00 |
scGPT - INFO - | epoch  12 | 400/3602 batches | lr 0.0000 | ms/batch 91.38 | loss 12.54 | mse 12.54 | mre  0.00 |
scGPT - INFO - | epoch  12 | 500/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 12.69 | mse 12.69 | mre  0.00 |
scGPT - INFO - | epoch  12 | 600/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 13.10 | mse 13.10 | mre  0.00 |
scGPT - INFO - | epoch  12 | 700/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 13.69 | mse 13.69 | mre  0.00 |
scGPT - INFO - | epoch  12 | 800/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 16.09 | mse 16.09 | mre  0.00 |
scGPT - INFO - | epoch  12 | 900/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 13.25 | mse 13.25 | mre  0.00 |
scGPT - INFO - | epoch  12 | 1000/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 14.50 | mse 14.50 | mre  0.00 |
scGPT - INFO - | epoch  12 | 1100/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 12.96 | mse 12.96 | mre  0.00 |
scGPT - INFO - | epoch  12 | 1200/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 13.65 | mse 13.65 | mre  0.00 |
scGPT - INFO - | epoch  12 | 1300/3602 batches | lr 0.0000 | ms/batch 91.08 | loss 14.24 | mse 14.24 | mre  0.00 |
scGPT - INFO - | epoch  12 | 1400/3602 batches | lr 0.0000 | ms/batch 91.54 | loss 15.88 | mse 15.88 | mre  0.00 |
scGPT - INFO - | epoch  12 | 1500/3602 batches | lr 0.0000 | ms/batch 91.14 | loss 15.54 | mse 15.54 | mre  0.00 |
scGPT - INFO - | epoch  12 | 1600/3602 batches | lr 0.0000 | ms/batch 91.05 | loss 12.54 | mse 12.54 | mre  0.00 |
scGPT - INFO - | epoch  12 | 1700/3602 batches | lr 0.0000 | ms/batch 90.95 | loss 13.52 | mse 13.52 | mre  0.00 |
scGPT - INFO - | epoch  12 | 1800/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 12.79 | mse 12.79 | mre  0.00 |
scGPT - INFO - | epoch  12 | 1900/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 15.71 | mse 15.71 | mre  0.00 |
scGPT - INFO - | epoch  12 | 2000/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 14.08 | mse 14.08 | mre  0.00 |
scGPT - INFO - | epoch  12 | 2100/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 15.19 | mse 15.19 | mre  0.00 |
scGPT - INFO - | epoch  12 | 2200/3602 batches | lr 0.0000 | ms/batch 90.62 | loss 13.84 | mse 13.84 | mre  0.00 |
scGPT - INFO - | epoch  12 | 2300/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 13.20 | mse 13.20 | mre  0.00 |
scGPT - INFO - | epoch  12 | 2400/3602 batches | lr 0.0000 | ms/batch 91.19 | loss 13.55 | mse 13.55 | mre  0.00 |
scGPT - INFO - | epoch  12 | 2500/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 13.22 | mse 13.22 | mre  0.00 |
scGPT - INFO - | epoch  12 | 2600/3602 batches | lr 0.0000 | ms/batch 90.58 | loss 16.45 | mse 16.45 | mre  0.00 |
scGPT - INFO - | epoch  12 | 2700/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 13.86 | mse 13.86 | mre  0.00 |
scGPT - INFO - | epoch  12 | 2800/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 13.98 | mse 13.98 | mre  0.00 |
scGPT - INFO - | epoch  12 | 2900/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 13.68 | mse 13.68 | mre  0.00 |
scGPT - INFO - | epoch  12 | 3000/3602 batches | lr 0.0000 | ms/batch 90.61 | loss 14.03 | mse 14.03 | mre  0.00 |
scGPT - INFO - | epoch  12 | 3100/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 13.09 | mse 13.09 | mre  0.00 |
scGPT - INFO - | epoch  12 | 3200/3602 batches | lr 0.0000 | ms/batch 90.62 | loss 13.59 | mse 13.59 | mre  0.00 |
scGPT - INFO - | epoch  12 | 3300/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 12.25 | mse 12.25 | mre  0.00 |
scGPT - INFO - | epoch  12 | 3400/3602 batches | lr 0.0000 | ms/batch 91.21 | loss 13.48 | mse 13.48 | mre  0.00 |
scGPT - INFO - | epoch  12 | 3500/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 12.62 | mse 12.62 | mre  0.00 |
scGPT - INFO - | epoch  12 | 3600/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 15.30 | mse 15.30 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  12 | time: 339.54s | valid loss/mse 13.4826 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 13.4826
best_loss: 13.673745089702392, min_delta 0.0001, val_loss 13.482574017009187
Loss error: 0.191171072693205
epoch:  12
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[5.395 3.81  3.545 ... 3.652 3.688 3.709]
 [6.93  3.643 3.371 ... 3.541 3.693 3.94 ]
 [4.355 3.812 3.977 ... 3.836 3.758 3.418]
 ...
 [4.926 3.715 3.844 ... 4.113 3.713 3.588]
 [4.97  3.795 3.959 ... 4.17  3.81  3.639]
 [6.875 3.648 3.352 ... 3.535 3.676 3.936]]
(801, 11) (801, 11)
By feature:  MSE:  0.6129¬±0.0 MAE:  0.5129¬±0.0 R2:  0.1794¬±0.0 PCC:  0.7687¬±0.0 Cosine Similarity:  0.9889¬±0.0
By sample:  MSE:  0.6129¬±0.0 MAE:  0.5129¬±0.0 R2:  0.2465¬±0.0 PCC:  0.4834¬±0.0 Cosine Similarity:  0.9846¬±0.0
scGPT - INFO - By feature: MSE: 0.6129000186920166¬±0.0 MAE: 0.5128999948501587¬±0.0 R2: 0.1794¬±0.0 PCC: 0.7687¬±0.0 Cosine Similarity: 0.9889¬±0.0
scGPT - INFO - By sample: MSE: 0.6129000186920166¬±0.0 MAE: 0.5128999948501587¬±0.0 R2: 0.2465¬±0.0 PCC: 0.4834¬±0.0 Cosine Similarity: 0.9846¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  13
Training model
scGPT - INFO - | epoch  13 | 100/3602 batches | lr 0.0000 | ms/batch 91.89 | loss 13.92 | mse 13.92 | mre  0.00 |
scGPT - INFO - | epoch  13 | 200/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 14.17 | mse 14.17 | mre  0.00 |
scGPT - INFO - | epoch  13 | 300/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 12.51 | mse 12.51 | mre  0.00 |
scGPT - INFO - | epoch  13 | 400/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 12.47 | mse 12.47 | mre  0.00 |
scGPT - INFO - | epoch  13 | 500/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 11.99 | mse 11.99 | mre  0.00 |
scGPT - INFO - | epoch  13 | 600/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 12.57 | mse 12.57 | mre  0.00 |
scGPT - INFO - | epoch  13 | 700/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 13.24 | mse 13.24 | mre  0.00 |
scGPT - INFO - | epoch  13 | 800/3602 batches | lr 0.0000 | ms/batch 91.26 | loss 16.10 | mse 16.10 | mre  0.00 |
scGPT - INFO - | epoch  13 | 900/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 13.01 | mse 13.01 | mre  0.00 |
scGPT - INFO - | epoch  13 | 1000/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 14.32 | mse 14.32 | mre  0.00 |
scGPT - INFO - | epoch  13 | 1100/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 12.66 | mse 12.66 | mre  0.00 |
scGPT - INFO - | epoch  13 | 1200/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 13.60 | mse 13.60 | mre  0.00 |
scGPT - INFO - | epoch  13 | 1300/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 13.87 | mse 13.87 | mre  0.00 |
scGPT - INFO - | epoch  13 | 1400/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 15.40 | mse 15.40 | mre  0.00 |
scGPT - INFO - | epoch  13 | 1500/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 15.29 | mse 15.29 | mre  0.00 |
scGPT - INFO - | epoch  13 | 1600/3602 batches | lr 0.0000 | ms/batch 92.71 | loss 11.77 | mse 11.77 | mre  0.00 |
scGPT - INFO - | epoch  13 | 1700/3602 batches | lr 0.0000 | ms/batch 92.69 | loss 13.41 | mse 13.41 | mre  0.00 |
scGPT - INFO - | epoch  13 | 1800/3602 batches | lr 0.0000 | ms/batch 93.24 | loss 12.64 | mse 12.64 | mre  0.00 |
scGPT - INFO - | epoch  13 | 1900/3602 batches | lr 0.0000 | ms/batch 92.73 | loss 15.27 | mse 15.27 | mre  0.00 |
scGPT - INFO - | epoch  13 | 2000/3602 batches | lr 0.0000 | ms/batch 92.63 | loss 13.11 | mse 13.11 | mre  0.00 |
scGPT - INFO - | epoch  13 | 2100/3602 batches | lr 0.0000 | ms/batch 92.87 | loss 15.48 | mse 15.48 | mre  0.00 |
scGPT - INFO - | epoch  13 | 2200/3602 batches | lr 0.0000 | ms/batch 92.60 | loss 13.32 | mse 13.32 | mre  0.00 |
scGPT - INFO - | epoch  13 | 2300/3602 batches | lr 0.0000 | ms/batch 92.73 | loss 13.29 | mse 13.29 | mre  0.00 |
scGPT - INFO - | epoch  13 | 2400/3602 batches | lr 0.0000 | ms/batch 92.71 | loss 13.51 | mse 13.51 | mre  0.00 |
scGPT - INFO - | epoch  13 | 2500/3602 batches | lr 0.0000 | ms/batch 92.64 | loss 12.47 | mse 12.47 | mre  0.00 |
scGPT - INFO - | epoch  13 | 2600/3602 batches | lr 0.0000 | ms/batch 92.66 | loss 16.15 | mse 16.15 | mre  0.00 |
scGPT - INFO - | epoch  13 | 2700/3602 batches | lr 0.0000 | ms/batch 92.63 | loss 12.98 | mse 12.98 | mre  0.00 |
scGPT - INFO - | epoch  13 | 2800/3602 batches | lr 0.0000 | ms/batch 93.21 | loss 13.38 | mse 13.38 | mre  0.00 |
scGPT - INFO - | epoch  13 | 2900/3602 batches | lr 0.0000 | ms/batch 92.67 | loss 13.49 | mse 13.49 | mre  0.00 |
scGPT - INFO - | epoch  13 | 3000/3602 batches | lr 0.0000 | ms/batch 92.59 | loss 13.89 | mse 13.89 | mre  0.00 |
scGPT - INFO - | epoch  13 | 3100/3602 batches | lr 0.0000 | ms/batch 92.59 | loss 13.39 | mse 13.39 | mre  0.00 |
scGPT - INFO - | epoch  13 | 3200/3602 batches | lr 0.0000 | ms/batch 92.70 | loss 13.21 | mse 13.21 | mre  0.00 |
scGPT - INFO - | epoch  13 | 3300/3602 batches | lr 0.0000 | ms/batch 92.57 | loss 11.88 | mse 11.88 | mre  0.00 |
scGPT - INFO - | epoch  13 | 3400/3602 batches | lr 0.0000 | ms/batch 92.70 | loss 13.44 | mse 13.44 | mre  0.00 |
scGPT - INFO - | epoch  13 | 3500/3602 batches | lr 0.0000 | ms/batch 92.72 | loss 12.26 | mse 12.26 | mre  0.00 |
scGPT - INFO - | epoch  13 | 3600/3602 batches | lr 0.0000 | ms/batch 92.68 | loss 14.80 | mse 14.80 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  13 | time: 344.02s | valid loss/mse 13.2803 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 13.2803
best_loss: 13.482574017009187, min_delta 0.0001, val_loss 13.280263608165745
Loss error: 0.20231040884344154
epoch:  13
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[4.54  3.848 3.688 ... 3.71  3.68  3.516]
 [7.02  3.66  3.379 ... 3.547 3.715 3.963]
 [4.598 3.695 3.852 ... 3.969 3.695 3.514]
 ...
 [4.85  3.72  3.82  ... 4.08  3.705 3.574]
 [4.945 3.818 3.95  ... 4.164 3.812 3.643]
 [6.9   3.65  3.336 ... 3.55  3.678 3.947]]
(801, 11) (801, 11)
By feature:  MSE:  0.6037¬±0.0 MAE:  0.5104¬±0.0 R2:  0.1942¬±0.0 PCC:  0.7722¬±0.0 Cosine Similarity:  0.9891¬±0.0
By sample:  MSE:  0.6037¬±0.0 MAE:  0.5104¬±0.0 R2:  0.2546¬±0.0 PCC:  0.4908¬±0.0 Cosine Similarity:  0.9847¬±0.0
scGPT - INFO - By feature: MSE: 0.6036999821662903¬±0.0 MAE: 0.5103999972343445¬±0.0 R2: 0.1942¬±0.0 PCC: 0.7722¬±0.0 Cosine Similarity: 0.9891¬±0.0
scGPT - INFO - By sample: MSE: 0.6036999821662903¬±0.0 MAE: 0.5103999972343445¬±0.0 R2: 0.2546¬±0.0 PCC: 0.4908¬±0.0 Cosine Similarity: 0.9847¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  14
Training model
scGPT - INFO - | epoch  14 | 100/3602 batches | lr 0.0000 | ms/batch 93.95 | loss 13.58 | mse 13.58 | mre  0.00 |
scGPT - INFO - | epoch  14 | 200/3602 batches | lr 0.0000 | ms/batch 93.22 | loss 13.82 | mse 13.82 | mre  0.00 |
scGPT - INFO - | epoch  14 | 300/3602 batches | lr 0.0000 | ms/batch 92.71 | loss 11.63 | mse 11.63 | mre  0.00 |
scGPT - INFO - | epoch  14 | 400/3602 batches | lr 0.0000 | ms/batch 92.65 | loss 12.26 | mse 12.26 | mre  0.00 |
scGPT - INFO - | epoch  14 | 500/3602 batches | lr 0.0000 | ms/batch 91.59 | loss 11.50 | mse 11.50 | mre  0.00 |
scGPT - INFO - | epoch  14 | 600/3602 batches | lr 0.0000 | ms/batch 92.72 | loss 12.65 | mse 12.65 | mre  0.00 |
scGPT - INFO - | epoch  14 | 700/3602 batches | lr 0.0000 | ms/batch 92.71 | loss 13.30 | mse 13.30 | mre  0.00 |
scGPT - INFO - | epoch  14 | 800/3602 batches | lr 0.0000 | ms/batch 92.68 | loss 15.33 | mse 15.33 | mre  0.00 |
scGPT - INFO - | epoch  14 | 900/3602 batches | lr 0.0000 | ms/batch 92.68 | loss 12.67 | mse 12.67 | mre  0.00 |
scGPT - INFO - | epoch  14 | 1000/3602 batches | lr 0.0000 | ms/batch 92.73 | loss 14.27 | mse 14.27 | mre  0.00 |
scGPT - INFO - | epoch  14 | 1100/3602 batches | lr 0.0000 | ms/batch 92.74 | loss 12.61 | mse 12.61 | mre  0.00 |
scGPT - INFO - | epoch  14 | 1200/3602 batches | lr 0.0000 | ms/batch 93.27 | loss 13.47 | mse 13.47 | mre  0.00 |
scGPT - INFO - | epoch  14 | 1300/3602 batches | lr 0.0000 | ms/batch 92.67 | loss 13.73 | mse 13.73 | mre  0.00 |
scGPT - INFO - | epoch  14 | 1400/3602 batches | lr 0.0000 | ms/batch 92.66 | loss 15.60 | mse 15.60 | mre  0.00 |
scGPT - INFO - | epoch  14 | 1500/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 15.27 | mse 15.27 | mre  0.00 |
scGPT - INFO - | epoch  14 | 1600/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 11.73 | mse 11.73 | mre  0.00 |
scGPT - INFO - | epoch  14 | 1700/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 13.14 | mse 13.14 | mre  0.00 |
scGPT - INFO - | epoch  14 | 1800/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 12.55 | mse 12.55 | mre  0.00 |
scGPT - INFO - | epoch  14 | 1900/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 15.28 | mse 15.28 | mre  0.00 |
scGPT - INFO - | epoch  14 | 2000/3602 batches | lr 0.0000 | ms/batch 90.62 | loss 13.41 | mse 13.41 | mre  0.00 |
scGPT - INFO - | epoch  14 | 2100/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 14.99 | mse 14.99 | mre  0.00 |
scGPT - INFO - | epoch  14 | 2200/3602 batches | lr 0.0000 | ms/batch 91.14 | loss 12.79 | mse 12.79 | mre  0.00 |
scGPT - INFO - | epoch  14 | 2300/3602 batches | lr 0.0000 | ms/batch 90.60 | loss 12.74 | mse 12.74 | mre  0.00 |
scGPT - INFO - | epoch  14 | 2400/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 12.82 | mse 12.82 | mre  0.00 |
scGPT - INFO - | epoch  14 | 2500/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 12.38 | mse 12.38 | mre  0.00 |
scGPT - INFO - | epoch  14 | 2600/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 15.70 | mse 15.70 | mre  0.00 |
scGPT - INFO - | epoch  14 | 2700/3602 batches | lr 0.0000 | ms/batch 90.61 | loss 13.12 | mse 13.12 | mre  0.00 |
scGPT - INFO - | epoch  14 | 2800/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 13.55 | mse 13.55 | mre  0.00 |
scGPT - INFO - | epoch  14 | 2900/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 13.69 | mse 13.69 | mre  0.00 |
scGPT - INFO - | epoch  14 | 3000/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 13.40 | mse 13.40 | mre  0.00 |
scGPT - INFO - | epoch  14 | 3100/3602 batches | lr 0.0000 | ms/batch 90.58 | loss 13.10 | mse 13.10 | mre  0.00 |
scGPT - INFO - | epoch  14 | 3200/3602 batches | lr 0.0000 | ms/batch 91.23 | loss 13.76 | mse 13.76 | mre  0.00 |
scGPT - INFO - | epoch  14 | 3300/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 11.51 | mse 11.51 | mre  0.00 |
scGPT - INFO - | epoch  14 | 3400/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 12.77 | mse 12.77 | mre  0.00 |
scGPT - INFO - | epoch  14 | 3500/3602 batches | lr 0.0000 | ms/batch 90.61 | loss 12.00 | mse 12.00 | mre  0.00 |
scGPT - INFO - | epoch  14 | 3600/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 14.25 | mse 14.25 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  14 | time: 341.90s | valid loss/mse 13.0933 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 13.0933
best_loss: 13.280263608165745, min_delta 0.0001, val_loss 13.093287881691655
Loss error: 0.1869757264740901
epoch:  14
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[5.402 3.729 3.482 ... 3.76  3.666 3.72 ]
 [6.945 3.635 3.367 ... 3.525 3.682 3.941]
 [4.605 3.664 3.816 ... 3.957 3.666 3.51 ]
 ...
 [4.79  3.703 3.807 ... 4.05  3.67  3.537]
 [4.9   3.81  3.938 ... 4.137 3.79  3.613]
 [6.9   3.613 3.312 ... 3.541 3.648 3.94 ]]
(801, 11) (801, 11)
By feature:  MSE:  0.5952¬±0.0 MAE:  0.5051¬±0.0 R2:  0.2086¬±0.0 PCC:  0.7814¬±0.0 Cosine Similarity:  0.9893¬±0.0
By sample:  MSE:  0.5952¬±0.0 MAE:  0.5051¬±0.0 R2:  0.2596¬±0.0 PCC:  0.4944¬±0.0 Cosine Similarity:  0.9849¬±0.0
scGPT - INFO - By feature: MSE: 0.5952000021934509¬±0.0 MAE: 0.5051000118255615¬±0.0 R2: 0.2086¬±0.0 PCC: 0.7814¬±0.0 Cosine Similarity: 0.9893¬±0.0
scGPT - INFO - By sample: MSE: 0.5952000021934509¬±0.0 MAE: 0.5051000118255615¬±0.0 R2: 0.2596¬±0.0 PCC: 0.4944¬±0.0 Cosine Similarity: 0.9849¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  15
Training model
scGPT - INFO - | epoch  15 | 100/3602 batches | lr 0.0000 | ms/batch 91.96 | loss 12.96 | mse 12.96 | mre  0.00 |
scGPT - INFO - | epoch  15 | 200/3602 batches | lr 0.0000 | ms/batch 92.13 | loss 13.66 | mse 13.66 | mre  0.00 |
scGPT - INFO - | epoch  15 | 300/3602 batches | lr 0.0000 | ms/batch 92.74 | loss 11.51 | mse 11.51 | mre  0.00 |
scGPT - INFO - | epoch  15 | 400/3602 batches | lr 0.0000 | ms/batch 92.73 | loss 12.36 | mse 12.36 | mre  0.00 |
scGPT - INFO - | epoch  15 | 500/3602 batches | lr 0.0000 | ms/batch 92.78 | loss 11.19 | mse 11.19 | mre  0.00 |
scGPT - INFO - | epoch  15 | 600/3602 batches | lr 0.0000 | ms/batch 93.27 | loss 12.55 | mse 12.55 | mre  0.00 |
scGPT - INFO - | epoch  15 | 700/3602 batches | lr 0.0000 | ms/batch 92.89 | loss 13.08 | mse 13.08 | mre  0.00 |
scGPT - INFO - | epoch  15 | 800/3602 batches | lr 0.0000 | ms/batch 92.79 | loss 15.52 | mse 15.52 | mre  0.00 |
scGPT - INFO - | epoch  15 | 900/3602 batches | lr 0.0000 | ms/batch 92.74 | loss 12.97 | mse 12.97 | mre  0.00 |
scGPT - INFO - | epoch  15 | 1000/3602 batches | lr 0.0000 | ms/batch 92.71 | loss 14.24 | mse 14.24 | mre  0.00 |
scGPT - INFO - | epoch  15 | 1100/3602 batches | lr 0.0000 | ms/batch 92.77 | loss 12.08 | mse 12.08 | mre  0.00 |
scGPT - INFO - | epoch  15 | 1200/3602 batches | lr 0.0000 | ms/batch 92.79 | loss 12.99 | mse 12.99 | mre  0.00 |
scGPT - INFO - | epoch  15 | 1300/3602 batches | lr 0.0000 | ms/batch 92.79 | loss 13.30 | mse 13.30 | mre  0.00 |
scGPT - INFO - | epoch  15 | 1400/3602 batches | lr 0.0000 | ms/batch 92.78 | loss 15.20 | mse 15.20 | mre  0.00 |
scGPT - INFO - | epoch  15 | 1500/3602 batches | lr 0.0000 | ms/batch 92.26 | loss 14.81 | mse 14.81 | mre  0.00 |
scGPT - INFO - | epoch  15 | 1600/3602 batches | lr 0.0000 | ms/batch 91.25 | loss 12.07 | mse 12.07 | mre  0.00 |
scGPT - INFO - | epoch  15 | 1700/3602 batches | lr 0.0000 | ms/batch 92.62 | loss 12.80 | mse 12.80 | mre  0.00 |
scGPT - INFO - | epoch  15 | 1800/3602 batches | lr 0.0000 | ms/batch 92.79 | loss 12.00 | mse 12.00 | mre  0.00 |
scGPT - INFO - | epoch  15 | 1900/3602 batches | lr 0.0000 | ms/batch 92.74 | loss 15.04 | mse 15.04 | mre  0.00 |
scGPT - INFO - | epoch  15 | 2000/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 13.01 | mse 13.01 | mre  0.00 |
scGPT - INFO - | epoch  15 | 2100/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 14.99 | mse 14.99 | mre  0.00 |
scGPT - INFO - | epoch  15 | 2200/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 12.75 | mse 12.75 | mre  0.00 |
scGPT - INFO - | epoch  15 | 2300/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 12.24 | mse 12.24 | mre  0.00 |
scGPT - INFO - | epoch  15 | 2400/3602 batches | lr 0.0000 | ms/batch 91.35 | loss 12.75 | mse 12.75 | mre  0.00 |
scGPT - INFO - | epoch  15 | 2500/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 11.99 | mse 11.99 | mre  0.00 |
scGPT - INFO - | epoch  15 | 2600/3602 batches | lr 0.0000 | ms/batch 91.23 | loss 15.59 | mse 15.59 | mre  0.00 |
scGPT - INFO - | epoch  15 | 2700/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 12.90 | mse 12.90 | mre  0.00 |
scGPT - INFO - | epoch  15 | 2800/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 13.66 | mse 13.66 | mre  0.00 |
scGPT - INFO - | epoch  15 | 2900/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 13.38 | mse 13.38 | mre  0.00 |
scGPT - INFO - | epoch  15 | 3000/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 13.14 | mse 13.14 | mre  0.00 |
scGPT - INFO - | epoch  15 | 3100/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 12.39 | mse 12.39 | mre  0.00 |
scGPT - INFO - | epoch  15 | 3200/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 13.01 | mse 13.01 | mre  0.00 |
scGPT - INFO - | epoch  15 | 3300/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 11.13 | mse 11.13 | mre  0.00 |
scGPT - INFO - | epoch  15 | 3400/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 12.24 | mse 12.24 | mre  0.00 |
scGPT - INFO - | epoch  15 | 3500/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 12.01 | mse 12.01 | mre  0.00 |
scGPT - INFO - | epoch  15 | 3600/3602 batches | lr 0.0000 | ms/batch 91.27 | loss 14.24 | mse 14.24 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  15 | time: 342.83s | valid loss/mse 13.0372 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 13.0372
best_loss: 13.093287881691655, min_delta 0.0001, val_loss 13.037153130911113
Loss error: 0.056134750780541864
epoch:  15
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[5.1   3.818 3.607 ... 3.734 3.73  3.73 ]
 [6.984 3.674 3.396 ... 3.525 3.709 3.959]
 [4.62  3.69  3.883 ... 3.992 3.715 3.545]
 ...
 [4.85  3.79  3.91  ... 4.094 3.758 3.598]
 [4.918 3.85  4.    ... 4.15  3.828 3.64 ]
 [6.715 3.682 3.363 ... 3.547 3.682 3.938]]
(801, 11) (801, 11)
By feature:  MSE:  0.5926¬±0.0 MAE:  0.5072¬±0.0 R2:  0.2164¬±0.0 PCC:  0.7756¬±0.0 Cosine Similarity:  0.9893¬±0.0
By sample:  MSE:  0.5926¬±0.0 MAE:  0.5072¬±0.0 R2:  0.2635¬±0.0 PCC:  0.4968¬±0.0 Cosine Similarity:  0.9849¬±0.0
scGPT - INFO - By feature: MSE: 0.5925999879837036¬±0.0 MAE: 0.5072000026702881¬±0.0 R2: 0.2164¬±0.0 PCC: 0.7756¬±0.0 Cosine Similarity: 0.9893¬±0.0
scGPT - INFO - By sample: MSE: 0.5925999879837036¬±0.0 MAE: 0.5072000026702881¬±0.0 R2: 0.2635¬±0.0 PCC: 0.4968¬±0.0 Cosine Similarity: 0.9849¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  16
Training model
scGPT - INFO - | epoch  16 | 100/3602 batches | lr 0.0000 | ms/batch 91.92 | loss 12.58 | mse 12.58 | mre  0.00 |
scGPT - INFO - | epoch  16 | 200/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 13.52 | mse 13.52 | mre  0.00 |
scGPT - INFO - | epoch  16 | 300/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 11.52 | mse 11.52 | mre  0.00 |
scGPT - INFO - | epoch  16 | 400/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 11.96 | mse 11.96 | mre  0.00 |
scGPT - INFO - | epoch  16 | 500/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 11.20 | mse 11.20 | mre  0.00 |
scGPT - INFO - | epoch  16 | 600/3602 batches | lr 0.0000 | ms/batch 92.82 | loss 12.19 | mse 12.19 | mre  0.00 |
scGPT - INFO - | epoch  16 | 700/3602 batches | lr 0.0000 | ms/batch 92.75 | loss 12.65 | mse 12.65 | mre  0.00 |
scGPT - INFO - | epoch  16 | 800/3602 batches | lr 0.0000 | ms/batch 92.69 | loss 15.44 | mse 15.44 | mre  0.00 |
scGPT - INFO - | epoch  16 | 900/3602 batches | lr 0.0000 | ms/batch 92.79 | loss 12.87 | mse 12.87 | mre  0.00 |
scGPT - INFO - | epoch  16 | 1000/3602 batches | lr 0.0000 | ms/batch 93.41 | loss 13.91 | mse 13.91 | mre  0.00 |
scGPT - INFO - | epoch  16 | 1100/3602 batches | lr 0.0000 | ms/batch 92.85 | loss 12.26 | mse 12.26 | mre  0.00 |
scGPT - INFO - | epoch  16 | 1200/3602 batches | lr 0.0000 | ms/batch 92.79 | loss 12.88 | mse 12.88 | mre  0.00 |
scGPT - INFO - | epoch  16 | 1300/3602 batches | lr 0.0000 | ms/batch 92.77 | loss 13.12 | mse 13.12 | mre  0.00 |
scGPT - INFO - | epoch  16 | 1400/3602 batches | lr 0.0000 | ms/batch 92.74 | loss 14.76 | mse 14.76 | mre  0.00 |
scGPT - INFO - | epoch  16 | 1500/3602 batches | lr 0.0000 | ms/batch 92.78 | loss 14.64 | mse 14.64 | mre  0.00 |
scGPT - INFO - | epoch  16 | 1600/3602 batches | lr 0.0000 | ms/batch 92.76 | loss 11.59 | mse 11.59 | mre  0.00 |
scGPT - INFO - | epoch  16 | 1700/3602 batches | lr 0.0000 | ms/batch 92.72 | loss 12.51 | mse 12.51 | mre  0.00 |
scGPT - INFO - | epoch  16 | 1800/3602 batches | lr 0.0000 | ms/batch 92.76 | loss 12.00 | mse 12.00 | mre  0.00 |
scGPT - INFO - | epoch  16 | 1900/3602 batches | lr 0.0000 | ms/batch 91.77 | loss 14.43 | mse 14.43 | mre  0.00 |
scGPT - INFO - | epoch  16 | 2000/3602 batches | lr 0.0000 | ms/batch 91.21 | loss 12.38 | mse 12.38 | mre  0.00 |
scGPT - INFO - | epoch  16 | 2100/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 14.63 | mse 14.63 | mre  0.00 |
scGPT - INFO - | epoch  16 | 2200/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 12.30 | mse 12.30 | mre  0.00 |
scGPT - INFO - | epoch  16 | 2300/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 12.54 | mse 12.54 | mre  0.00 |
scGPT - INFO - | epoch  16 | 2400/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 12.81 | mse 12.81 | mre  0.00 |
scGPT - INFO - | epoch  16 | 2500/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 12.12 | mse 12.12 | mre  0.00 |
scGPT - INFO - | epoch  16 | 2600/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 15.79 | mse 15.79 | mre  0.00 |
scGPT - INFO - | epoch  16 | 2700/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 12.77 | mse 12.77 | mre  0.00 |
scGPT - INFO - | epoch  16 | 2800/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 13.18 | mse 13.18 | mre  0.00 |
scGPT - INFO - | epoch  16 | 2900/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 13.39 | mse 13.39 | mre  0.00 |
scGPT - INFO - | epoch  16 | 3000/3602 batches | lr 0.0000 | ms/batch 91.25 | loss 13.56 | mse 13.56 | mre  0.00 |
scGPT - INFO - | epoch  16 | 3100/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 12.32 | mse 12.32 | mre  0.00 |
scGPT - INFO - | epoch  16 | 3200/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 13.24 | mse 13.24 | mre  0.00 |
scGPT - INFO - | epoch  16 | 3300/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 11.44 | mse 11.44 | mre  0.00 |
scGPT - INFO - | epoch  16 | 3400/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 12.27 | mse 12.27 | mre  0.00 |
scGPT - INFO - | epoch  16 | 3500/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 11.81 | mse 11.81 | mre  0.00 |
scGPT - INFO - | epoch  16 | 3600/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 13.40 | mse 13.40 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  16 | time: 342.05s | valid loss/mse 12.9970 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.9970
best_loss: 13.037153130911113, min_delta 0.0001, val_loss 12.997015518790922
Loss error: 0.040137612120190624
epoch:  16
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[5.484 3.705 3.473 ... 3.75  3.695 3.777]
 [7.023 3.65  3.373 ... 3.523 3.705 3.959]
 [4.684 3.66  3.846 ... 4.03  3.715 3.568]
 ...
 [4.8   3.727 3.838 ... 4.043 3.709 3.555]
 [4.883 3.799 3.926 ... 4.105 3.79  3.605]
 [6.867 3.625 3.297 ... 3.559 3.66  3.947]]
(801, 11) (801, 11)
By feature:  MSE:  0.5908¬±0.0 MAE:  0.506¬±0.0 R2:  0.2067¬±0.0 PCC:  0.7825¬±0.0 Cosine Similarity:  0.9895¬±0.0
By sample:  MSE:  0.5908¬±0.0 MAE:  0.506¬±0.0 R2:  0.2644¬±0.0 PCC:  0.4988¬±0.0 Cosine Similarity:  0.985¬±0.0
scGPT - INFO - By feature: MSE: 0.5907999873161316¬±0.0 MAE: 0.5059999823570251¬±0.0 R2: 0.2067¬±0.0 PCC: 0.7825¬±0.0 Cosine Similarity: 0.9895¬±0.0
scGPT - INFO - By sample: MSE: 0.5907999873161316¬±0.0 MAE: 0.5059999823570251¬±0.0 R2: 0.2644¬±0.0 PCC: 0.4988¬±0.0 Cosine Similarity: 0.985¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  17
Training model
scGPT - INFO - | epoch  17 | 100/3602 batches | lr 0.0000 | ms/batch 91.88 | loss 12.44 | mse 12.44 | mre  0.00 |
scGPT - INFO - | epoch  17 | 200/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 13.48 | mse 13.48 | mre  0.00 |
scGPT - INFO - | epoch  17 | 300/3602 batches | lr 0.0000 | ms/batch 91.02 | loss 11.03 | mse 11.03 | mre  0.00 |
scGPT - INFO - | epoch  17 | 400/3602 batches | lr 0.0000 | ms/batch 93.23 | loss 11.96 | mse 11.96 | mre  0.00 |
scGPT - INFO - | epoch  17 | 500/3602 batches | lr 0.0000 | ms/batch 92.65 | loss 11.42 | mse 11.42 | mre  0.00 |
scGPT - INFO - | epoch  17 | 600/3602 batches | lr 0.0000 | ms/batch 92.74 | loss 11.98 | mse 11.98 | mre  0.00 |
scGPT - INFO - | epoch  17 | 700/3602 batches | lr 0.0000 | ms/batch 92.71 | loss 12.62 | mse 12.62 | mre  0.00 |
scGPT - INFO - | epoch  17 | 800/3602 batches | lr 0.0000 | ms/batch 92.68 | loss 15.03 | mse 15.03 | mre  0.00 |
scGPT - INFO - | epoch  17 | 900/3602 batches | lr 0.0000 | ms/batch 92.82 | loss 12.30 | mse 12.30 | mre  0.00 |
scGPT - INFO - | epoch  17 | 1000/3602 batches | lr 0.0000 | ms/batch 92.82 | loss 13.82 | mse 13.82 | mre  0.00 |
scGPT - INFO - | epoch  17 | 1100/3602 batches | lr 0.0000 | ms/batch 92.85 | loss 12.44 | mse 12.44 | mre  0.00 |
scGPT - INFO - | epoch  17 | 1200/3602 batches | lr 0.0000 | ms/batch 92.74 | loss 12.87 | mse 12.87 | mre  0.00 |
scGPT - INFO - | epoch  17 | 1300/3602 batches | lr 0.0000 | ms/batch 92.75 | loss 12.84 | mse 12.84 | mre  0.00 |
scGPT - INFO - | epoch  17 | 1400/3602 batches | lr 0.0000 | ms/batch 93.25 | loss 14.82 | mse 14.82 | mre  0.00 |
scGPT - INFO - | epoch  17 | 1500/3602 batches | lr 0.0000 | ms/batch 92.70 | loss 14.63 | mse 14.63 | mre  0.00 |
scGPT - INFO - | epoch  17 | 1600/3602 batches | lr 0.0000 | ms/batch 92.73 | loss 11.35 | mse 11.35 | mre  0.00 |
scGPT - INFO - | epoch  17 | 1700/3602 batches | lr 0.0000 | ms/batch 92.76 | loss 11.98 | mse 11.98 | mre  0.00 |
scGPT - INFO - | epoch  17 | 1800/3602 batches | lr 0.0000 | ms/batch 92.69 | loss 11.90 | mse 11.90 | mre  0.00 |
scGPT - INFO - | epoch  17 | 1900/3602 batches | lr 0.0000 | ms/batch 92.69 | loss 14.48 | mse 14.48 | mre  0.00 |
scGPT - INFO - | epoch  17 | 2000/3602 batches | lr 0.0000 | ms/batch 92.79 | loss 12.82 | mse 12.82 | mre  0.00 |
scGPT - INFO - | epoch  17 | 2100/3602 batches | lr 0.0000 | ms/batch 92.79 | loss 14.38 | mse 14.38 | mre  0.00 |
scGPT - INFO - | epoch  17 | 2200/3602 batches | lr 0.0000 | ms/batch 92.81 | loss 11.97 | mse 11.97 | mre  0.00 |
scGPT - INFO - | epoch  17 | 2300/3602 batches | lr 0.0000 | ms/batch 92.73 | loss 12.56 | mse 12.56 | mre  0.00 |
scGPT - INFO - | epoch  17 | 2400/3602 batches | lr 0.0000 | ms/batch 93.32 | loss 12.33 | mse 12.33 | mre  0.00 |
scGPT - INFO - | epoch  17 | 2500/3602 batches | lr 0.0000 | ms/batch 92.78 | loss 12.23 | mse 12.23 | mre  0.00 |
scGPT - INFO - | epoch  17 | 2600/3602 batches | lr 0.0000 | ms/batch 92.73 | loss 16.15 | mse 16.15 | mre  0.00 |
scGPT - INFO - | epoch  17 | 2700/3602 batches | lr 0.0000 | ms/batch 92.72 | loss 12.28 | mse 12.28 | mre  0.00 |
scGPT - INFO - | epoch  17 | 2800/3602 batches | lr 0.0000 | ms/batch 92.79 | loss 13.26 | mse 13.26 | mre  0.00 |
scGPT - INFO - | epoch  17 | 2900/3602 batches | lr 0.0000 | ms/batch 92.78 | loss 13.23 | mse 13.23 | mre  0.00 |
scGPT - INFO - | epoch  17 | 3000/3602 batches | lr 0.0000 | ms/batch 92.79 | loss 13.14 | mse 13.14 | mre  0.00 |
scGPT - INFO - | epoch  17 | 3100/3602 batches | lr 0.0000 | ms/batch 92.71 | loss 12.32 | mse 12.32 | mre  0.00 |
scGPT - INFO - | epoch  17 | 3200/3602 batches | lr 0.0000 | ms/batch 92.76 | loss 12.97 | mse 12.97 | mre  0.00 |
scGPT - INFO - | epoch  17 | 3300/3602 batches | lr 0.0000 | ms/batch 92.81 | loss 11.25 | mse 11.25 | mre  0.00 |
scGPT - INFO - | epoch  17 | 3400/3602 batches | lr 0.0000 | ms/batch 93.23 | loss 12.33 | mse 12.33 | mre  0.00 |
scGPT - INFO - | epoch  17 | 3500/3602 batches | lr 0.0000 | ms/batch 92.83 | loss 11.46 | mse 11.46 | mre  0.00 |
scGPT - INFO - | epoch  17 | 3600/3602 batches | lr 0.0000 | ms/batch 92.78 | loss 13.66 | mse 13.66 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  17 | time: 346.14s | valid loss/mse 12.8996 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.8996
best_loss: 12.997015518790922, min_delta 0.0001, val_loss 12.899632474679626
Loss error: 0.09738304411129661
epoch:  17
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[5.793 3.684 3.443 ... 3.69  3.686 3.838]
 [7.016 3.658 3.395 ... 3.531 3.709 3.969]
 [4.62  3.65  3.842 ... 3.992 3.705 3.555]
 ...
 [4.777 3.734 3.855 ... 4.035 3.709 3.566]
 [4.902 3.84  3.977 ... 4.117 3.822 3.639]
 [6.875 3.64  3.316 ... 3.562 3.662 3.957]]
(801, 11) (801, 11)
By feature:  MSE:  0.5864¬±0.0 MAE:  0.503¬±0.0 R2:  0.2158¬±0.0 PCC:  0.7869¬±0.0 Cosine Similarity:  0.9896¬±0.0
By sample:  MSE:  0.5864¬±0.0 MAE:  0.503¬±0.0 R2:  0.2684¬±0.0 PCC:  0.5013¬±0.0 Cosine Similarity:  0.9851¬±0.0
scGPT - INFO - By feature: MSE: 0.5863999724388123¬±0.0 MAE: 0.503000020980835¬±0.0 R2: 0.2158¬±0.0 PCC: 0.7869¬±0.0 Cosine Similarity: 0.9896¬±0.0
scGPT - INFO - By sample: MSE: 0.5863999724388123¬±0.0 MAE: 0.503000020980835¬±0.0 R2: 0.2684¬±0.0 PCC: 0.5013¬±0.0 Cosine Similarity: 0.9851¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  18
Training model
scGPT - INFO - | epoch  18 | 100/3602 batches | lr 0.0000 | ms/batch 91.84 | loss 12.64 | mse 12.64 | mre  0.00 |
scGPT - INFO - | epoch  18 | 200/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 13.24 | mse 13.24 | mre  0.00 |
scGPT - INFO - | epoch  18 | 300/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 11.04 | mse 11.04 | mre  0.00 |
scGPT - INFO - | epoch  18 | 400/3602 batches | lr 0.0000 | ms/batch 90.62 | loss 11.50 | mse 11.50 | mre  0.00 |
scGPT - INFO - | epoch  18 | 500/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 10.91 | mse 10.91 | mre  0.00 |
scGPT - INFO - | epoch  18 | 600/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 11.88 | mse 11.88 | mre  0.00 |
scGPT - INFO - | epoch  18 | 700/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 12.64 | mse 12.64 | mre  0.00 |
scGPT - INFO - | epoch  18 | 800/3602 batches | lr 0.0000 | ms/batch 91.18 | loss 15.47 | mse 15.47 | mre  0.00 |
scGPT - INFO - | epoch  18 | 900/3602 batches | lr 0.0000 | ms/batch 90.60 | loss 12.26 | mse 12.26 | mre  0.00 |
scGPT - INFO - | epoch  18 | 1000/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 14.06 | mse 14.06 | mre  0.00 |
scGPT - INFO - | epoch  18 | 1100/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 11.92 | mse 11.92 | mre  0.00 |
scGPT - INFO - | epoch  18 | 1200/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 12.70 | mse 12.70 | mre  0.00 |
scGPT - INFO - | epoch  18 | 1300/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 12.59 | mse 12.59 | mre  0.00 |
scGPT - INFO - | epoch  18 | 1400/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 14.38 | mse 14.38 | mre  0.00 |
scGPT - INFO - | epoch  18 | 1500/3602 batches | lr 0.0000 | ms/batch 90.60 | loss 14.64 | mse 14.64 | mre  0.00 |
scGPT - INFO - | epoch  18 | 1600/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 11.43 | mse 11.43 | mre  0.00 |
scGPT - INFO - | epoch  18 | 1700/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 12.32 | mse 12.32 | mre  0.00 |
scGPT - INFO - | epoch  18 | 1800/3602 batches | lr 0.0000 | ms/batch 93.86 | loss 11.56 | mse 11.56 | mre  0.00 |
scGPT - INFO - | epoch  18 | 1900/3602 batches | lr 0.0000 | ms/batch 92.56 | loss 14.22 | mse 14.22 | mre  0.00 |
scGPT - INFO - | epoch  18 | 2000/3602 batches | lr 0.0000 | ms/batch 91.40 | loss 12.62 | mse 12.62 | mre  0.00 |
scGPT - INFO - | epoch  18 | 2100/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 14.17 | mse 14.17 | mre  0.00 |
scGPT - INFO - | epoch  18 | 2200/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 12.27 | mse 12.27 | mre  0.00 |
scGPT - INFO - | epoch  18 | 2300/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 12.49 | mse 12.49 | mre  0.00 |
scGPT - INFO - | epoch  18 | 2400/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 12.52 | mse 12.52 | mre  0.00 |
scGPT - INFO - | epoch  18 | 2500/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 11.90 | mse 11.90 | mre  0.00 |
scGPT - INFO - | epoch  18 | 2600/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 15.49 | mse 15.49 | mre  0.00 |
scGPT - INFO - | epoch  18 | 2700/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 12.08 | mse 12.08 | mre  0.00 |
scGPT - INFO - | epoch  18 | 2800/3602 batches | lr 0.0000 | ms/batch 91.28 | loss 12.90 | mse 12.90 | mre  0.00 |
scGPT - INFO - | epoch  18 | 2900/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 12.54 | mse 12.54 | mre  0.00 |
scGPT - INFO - | epoch  18 | 3000/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 12.96 | mse 12.96 | mre  0.00 |
scGPT - INFO - | epoch  18 | 3100/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 12.39 | mse 12.39 | mre  0.00 |
scGPT - INFO - | epoch  18 | 3200/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 13.01 | mse 13.01 | mre  0.00 |
scGPT - INFO - | epoch  18 | 3300/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 11.02 | mse 11.02 | mre  0.00 |
scGPT - INFO - | epoch  18 | 3400/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 11.85 | mse 11.85 | mre  0.00 |
scGPT - INFO - | epoch  18 | 3500/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 11.53 | mse 11.53 | mre  0.00 |
scGPT - INFO - | epoch  18 | 3600/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 13.63 | mse 13.63 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  18 | time: 339.84s | valid loss/mse 12.9242 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.899632474679626, min_delta 0.0001, val_loss 12.924184662721279
Loss error: -0.024552188041653267
scGPT - INFO - INFO: Early stopping counter 1 of 10
epoch:  18
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[5.945 3.664 3.418 ... 3.71  3.682 3.852]
 [7.    3.652 3.396 ... 3.518 3.703 3.957]
 [4.637 3.674 3.852 ... 4.035 3.713 3.566]
 ...
 [4.766 3.78  3.885 ... 4.055 3.729 3.572]
 [4.875 3.861 3.986 ... 4.13  3.822 3.63 ]
 [6.61  3.68  3.342 ... 3.59  3.648 3.926]]
(801, 11) (801, 11)
By feature:  MSE:  0.5875¬±0.0 MAE:  0.5026¬±0.0 R2:  0.2144¬±0.0 PCC:  0.7865¬±0.0 Cosine Similarity:  0.9895¬±0.0
By sample:  MSE:  0.5875¬±0.0 MAE:  0.5026¬±0.0 R2:  0.268¬±0.0 PCC:  0.501¬±0.0 Cosine Similarity:  0.9851¬±0.0
scGPT - INFO - By feature: MSE: 0.5874999761581421¬±0.0 MAE: 0.5026000142097473¬±0.0 R2: 0.2144¬±0.0 PCC: 0.7865¬±0.0 Cosine Similarity: 0.9895¬±0.0
scGPT - INFO - By sample: MSE: 0.5874999761581421¬±0.0 MAE: 0.5026000142097473¬±0.0 R2: 0.268¬±0.0 PCC: 0.501¬±0.0 Cosine Similarity: 0.9851¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  19
Training model
scGPT - INFO - | epoch  19 | 100/3602 batches | lr 0.0000 | ms/batch 91.92 | loss 12.05 | mse 12.05 | mre  0.00 |
scGPT - INFO - | epoch  19 | 200/3602 batches | lr 0.0000 | ms/batch 91.22 | loss 13.40 | mse 13.40 | mre  0.00 |
scGPT - INFO - | epoch  19 | 300/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 10.72 | mse 10.72 | mre  0.00 |
scGPT - INFO - | epoch  19 | 400/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 11.52 | mse 11.52 | mre  0.00 |
scGPT - INFO - | epoch  19 | 500/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 10.76 | mse 10.76 | mre  0.00 |
scGPT - INFO - | epoch  19 | 600/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 12.01 | mse 12.01 | mre  0.00 |
scGPT - INFO - | epoch  19 | 700/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 12.75 | mse 12.75 | mre  0.00 |
scGPT - INFO - | epoch  19 | 800/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 14.88 | mse 14.88 | mre  0.00 |
scGPT - INFO - | epoch  19 | 900/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 12.24 | mse 12.24 | mre  0.00 |
scGPT - INFO - | epoch  19 | 1000/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 13.73 | mse 13.73 | mre  0.00 |
scGPT - INFO - | epoch  19 | 1100/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 12.43 | mse 12.43 | mre  0.00 |
scGPT - INFO - | epoch  19 | 1200/3602 batches | lr 0.0000 | ms/batch 91.23 | loss 12.07 | mse 12.07 | mre  0.00 |
scGPT - INFO - | epoch  19 | 1300/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 12.57 | mse 12.57 | mre  0.00 |
scGPT - INFO - | epoch  19 | 1400/3602 batches | lr 0.0000 | ms/batch 90.93 | loss 14.45 | mse 14.45 | mre  0.00 |
scGPT - INFO - | epoch  19 | 1500/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 15.15 | mse 15.15 | mre  0.00 |
scGPT - INFO - | epoch  19 | 1600/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 10.82 | mse 10.82 | mre  0.00 |
scGPT - INFO - | epoch  19 | 1700/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 11.92 | mse 11.92 | mre  0.00 |
scGPT - INFO - | epoch  19 | 1800/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 11.73 | mse 11.73 | mre  0.00 |
scGPT - INFO - | epoch  19 | 1900/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 14.56 | mse 14.56 | mre  0.00 |
scGPT - INFO - | epoch  19 | 2000/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 12.20 | mse 12.20 | mre  0.00 |
scGPT - INFO - | epoch  19 | 2100/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 14.13 | mse 14.13 | mre  0.00 |
scGPT - INFO - | epoch  19 | 2200/3602 batches | lr 0.0000 | ms/batch 91.30 | loss 11.77 | mse 11.77 | mre  0.00 |
scGPT - INFO - | epoch  19 | 2300/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 12.11 | mse 12.11 | mre  0.00 |
scGPT - INFO - | epoch  19 | 2400/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 12.09 | mse 12.09 | mre  0.00 |
scGPT - INFO - | epoch  19 | 2500/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 11.94 | mse 11.94 | mre  0.00 |
scGPT - INFO - | epoch  19 | 2600/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 15.22 | mse 15.22 | mre  0.00 |
scGPT - INFO - | epoch  19 | 2700/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 12.18 | mse 12.18 | mre  0.00 |
scGPT - INFO - | epoch  19 | 2800/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 12.70 | mse 12.70 | mre  0.00 |
scGPT - INFO - | epoch  19 | 2900/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 13.27 | mse 13.27 | mre  0.00 |
scGPT - INFO - | epoch  19 | 3000/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 13.11 | mse 13.11 | mre  0.00 |
scGPT - INFO - | epoch  19 | 3100/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 11.97 | mse 11.97 | mre  0.00 |
scGPT - INFO - | epoch  19 | 3200/3602 batches | lr 0.0000 | ms/batch 91.28 | loss 12.60 | mse 12.60 | mre  0.00 |
scGPT - INFO - | epoch  19 | 3300/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 10.75 | mse 10.75 | mre  0.00 |
scGPT - INFO - | epoch  19 | 3400/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 12.10 | mse 12.10 | mre  0.00 |
scGPT - INFO - | epoch  19 | 3500/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 11.12 | mse 11.12 | mre  0.00 |
scGPT - INFO - | epoch  19 | 3600/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 13.24 | mse 13.24 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  19 | time: 339.44s | valid loss/mse 12.9252 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.899632474679626, min_delta 0.0001, val_loss 12.925187956974302
Loss error: -0.02555548229467597
scGPT - INFO - INFO: Early stopping counter 2 of 10
epoch:  19
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.145 3.646 3.406 ... 3.684 3.688 3.896]
 [7.027 3.662 3.402 ... 3.512 3.71  3.97 ]
 [4.703 3.676 3.885 ... 4.055 3.74  3.596]
 ...
 [4.836 3.793 3.924 ... 4.07  3.762 3.6  ]
 [4.938 3.871 4.02  ... 4.145 3.852 3.652]
 [6.887 3.654 3.322 ... 3.55  3.662 3.969]]
(801, 11) (801, 11)
By feature:  MSE:  0.5876¬±0.0 MAE:  0.5063¬±0.0 R2:  0.217¬±0.0 PCC:  0.7883¬±0.0 Cosine Similarity:  0.9896¬±0.0
By sample:  MSE:  0.5876¬±0.0 MAE:  0.5063¬±0.0 R2:  0.2652¬±0.0 PCC:  0.4992¬±0.0 Cosine Similarity:  0.9851¬±0.0
scGPT - INFO - By feature: MSE: 0.5875999927520752¬±0.0 MAE: 0.5062999725341797¬±0.0 R2: 0.217¬±0.0 PCC: 0.7883¬±0.0 Cosine Similarity: 0.9896¬±0.0
scGPT - INFO - By sample: MSE: 0.5875999927520752¬±0.0 MAE: 0.5062999725341797¬±0.0 R2: 0.2652¬±0.0 PCC: 0.4992¬±0.0 Cosine Similarity: 0.9851¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  20
Training model
scGPT - INFO - | epoch  20 | 100/3602 batches | lr 0.0000 | ms/batch 92.12 | loss 11.83 | mse 11.83 | mre  0.00 |
scGPT - INFO - | epoch  20 | 200/3602 batches | lr 0.0000 | ms/batch 91.00 | loss 12.94 | mse 12.94 | mre  0.00 |
scGPT - INFO - | epoch  20 | 300/3602 batches | lr 0.0000 | ms/batch 90.93 | loss 10.76 | mse 10.76 | mre  0.00 |
scGPT - INFO - | epoch  20 | 400/3602 batches | lr 0.0000 | ms/batch 90.95 | loss 11.70 | mse 11.70 | mre  0.00 |
scGPT - INFO - | epoch  20 | 500/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 10.39 | mse 10.39 | mre  0.00 |
scGPT - INFO - | epoch  20 | 600/3602 batches | lr 0.0000 | ms/batch 91.51 | loss 11.74 | mse 11.74 | mre  0.00 |
scGPT - INFO - | epoch  20 | 700/3602 batches | lr 0.0000 | ms/batch 90.95 | loss 12.27 | mse 12.27 | mre  0.00 |
scGPT - INFO - | epoch  20 | 800/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 14.89 | mse 14.89 | mre  0.00 |
scGPT - INFO - | epoch  20 | 900/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 12.23 | mse 12.23 | mre  0.00 |
scGPT - INFO - | epoch  20 | 1000/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 14.05 | mse 14.05 | mre  0.00 |
scGPT - INFO - | epoch  20 | 1100/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 11.94 | mse 11.94 | mre  0.00 |
scGPT - INFO - | epoch  20 | 1200/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 12.00 | mse 12.00 | mre  0.00 |
scGPT - INFO - | epoch  20 | 1300/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 12.10 | mse 12.10 | mre  0.00 |
scGPT - INFO - | epoch  20 | 1400/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 14.24 | mse 14.24 | mre  0.00 |
scGPT - INFO - | epoch  20 | 1500/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 14.14 | mse 14.14 | mre  0.00 |
scGPT - INFO - | epoch  20 | 1600/3602 batches | lr 0.0000 | ms/batch 91.42 | loss 11.17 | mse 11.17 | mre  0.00 |
scGPT - INFO - | epoch  20 | 1700/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 12.01 | mse 12.01 | mre  0.00 |
scGPT - INFO - | epoch  20 | 1800/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 11.63 | mse 11.63 | mre  0.00 |
scGPT - INFO - | epoch  20 | 1900/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 14.64 | mse 14.64 | mre  0.00 |
scGPT - INFO - | epoch  20 | 2000/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 12.28 | mse 12.28 | mre  0.00 |
scGPT - INFO - | epoch  20 | 2100/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 14.01 | mse 14.01 | mre  0.00 |
scGPT - INFO - | epoch  20 | 2200/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 11.89 | mse 11.89 | mre  0.00 |
scGPT - INFO - | epoch  20 | 2300/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 12.04 | mse 12.04 | mre  0.00 |
scGPT - INFO - | epoch  20 | 2400/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 12.35 | mse 12.35 | mre  0.00 |
scGPT - INFO - | epoch  20 | 2500/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 11.82 | mse 11.82 | mre  0.00 |
scGPT - INFO - | epoch  20 | 2600/3602 batches | lr 0.0000 | ms/batch 91.46 | loss 15.49 | mse 15.49 | mre  0.00 |
scGPT - INFO - | epoch  20 | 2700/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 11.76 | mse 11.76 | mre  0.00 |
scGPT - INFO - | epoch  20 | 2800/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 12.66 | mse 12.66 | mre  0.00 |
scGPT - INFO - | epoch  20 | 2900/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 13.02 | mse 13.02 | mre  0.00 |
scGPT - INFO - | epoch  20 | 3000/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 12.91 | mse 12.91 | mre  0.00 |
scGPT - INFO - | epoch  20 | 3100/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 12.16 | mse 12.16 | mre  0.00 |
scGPT - INFO - | epoch  20 | 3200/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 12.33 | mse 12.33 | mre  0.00 |
scGPT - INFO - | epoch  20 | 3300/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 10.67 | mse 10.67 | mre  0.00 |
scGPT - INFO - | epoch  20 | 3400/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 11.78 | mse 11.78 | mre  0.00 |
scGPT - INFO - | epoch  20 | 3500/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 11.54 | mse 11.54 | mre  0.00 |
scGPT - INFO - | epoch  20 | 3600/3602 batches | lr 0.0000 | ms/batch 91.37 | loss 13.05 | mse 13.05 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  20 | time: 339.93s | valid loss/mse 12.9001 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.899632474679626, min_delta 0.0001, val_loss 12.900124731284105
Loss error: -0.0004922566044793797
scGPT - INFO - INFO: Early stopping counter 3 of 10
epoch:  20
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.52  3.592 3.344 ... 3.602 3.643 3.918]
 [6.996 3.654 3.41  ... 3.52  3.7   3.957]
 [4.652 3.637 3.857 ... 4.04  3.719 3.572]
 ...
 [4.793 3.771 3.904 ... 4.066 3.744 3.588]
 [4.902 3.85  4.    ... 4.133 3.834 3.637]
 [6.81  3.65  3.312 ... 3.562 3.639 3.94 ]]
(801, 11) (801, 11)
By feature:  MSE:  0.5864¬±0.0 MAE:  0.5024¬±0.0 R2:  0.2152¬±0.0 PCC:  0.7898¬±0.0 Cosine Similarity:  0.9896¬±0.0
By sample:  MSE:  0.5864¬±0.0 MAE:  0.5024¬±0.0 R2:  0.2679¬±0.0 PCC:  0.5021¬±0.0 Cosine Similarity:  0.9851¬±0.0
scGPT - INFO - By feature: MSE: 0.5863999724388123¬±0.0 MAE: 0.5023999810218811¬±0.0 R2: 0.2152¬±0.0 PCC: 0.7898¬±0.0 Cosine Similarity: 0.9896¬±0.0
scGPT - INFO - By sample: MSE: 0.5863999724388123¬±0.0 MAE: 0.5023999810218811¬±0.0 R2: 0.2679¬±0.0 PCC: 0.5021¬±0.0 Cosine Similarity: 0.9851¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  21
Training model
scGPT - INFO - | epoch  21 | 100/3602 batches | lr 0.0000 | ms/batch 92.02 | loss 12.13 | mse 12.13 | mre  0.00 |
scGPT - INFO - | epoch  21 | 200/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 12.88 | mse 12.88 | mre  0.00 |
scGPT - INFO - | epoch  21 | 300/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 10.65 | mse 10.65 | mre  0.00 |
scGPT - INFO - | epoch  21 | 400/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 11.27 | mse 11.27 | mre  0.00 |
scGPT - INFO - | epoch  21 | 500/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 10.52 | mse 10.52 | mre  0.00 |
scGPT - INFO - | epoch  21 | 600/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 11.89 | mse 11.89 | mre  0.00 |
scGPT - INFO - | epoch  21 | 700/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 12.43 | mse 12.43 | mre  0.00 |
scGPT - INFO - | epoch  21 | 800/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 14.80 | mse 14.80 | mre  0.00 |
scGPT - INFO - | epoch  21 | 900/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 12.12 | mse 12.12 | mre  0.00 |
scGPT - INFO - | epoch  21 | 1000/3602 batches | lr 0.0000 | ms/batch 91.38 | loss 13.34 | mse 13.34 | mre  0.00 |
scGPT - INFO - | epoch  21 | 1100/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 11.76 | mse 11.76 | mre  0.00 |
scGPT - INFO - | epoch  21 | 1200/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 12.31 | mse 12.31 | mre  0.00 |
scGPT - INFO - | epoch  21 | 1300/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 11.69 | mse 11.69 | mre  0.00 |
scGPT - INFO - | epoch  21 | 1400/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 13.97 | mse 13.97 | mre  0.00 |
scGPT - INFO - | epoch  21 | 1500/3602 batches | lr 0.0000 | ms/batch 90.62 | loss 14.41 | mse 14.41 | mre  0.00 |
scGPT - INFO - | epoch  21 | 1600/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 10.68 | mse 10.68 | mre  0.00 |
scGPT - INFO - | epoch  21 | 1700/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 11.77 | mse 11.77 | mre  0.00 |
scGPT - INFO - | epoch  21 | 1800/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 11.53 | mse 11.53 | mre  0.00 |
scGPT - INFO - | epoch  21 | 1900/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 14.48 | mse 14.48 | mre  0.00 |
scGPT - INFO - | epoch  21 | 2000/3602 batches | lr 0.0000 | ms/batch 91.26 | loss 12.20 | mse 12.20 | mre  0.00 |
scGPT - INFO - | epoch  21 | 2100/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 14.11 | mse 14.11 | mre  0.00 |
scGPT - INFO - | epoch  21 | 2200/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 11.74 | mse 11.74 | mre  0.00 |
scGPT - INFO - | epoch  21 | 2300/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 11.74 | mse 11.74 | mre  0.00 |
scGPT - INFO - | epoch  21 | 2400/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 11.94 | mse 11.94 | mre  0.00 |
scGPT - INFO - | epoch  21 | 2500/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 11.69 | mse 11.69 | mre  0.00 |
scGPT - INFO - | epoch  21 | 2600/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 15.58 | mse 15.58 | mre  0.00 |
scGPT - INFO - | epoch  21 | 2700/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 12.11 | mse 12.11 | mre  0.00 |
scGPT - INFO - | epoch  21 | 2800/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 13.03 | mse 13.03 | mre  0.00 |
scGPT - INFO - | epoch  21 | 2900/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 12.67 | mse 12.67 | mre  0.00 |
scGPT - INFO - | epoch  21 | 3000/3602 batches | lr 0.0000 | ms/batch 91.23 | loss 12.67 | mse 12.67 | mre  0.00 |
scGPT - INFO - | epoch  21 | 3100/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 11.58 | mse 11.58 | mre  0.00 |
scGPT - INFO - | epoch  21 | 3200/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 12.36 | mse 12.36 | mre  0.00 |
scGPT - INFO - | epoch  21 | 3300/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 10.61 | mse 10.61 | mre  0.00 |
scGPT - INFO - | epoch  21 | 3400/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 11.56 | mse 11.56 | mre  0.00 |
scGPT - INFO - | epoch  21 | 3500/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 11.11 | mse 11.11 | mre  0.00 |
scGPT - INFO - | epoch  21 | 3600/3602 batches | lr 0.0000 | ms/batch 90.61 | loss 13.08 | mse 13.08 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  21 | time: 339.37s | valid loss/mse 12.9243 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.899632474679626, min_delta 0.0001, val_loss 12.92429559641563
Loss error: -0.02466312173600471
scGPT - INFO - INFO: Early stopping counter 4 of 10
epoch:  21
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.598 3.592 3.342 ... 3.598 3.652 3.926]
 [7.02  3.646 3.39  ... 3.518 3.693 3.96 ]
 [4.637 3.625 3.828 ... 4.043 3.7   3.572]
 ...
 [4.773 3.77  3.88  ... 4.06  3.73  3.584]
 [4.89  3.852 3.984 ... 4.137 3.83  3.643]
 [6.902 3.639 3.297 ... 3.545 3.645 3.951]]
(801, 11) (801, 11)
By feature:  MSE:  0.5875¬±0.0 MAE:  0.5025¬±0.0 R2:  0.212¬±0.0 PCC:  0.7913¬±0.0 Cosine Similarity:  0.9896¬±0.0
By sample:  MSE:  0.5875¬±0.0 MAE:  0.5025¬±0.0 R2:  0.2652¬±0.0 PCC:  0.4996¬±0.0 Cosine Similarity:  0.9851¬±0.0
scGPT - INFO - By feature: MSE: 0.5874999761581421¬±0.0 MAE: 0.5024999976158142¬±0.0 R2: 0.212¬±0.0 PCC: 0.7913¬±0.0 Cosine Similarity: 0.9896¬±0.0
scGPT - INFO - By sample: MSE: 0.5874999761581421¬±0.0 MAE: 0.5024999976158142¬±0.0 R2: 0.2652¬±0.0 PCC: 0.4996¬±0.0 Cosine Similarity: 0.9851¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  22
Training model
scGPT - INFO - | epoch  22 | 100/3602 batches | lr 0.0000 | ms/batch 91.97 | loss 11.88 | mse 11.88 | mre  0.00 |
scGPT - INFO - | epoch  22 | 200/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 12.81 | mse 12.81 | mre  0.00 |
scGPT - INFO - | epoch  22 | 300/3602 batches | lr 0.0000 | ms/batch 91.06 | loss 10.75 | mse 10.75 | mre  0.00 |
scGPT - INFO - | epoch  22 | 400/3602 batches | lr 0.0000 | ms/batch 91.37 | loss 11.21 | mse 11.21 | mre  0.00 |
scGPT - INFO - | epoch  22 | 500/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 10.23 | mse 10.23 | mre  0.00 |
scGPT - INFO - | epoch  22 | 600/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 11.86 | mse 11.86 | mre  0.00 |
scGPT - INFO - | epoch  22 | 700/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 12.06 | mse 12.06 | mre  0.00 |
scGPT - INFO - | epoch  22 | 800/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 14.89 | mse 14.89 | mre  0.00 |
scGPT - INFO - | epoch  22 | 900/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 12.26 | mse 12.26 | mre  0.00 |
scGPT - INFO - | epoch  22 | 1000/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 13.06 | mse 13.06 | mre  0.00 |
scGPT - INFO - | epoch  22 | 1100/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 11.60 | mse 11.60 | mre  0.00 |
scGPT - INFO - | epoch  22 | 1200/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 11.61 | mse 11.61 | mre  0.00 |
scGPT - INFO - | epoch  22 | 1300/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 11.97 | mse 11.97 | mre  0.00 |
scGPT - INFO - | epoch  22 | 1400/3602 batches | lr 0.0000 | ms/batch 91.41 | loss 13.91 | mse 13.91 | mre  0.00 |
scGPT - INFO - | epoch  22 | 1500/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 14.46 | mse 14.46 | mre  0.00 |
scGPT - INFO - | epoch  22 | 1600/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 10.37 | mse 10.37 | mre  0.00 |
scGPT - INFO - | epoch  22 | 1700/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 11.69 | mse 11.69 | mre  0.00 |
scGPT - INFO - | epoch  22 | 1800/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 11.45 | mse 11.45 | mre  0.00 |
scGPT - INFO - | epoch  22 | 1900/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 14.41 | mse 14.41 | mre  0.00 |
scGPT - INFO - | epoch  22 | 2000/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 12.01 | mse 12.01 | mre  0.00 |
scGPT - INFO - | epoch  22 | 2100/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 13.65 | mse 13.65 | mre  0.00 |
scGPT - INFO - | epoch  22 | 2200/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 11.75 | mse 11.75 | mre  0.00 |
scGPT - INFO - | epoch  22 | 2300/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 11.34 | mse 11.34 | mre  0.00 |
scGPT - INFO - | epoch  22 | 2400/3602 batches | lr 0.0000 | ms/batch 91.18 | loss 12.44 | mse 12.44 | mre  0.00 |
scGPT - INFO - | epoch  22 | 2500/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 11.52 | mse 11.52 | mre  0.00 |
scGPT - INFO - | epoch  22 | 2600/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 15.22 | mse 15.22 | mre  0.00 |
scGPT - INFO - | epoch  22 | 2700/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 11.37 | mse 11.37 | mre  0.00 |
scGPT - INFO - | epoch  22 | 2800/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 12.93 | mse 12.93 | mre  0.00 |
scGPT - INFO - | epoch  22 | 2900/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 12.62 | mse 12.62 | mre  0.00 |
scGPT - INFO - | epoch  22 | 3000/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 12.08 | mse 12.08 | mre  0.00 |
scGPT - INFO - | epoch  22 | 3100/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 11.46 | mse 11.46 | mre  0.00 |
scGPT - INFO - | epoch  22 | 3200/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 12.26 | mse 12.26 | mre  0.00 |
scGPT - INFO - | epoch  22 | 3300/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 10.43 | mse 10.43 | mre  0.00 |
scGPT - INFO - | epoch  22 | 3400/3602 batches | lr 0.0000 | ms/batch 91.21 | loss 10.93 | mse 10.93 | mre  0.00 |
scGPT - INFO - | epoch  22 | 3500/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 11.02 | mse 11.02 | mre  0.00 |
scGPT - INFO - | epoch  22 | 3600/3602 batches | lr 0.0000 | ms/batch 90.61 | loss 12.80 | mse 12.80 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  22 | time: 339.47s | valid loss/mse 12.9265 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.899632474679626, min_delta 0.0001, val_loss 12.926481392350833
Loss error: -0.026848917671207673
scGPT - INFO - INFO: Early stopping counter 5 of 10
epoch:  22
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.67  3.568 3.338 ... 3.598 3.654 3.932]
 [7.086 3.63  3.39  ... 3.512 3.695 3.963]
 [4.63  3.607 3.838 ... 4.05  3.68  3.557]
 ...
 [4.76  3.76  3.89  ... 4.05  3.729 3.572]
 [4.89  3.852 4.    ... 4.137 3.832 3.635]
 [6.844 3.646 3.3   ... 3.572 3.635 3.94 ]]
(801, 11) (801, 11)
By feature:  MSE:  0.5876¬±0.0 MAE:  0.5032¬±0.0 R2:  0.2194¬±0.0 PCC:  0.7921¬±0.0 Cosine Similarity:  0.9896¬±0.0
By sample:  MSE:  0.5876¬±0.0 MAE:  0.5032¬±0.0 R2:  0.2671¬±0.0 PCC:  0.5026¬±0.0 Cosine Similarity:  0.9851¬±0.0
scGPT - INFO - By feature: MSE: 0.5875999927520752¬±0.0 MAE: 0.5031999945640564¬±0.0 R2: 0.2194¬±0.0 PCC: 0.7921¬±0.0 Cosine Similarity: 0.9896¬±0.0
scGPT - INFO - By sample: MSE: 0.5875999927520752¬±0.0 MAE: 0.5031999945640564¬±0.0 R2: 0.2671¬±0.0 PCC: 0.5026¬±0.0 Cosine Similarity: 0.9851¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  23
Training model
scGPT - INFO - | epoch  23 | 100/3602 batches | lr 0.0000 | ms/batch 91.85 | loss 11.66 | mse 11.66 | mre  0.00 |
scGPT - INFO - | epoch  23 | 200/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 12.79 | mse 12.79 | mre  0.00 |
scGPT - INFO - | epoch  23 | 300/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 10.36 | mse 10.36 | mre  0.00 |
scGPT - INFO - | epoch  23 | 400/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 11.29 | mse 11.29 | mre  0.00 |
scGPT - INFO - | epoch  23 | 500/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 10.14 | mse 10.14 | mre  0.00 |
scGPT - INFO - | epoch  23 | 600/3602 batches | lr 0.0000 | ms/batch 90.56 | loss 11.27 | mse 11.27 | mre  0.00 |
scGPT - INFO - | epoch  23 | 700/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 12.06 | mse 12.06 | mre  0.00 |
scGPT - INFO - | epoch  23 | 800/3602 batches | lr 0.0000 | ms/batch 91.29 | loss 14.77 | mse 14.77 | mre  0.00 |
scGPT - INFO - | epoch  23 | 900/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 11.86 | mse 11.86 | mre  0.00 |
scGPT - INFO - | epoch  23 | 1000/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 13.18 | mse 13.18 | mre  0.00 |
scGPT - INFO - | epoch  23 | 1100/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 11.65 | mse 11.65 | mre  0.00 |
scGPT - INFO - | epoch  23 | 1200/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 11.52 | mse 11.52 | mre  0.00 |
scGPT - INFO - | epoch  23 | 1300/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 11.70 | mse 11.70 | mre  0.00 |
scGPT - INFO - | epoch  23 | 1400/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 13.81 | mse 13.81 | mre  0.00 |
scGPT - INFO - | epoch  23 | 1500/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 14.29 | mse 14.29 | mre  0.00 |
scGPT - INFO - | epoch  23 | 1600/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 10.65 | mse 10.65 | mre  0.00 |
scGPT - INFO - | epoch  23 | 1700/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 11.52 | mse 11.52 | mre  0.00 |
scGPT - INFO - | epoch  23 | 1800/3602 batches | lr 0.0000 | ms/batch 91.24 | loss 11.30 | mse 11.30 | mre  0.00 |
scGPT - INFO - | epoch  23 | 1900/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 14.27 | mse 14.27 | mre  0.00 |
scGPT - INFO - | epoch  23 | 2000/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 11.85 | mse 11.85 | mre  0.00 |
scGPT - INFO - | epoch  23 | 2100/3602 batches | lr 0.0000 | ms/batch 91.45 | loss 13.32 | mse 13.32 | mre  0.00 |
scGPT - INFO - | epoch  23 | 2200/3602 batches | lr 0.0000 | ms/batch 91.48 | loss 11.70 | mse 11.70 | mre  0.00 |
scGPT - INFO - | epoch  23 | 2300/3602 batches | lr 0.0000 | ms/batch 91.46 | loss 11.80 | mse 11.80 | mre  0.00 |
scGPT - INFO - | epoch  23 | 2400/3602 batches | lr 0.0000 | ms/batch 91.41 | loss 12.30 | mse 12.30 | mre  0.00 |
scGPT - INFO - | epoch  23 | 2500/3602 batches | lr 0.0000 | ms/batch 91.35 | loss 11.41 | mse 11.41 | mre  0.00 |
scGPT - INFO - | epoch  23 | 2600/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 15.09 | mse 15.09 | mre  0.00 |
scGPT - INFO - | epoch  23 | 2700/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 11.33 | mse 11.33 | mre  0.00 |
scGPT - INFO - | epoch  23 | 2800/3602 batches | lr 0.0000 | ms/batch 91.29 | loss 12.48 | mse 12.48 | mre  0.00 |
scGPT - INFO - | epoch  23 | 2900/3602 batches | lr 0.0000 | ms/batch 91.38 | loss 12.45 | mse 12.45 | mre  0.00 |
scGPT - INFO - | epoch  23 | 3000/3602 batches | lr 0.0000 | ms/batch 91.43 | loss 12.06 | mse 12.06 | mre  0.00 |
scGPT - INFO - | epoch  23 | 3100/3602 batches | lr 0.0000 | ms/batch 91.41 | loss 11.38 | mse 11.38 | mre  0.00 |
scGPT - INFO - | epoch  23 | 3200/3602 batches | lr 0.0000 | ms/batch 91.41 | loss 12.32 | mse 12.32 | mre  0.00 |
scGPT - INFO - | epoch  23 | 3300/3602 batches | lr 0.0000 | ms/batch 91.25 | loss 10.39 | mse 10.39 | mre  0.00 |
scGPT - INFO - | epoch  23 | 3400/3602 batches | lr 0.0000 | ms/batch 91.33 | loss 11.20 | mse 11.20 | mre  0.00 |
scGPT - INFO - | epoch  23 | 3500/3602 batches | lr 0.0000 | ms/batch 91.28 | loss 10.76 | mse 10.76 | mre  0.00 |
scGPT - INFO - | epoch  23 | 3600/3602 batches | lr 0.0000 | ms/batch 91.28 | loss 12.83 | mse 12.83 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  23 | time: 340.28s | valid loss/mse 12.9778 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.899632474679626, min_delta 0.0001, val_loss 12.977816678462702
Loss error: -0.078184203783076
scGPT - INFO - INFO: Early stopping counter 6 of 10
epoch:  23
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.766 3.545 3.324 ... 3.533 3.654 3.924]
 [7.023 3.637 3.402 ... 3.51  3.701 3.953]
 [4.652 3.615 3.855 ... 4.06  3.703 3.572]
 ...
 [4.793 3.785 3.916 ... 4.06  3.754 3.59 ]
 [4.938 3.883 4.035 ... 4.145 3.87  3.656]
 [6.945 3.629 3.299 ... 3.525 3.64  3.947]]
(801, 11) (801, 11)
By feature:  MSE:  0.5899¬±0.0 MAE:  0.5034¬±0.0 R2:  0.218¬±0.0 PCC:  0.7908¬±0.0 Cosine Similarity:  0.9895¬±0.0
By sample:  MSE:  0.5899¬±0.0 MAE:  0.5034¬±0.0 R2:  0.2649¬±0.0 PCC:  0.5006¬±0.0 Cosine Similarity:  0.985¬±0.0
scGPT - INFO - By feature: MSE: 0.589900016784668¬±0.0 MAE: 0.5034000277519226¬±0.0 R2: 0.218¬±0.0 PCC: 0.7908¬±0.0 Cosine Similarity: 0.9895¬±0.0
scGPT - INFO - By sample: MSE: 0.589900016784668¬±0.0 MAE: 0.5034000277519226¬±0.0 R2: 0.2649¬±0.0 PCC: 0.5006¬±0.0 Cosine Similarity: 0.985¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  24
Training model
scGPT - INFO - | epoch  24 | 100/3602 batches | lr 0.0000 | ms/batch 92.52 | loss 11.07 | mse 11.07 | mre  0.00 |
scGPT - INFO - | epoch  24 | 200/3602 batches | lr 0.0000 | ms/batch 92.24 | loss 12.85 | mse 12.85 | mre  0.00 |
scGPT - INFO - | epoch  24 | 300/3602 batches | lr 0.0000 | ms/batch 91.62 | loss 10.00 | mse 10.00 | mre  0.00 |
scGPT - INFO - | epoch  24 | 400/3602 batches | lr 0.0000 | ms/batch 91.46 | loss 10.91 | mse 10.91 | mre  0.00 |
scGPT - INFO - | epoch  24 | 500/3602 batches | lr 0.0000 | ms/batch 91.51 | loss  9.99 | mse  9.99 | mre  0.00 |
scGPT - INFO - | epoch  24 | 600/3602 batches | lr 0.0000 | ms/batch 91.47 | loss 11.45 | mse 11.45 | mre  0.00 |
scGPT - INFO - | epoch  24 | 700/3602 batches | lr 0.0000 | ms/batch 91.49 | loss 12.02 | mse 12.02 | mre  0.00 |
scGPT - INFO - | epoch  24 | 800/3602 batches | lr 0.0000 | ms/batch 91.43 | loss 14.79 | mse 14.79 | mre  0.00 |
scGPT - INFO - | epoch  24 | 900/3602 batches | lr 0.0000 | ms/batch 91.44 | loss 11.90 | mse 11.90 | mre  0.00 |
scGPT - INFO - | epoch  24 | 1000/3602 batches | lr 0.0000 | ms/batch 91.41 | loss 13.12 | mse 13.12 | mre  0.00 |
scGPT - INFO - | epoch  24 | 1100/3602 batches | lr 0.0000 | ms/batch 91.38 | loss 11.27 | mse 11.27 | mre  0.00 |
scGPT - INFO - | epoch  24 | 1200/3602 batches | lr 0.0000 | ms/batch 92.27 | loss 11.42 | mse 11.42 | mre  0.00 |
scGPT - INFO - | epoch  24 | 1300/3602 batches | lr 0.0000 | ms/batch 91.38 | loss 11.36 | mse 11.36 | mre  0.00 |
scGPT - INFO - | epoch  24 | 1400/3602 batches | lr 0.0000 | ms/batch 91.39 | loss 13.91 | mse 13.91 | mre  0.00 |
scGPT - INFO - | epoch  24 | 1500/3602 batches | lr 0.0000 | ms/batch 91.38 | loss 14.49 | mse 14.49 | mre  0.00 |
scGPT - INFO - | epoch  24 | 1600/3602 batches | lr 0.0000 | ms/batch 91.43 | loss 10.51 | mse 10.51 | mre  0.00 |
scGPT - INFO - | epoch  24 | 1700/3602 batches | lr 0.0000 | ms/batch 91.40 | loss 11.19 | mse 11.19 | mre  0.00 |
scGPT - INFO - | epoch  24 | 1800/3602 batches | lr 0.0000 | ms/batch 91.42 | loss 11.16 | mse 11.16 | mre  0.00 |
scGPT - INFO - | epoch  24 | 1900/3602 batches | lr 0.0000 | ms/batch 91.47 | loss 13.75 | mse 13.75 | mre  0.00 |
scGPT - INFO - | epoch  24 | 2000/3602 batches | lr 0.0000 | ms/batch 91.65 | loss 11.77 | mse 11.77 | mre  0.00 |
scGPT - INFO - | epoch  24 | 2100/3602 batches | lr 0.0000 | ms/batch 92.04 | loss 13.56 | mse 13.56 | mre  0.00 |
scGPT - INFO - | epoch  24 | 2200/3602 batches | lr 0.0000 | ms/batch 92.90 | loss 11.27 | mse 11.27 | mre  0.00 |
scGPT - INFO - | epoch  24 | 2300/3602 batches | lr 0.0000 | ms/batch 92.01 | loss 11.74 | mse 11.74 | mre  0.00 |
scGPT - INFO - | epoch  24 | 2400/3602 batches | lr 0.0000 | ms/batch 92.05 | loss 12.26 | mse 12.26 | mre  0.00 |
scGPT - INFO - | epoch  24 | 2500/3602 batches | lr 0.0000 | ms/batch 92.13 | loss 11.19 | mse 11.19 | mre  0.00 |
scGPT - INFO - | epoch  24 | 2600/3602 batches | lr 0.0000 | ms/batch 91.97 | loss 15.02 | mse 15.02 | mre  0.00 |
scGPT - INFO - | epoch  24 | 2700/3602 batches | lr 0.0000 | ms/batch 92.57 | loss 11.30 | mse 11.30 | mre  0.00 |
scGPT - INFO - | epoch  24 | 2800/3602 batches | lr 0.0000 | ms/batch 92.39 | loss 12.31 | mse 12.31 | mre  0.00 |
scGPT - INFO - | epoch  24 | 2900/3602 batches | lr 0.0000 | ms/batch 92.41 | loss 12.68 | mse 12.68 | mre  0.00 |
scGPT - INFO - | epoch  24 | 3000/3602 batches | lr 0.0000 | ms/batch 92.49 | loss 11.90 | mse 11.90 | mre  0.00 |
scGPT - INFO - | epoch  24 | 3100/3602 batches | lr 0.0000 | ms/batch 92.79 | loss 11.91 | mse 11.91 | mre  0.00 |
scGPT - INFO - | epoch  24 | 3200/3602 batches | lr 0.0000 | ms/batch 93.46 | loss 11.80 | mse 11.80 | mre  0.00 |
scGPT - INFO - | epoch  24 | 3300/3602 batches | lr 0.0000 | ms/batch 92.54 | loss 10.58 | mse 10.58 | mre  0.00 |
scGPT - INFO - | epoch  24 | 3400/3602 batches | lr 0.0000 | ms/batch 92.63 | loss 11.41 | mse 11.41 | mre  0.00 |
scGPT - INFO - | epoch  24 | 3500/3602 batches | lr 0.0000 | ms/batch 92.37 | loss 10.84 | mse 10.84 | mre  0.00 |
scGPT - INFO - | epoch  24 | 3600/3602 batches | lr 0.0000 | ms/batch 92.56 | loss 12.36 | mse 12.36 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  24 | time: 343.72s | valid loss/mse 12.9647 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.899632474679626, min_delta 0.0001, val_loss 12.964728348040849
Loss error: -0.0650958733612228
scGPT - INFO - INFO: Early stopping counter 7 of 10
epoch:  24
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.81  3.549 3.309 ... 3.516 3.635 3.92 ]
 [7.02  3.627 3.371 ... 3.498 3.674 3.94 ]
 [4.617 3.58  3.81  ... 4.043 3.67  3.549]
 ...
 [4.78  3.77  3.885 ... 4.05  3.73  3.576]
 [4.906 3.854 3.99  ... 4.133 3.832 3.635]
 [6.95  3.627 3.283 ... 3.516 3.62  3.938]]
(801, 11) (801, 11)
By feature:  MSE:  0.5894¬±0.0 MAE:  0.5014¬±0.0 R2:  0.2146¬±0.0 PCC:  0.7923¬±0.0 Cosine Similarity:  0.9895¬±0.0
By sample:  MSE:  0.5894¬±0.0 MAE:  0.5014¬±0.0 R2:  0.2652¬±0.0 PCC:  0.5024¬±0.0 Cosine Similarity:  0.9851¬±0.0
scGPT - INFO - By feature: MSE: 0.5893999934196472¬±0.0 MAE: 0.5013999938964844¬±0.0 R2: 0.2146¬±0.0 PCC: 0.7923¬±0.0 Cosine Similarity: 0.9895¬±0.0
scGPT - INFO - By sample: MSE: 0.5893999934196472¬±0.0 MAE: 0.5013999938964844¬±0.0 R2: 0.2652¬±0.0 PCC: 0.5024¬±0.0 Cosine Similarity: 0.9851¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  25
Training model
scGPT - INFO - | epoch  25 | 100/3602 batches | lr 0.0000 | ms/batch 93.87 | loss 11.48 | mse 11.48 | mre  0.00 |
scGPT - INFO - | epoch  25 | 200/3602 batches | lr 0.0000 | ms/batch 92.42 | loss 12.85 | mse 12.85 | mre  0.00 |
scGPT - INFO - | epoch  25 | 300/3602 batches | lr 0.0000 | ms/batch 91.90 | loss 10.09 | mse 10.09 | mre  0.00 |
scGPT - INFO - | epoch  25 | 400/3602 batches | lr 0.0000 | ms/batch 91.78 | loss 11.12 | mse 11.12 | mre  0.00 |
scGPT - INFO - | epoch  25 | 500/3602 batches | lr 0.0000 | ms/batch 91.65 | loss 10.06 | mse 10.06 | mre  0.00 |
scGPT - INFO - | epoch  25 | 600/3602 batches | lr 0.0000 | ms/batch 92.30 | loss 11.26 | mse 11.26 | mre  0.00 |
scGPT - INFO - | epoch  25 | 700/3602 batches | lr 0.0000 | ms/batch 91.83 | loss 12.06 | mse 12.06 | mre  0.00 |
scGPT - INFO - | epoch  25 | 800/3602 batches | lr 0.0000 | ms/batch 91.72 | loss 14.70 | mse 14.70 | mre  0.00 |
scGPT - INFO - | epoch  25 | 900/3602 batches | lr 0.0000 | ms/batch 91.79 | loss 11.81 | mse 11.81 | mre  0.00 |
scGPT - INFO - | epoch  25 | 1000/3602 batches | lr 0.0000 | ms/batch 91.69 | loss 12.86 | mse 12.86 | mre  0.00 |
scGPT - INFO - | epoch  25 | 1100/3602 batches | lr 0.0000 | ms/batch 91.60 | loss 11.17 | mse 11.17 | mre  0.00 |
scGPT - INFO - | epoch  25 | 1200/3602 batches | lr 0.0000 | ms/batch 91.51 | loss 11.51 | mse 11.51 | mre  0.00 |
scGPT - INFO - | epoch  25 | 1300/3602 batches | lr 0.0000 | ms/batch 91.58 | loss 11.31 | mse 11.31 | mre  0.00 |
scGPT - INFO - | epoch  25 | 1400/3602 batches | lr 0.0000 | ms/batch 91.80 | loss 13.63 | mse 13.63 | mre  0.00 |
scGPT - INFO - | epoch  25 | 1500/3602 batches | lr 0.0000 | ms/batch 91.90 | loss 14.23 | mse 14.23 | mre  0.00 |
scGPT - INFO - | epoch  25 | 1600/3602 batches | lr 0.0000 | ms/batch 92.55 | loss 10.52 | mse 10.52 | mre  0.00 |
scGPT - INFO - | epoch  25 | 1700/3602 batches | lr 0.0000 | ms/batch 91.70 | loss 11.27 | mse 11.27 | mre  0.00 |
scGPT - INFO - | epoch  25 | 1800/3602 batches | lr 0.0000 | ms/batch 91.49 | loss 11.02 | mse 11.02 | mre  0.00 |
scGPT - INFO - | epoch  25 | 1900/3602 batches | lr 0.0000 | ms/batch 92.01 | loss 13.78 | mse 13.78 | mre  0.00 |
scGPT - INFO - | epoch  25 | 2000/3602 batches | lr 0.0000 | ms/batch 91.92 | loss 11.85 | mse 11.85 | mre  0.00 |
scGPT - INFO - | epoch  25 | 2100/3602 batches | lr 0.0000 | ms/batch 91.66 | loss 13.59 | mse 13.59 | mre  0.00 |
scGPT - INFO - | epoch  25 | 2200/3602 batches | lr 0.0000 | ms/batch 91.67 | loss 11.60 | mse 11.60 | mre  0.00 |
scGPT - INFO - | epoch  25 | 2300/3602 batches | lr 0.0000 | ms/batch 91.60 | loss 11.63 | mse 11.63 | mre  0.00 |
scGPT - INFO - | epoch  25 | 2400/3602 batches | lr 0.0000 | ms/batch 91.61 | loss 12.05 | mse 12.05 | mre  0.00 |
scGPT - INFO - | epoch  25 | 2500/3602 batches | lr 0.0000 | ms/batch 91.64 | loss 11.39 | mse 11.39 | mre  0.00 |
scGPT - INFO - | epoch  25 | 2600/3602 batches | lr 0.0000 | ms/batch 92.25 | loss 14.86 | mse 14.86 | mre  0.00 |
scGPT - INFO - | epoch  25 | 2700/3602 batches | lr 0.0000 | ms/batch 91.90 | loss 11.21 | mse 11.21 | mre  0.00 |
scGPT - INFO - | epoch  25 | 2800/3602 batches | lr 0.0000 | ms/batch 91.85 | loss 12.28 | mse 12.28 | mre  0.00 |
scGPT - INFO - | epoch  25 | 2900/3602 batches | lr 0.0000 | ms/batch 91.75 | loss 12.39 | mse 12.39 | mre  0.00 |
scGPT - INFO - | epoch  25 | 3000/3602 batches | lr 0.0000 | ms/batch 91.80 | loss 11.59 | mse 11.59 | mre  0.00 |
scGPT - INFO - | epoch  25 | 3100/3602 batches | lr 0.0000 | ms/batch 91.92 | loss 11.46 | mse 11.46 | mre  0.00 |
scGPT - INFO - | epoch  25 | 3200/3602 batches | lr 0.0000 | ms/batch 91.82 | loss 11.94 | mse 11.94 | mre  0.00 |
scGPT - INFO - | epoch  25 | 3300/3602 batches | lr 0.0000 | ms/batch 91.83 | loss 10.06 | mse 10.06 | mre  0.00 |
scGPT - INFO - | epoch  25 | 3400/3602 batches | lr 0.0000 | ms/batch 91.77 | loss 10.93 | mse 10.93 | mre  0.00 |
scGPT - INFO - | epoch  25 | 3500/3602 batches | lr 0.0000 | ms/batch 92.21 | loss 11.30 | mse 11.30 | mre  0.00 |
scGPT - INFO - | epoch  25 | 3600/3602 batches | lr 0.0000 | ms/batch 92.94 | loss 12.79 | mse 12.79 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  25 | time: 343.56s | valid loss/mse 12.9169 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.899632474679626, min_delta 0.0001, val_loss 12.916934928644016
Loss error: -0.017302453964390452
scGPT - INFO - INFO: Early stopping counter 8 of 10
epoch:  25
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.746 3.533 3.3   ... 3.53  3.635 3.904]
 [7.03  3.607 3.365 ... 3.484 3.676 3.934]
 [4.62  3.588 3.816 ... 4.04  3.672 3.541]
 ...
 [4.773 3.766 3.88  ... 4.04  3.729 3.566]
 [4.883 3.836 3.977 ... 4.113 3.82  3.615]
 [6.86  3.63  3.273 ... 3.523 3.607 3.92 ]]
(801, 11) (801, 11)
By feature:  MSE:  0.5872¬±0.0 MAE:  0.5005¬±0.0 R2:  0.2198¬±0.0 PCC:  0.7932¬±0.0 Cosine Similarity:  0.9896¬±0.0
By sample:  MSE:  0.5872¬±0.0 MAE:  0.5005¬±0.0 R2:  0.2678¬±0.0 PCC:  0.5045¬±0.0 Cosine Similarity:  0.9851¬±0.0
scGPT - INFO - By feature: MSE: 0.5871999859809875¬±0.0 MAE: 0.5005000233650208¬±0.0 R2: 0.2198¬±0.0 PCC: 0.7932¬±0.0 Cosine Similarity: 0.9896¬±0.0
scGPT - INFO - By sample: MSE: 0.5871999859809875¬±0.0 MAE: 0.5005000233650208¬±0.0 R2: 0.2678¬±0.0 PCC: 0.5045¬±0.0 Cosine Similarity: 0.9851¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  26
Training model
scGPT - INFO - | epoch  26 | 100/3602 batches | lr 0.0000 | ms/batch 93.28 | loss 11.16 | mse 11.16 | mre  0.00 |
scGPT - INFO - | epoch  26 | 200/3602 batches | lr 0.0000 | ms/batch 91.99 | loss 12.81 | mse 12.81 | mre  0.00 |
scGPT - INFO - | epoch  26 | 300/3602 batches | lr 0.0000 | ms/batch 91.81 | loss 10.07 | mse 10.07 | mre  0.00 |
scGPT - INFO - | epoch  26 | 400/3602 batches | lr 0.0000 | ms/batch 91.69 | loss 11.36 | mse 11.36 | mre  0.00 |
scGPT - INFO - | epoch  26 | 500/3602 batches | lr 0.0000 | ms/batch 91.61 | loss 10.13 | mse 10.13 | mre  0.00 |
scGPT - INFO - | epoch  26 | 600/3602 batches | lr 0.0000 | ms/batch 92.07 | loss 11.26 | mse 11.26 | mre  0.00 |
scGPT - INFO - | epoch  26 | 700/3602 batches | lr 0.0000 | ms/batch 91.64 | loss 11.84 | mse 11.84 | mre  0.00 |
scGPT - INFO - | epoch  26 | 800/3602 batches | lr 0.0000 | ms/batch 91.75 | loss 14.66 | mse 14.66 | mre  0.00 |
scGPT - INFO - | epoch  26 | 900/3602 batches | lr 0.0000 | ms/batch 91.77 | loss 11.99 | mse 11.99 | mre  0.00 |
scGPT - INFO - | epoch  26 | 1000/3602 batches | lr 0.0000 | ms/batch 92.32 | loss 13.04 | mse 13.04 | mre  0.00 |
scGPT - INFO - | epoch  26 | 1100/3602 batches | lr 0.0000 | ms/batch 91.73 | loss 10.98 | mse 10.98 | mre  0.00 |
scGPT - INFO - | epoch  26 | 1200/3602 batches | lr 0.0000 | ms/batch 92.06 | loss 11.47 | mse 11.47 | mre  0.00 |
scGPT - INFO - | epoch  26 | 1300/3602 batches | lr 0.0000 | ms/batch 91.38 | loss 11.34 | mse 11.34 | mre  0.00 |
scGPT - INFO - | epoch  26 | 1400/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 13.71 | mse 13.71 | mre  0.00 |
scGPT - INFO - | epoch  26 | 1500/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 14.16 | mse 14.16 | mre  0.00 |
scGPT - INFO - | epoch  26 | 1600/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 10.09 | mse 10.09 | mre  0.00 |
scGPT - INFO - | epoch  26 | 1700/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 11.34 | mse 11.34 | mre  0.00 |
scGPT - INFO - | epoch  26 | 1800/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 11.15 | mse 11.15 | mre  0.00 |
scGPT - INFO - | epoch  26 | 1900/3602 batches | lr 0.0000 | ms/batch 90.91 | loss 13.86 | mse 13.86 | mre  0.00 |
scGPT - INFO - | epoch  26 | 2000/3602 batches | lr 0.0000 | ms/batch 91.40 | loss 11.89 | mse 11.89 | mre  0.00 |
scGPT - INFO - | epoch  26 | 2100/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 13.53 | mse 13.53 | mre  0.00 |
scGPT - INFO - | epoch  26 | 2200/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 11.24 | mse 11.24 | mre  0.00 |
scGPT - INFO - | epoch  26 | 2300/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 11.11 | mse 11.11 | mre  0.00 |
scGPT - INFO - | epoch  26 | 2400/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 11.90 | mse 11.90 | mre  0.00 |
scGPT - INFO - | epoch  26 | 2500/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 11.29 | mse 11.29 | mre  0.00 |
scGPT - INFO - | epoch  26 | 2600/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 14.90 | mse 14.90 | mre  0.00 |
scGPT - INFO - | epoch  26 | 2700/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 11.05 | mse 11.05 | mre  0.00 |
scGPT - INFO - | epoch  26 | 2800/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 12.63 | mse 12.63 | mre  0.00 |
scGPT - INFO - | epoch  26 | 2900/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 12.50 | mse 12.50 | mre  0.00 |
scGPT - INFO - | epoch  26 | 3000/3602 batches | lr 0.0000 | ms/batch 91.26 | loss 11.59 | mse 11.59 | mre  0.00 |
scGPT - INFO - | epoch  26 | 3100/3602 batches | lr 0.0000 | ms/batch 91.45 | loss 11.15 | mse 11.15 | mre  0.00 |
scGPT - INFO - | epoch  26 | 3200/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 11.97 | mse 11.97 | mre  0.00 |
scGPT - INFO - | epoch  26 | 3300/3602 batches | lr 0.0000 | ms/batch 90.76 | loss  9.88 | mse  9.88 | mre  0.00 |
scGPT - INFO - | epoch  26 | 3400/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 11.07 | mse 11.07 | mre  0.00 |
scGPT - INFO - | epoch  26 | 3500/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 10.46 | mse 10.46 | mre  0.00 |
scGPT - INFO - | epoch  26 | 3600/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 12.36 | mse 12.36 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  26 | time: 340.97s | valid loss/mse 12.8755 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.8755
best_loss: 12.899632474679626, min_delta 0.0001, val_loss 12.875542593136263
Loss error: 0.02408988154336278
epoch:  26
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.867 3.523 3.285 ... 3.488 3.63  3.914]
 [7.008 3.602 3.363 ... 3.488 3.676 3.932]
 [4.613 3.592 3.812 ... 4.05  3.682 3.549]
 ...
 [4.773 3.781 3.883 ... 4.047 3.74  3.572]
 [4.902 3.86  3.99  ... 4.133 3.842 3.63 ]
 [6.953 3.615 3.275 ... 3.5   3.617 3.928]]
(801, 11) (801, 11)
By feature:  MSE:  0.5853¬±0.0 MAE:  0.4985¬±0.0 R2:  0.2214¬±0.0 PCC:  0.7941¬±0.0 Cosine Similarity:  0.9896¬±0.0
By sample:  MSE:  0.5853¬±0.0 MAE:  0.4985¬±0.0 R2:  0.2699¬±0.0 PCC:  0.5063¬±0.0 Cosine Similarity:  0.9852¬±0.0
scGPT - INFO - By feature: MSE: 0.5853000283241272¬±0.0 MAE: 0.4984999895095825¬±0.0 R2: 0.2214¬±0.0 PCC: 0.7941¬±0.0 Cosine Similarity: 0.9896¬±0.0
scGPT - INFO - By sample: MSE: 0.5853000283241272¬±0.0 MAE: 0.4984999895095825¬±0.0 R2: 0.2699¬±0.0 PCC: 0.5063¬±0.0 Cosine Similarity: 0.9852¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  27
Training model
scGPT - INFO - | epoch  27 | 100/3602 batches | lr 0.0000 | ms/batch 92.49 | loss 11.17 | mse 11.17 | mre  0.00 |
scGPT - INFO - | epoch  27 | 200/3602 batches | lr 0.0000 | ms/batch 91.21 | loss 12.88 | mse 12.88 | mre  0.00 |
scGPT - INFO - | epoch  27 | 300/3602 batches | lr 0.0000 | ms/batch 91.56 | loss 10.25 | mse 10.25 | mre  0.00 |
scGPT - INFO - | epoch  27 | 400/3602 batches | lr 0.0000 | ms/batch 92.00 | loss 11.27 | mse 11.27 | mre  0.00 |
scGPT - INFO - | epoch  27 | 500/3602 batches | lr 0.0000 | ms/batch 90.61 | loss 10.02 | mse 10.02 | mre  0.00 |
scGPT - INFO - | epoch  27 | 600/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 11.36 | mse 11.36 | mre  0.00 |
scGPT - INFO - | epoch  27 | 700/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 12.07 | mse 12.07 | mre  0.00 |
scGPT - INFO - | epoch  27 | 800/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 14.32 | mse 14.32 | mre  0.00 |
scGPT - INFO - | epoch  27 | 900/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 11.75 | mse 11.75 | mre  0.00 |
scGPT - INFO - | epoch  27 | 1000/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 13.06 | mse 13.06 | mre  0.00 |
scGPT - INFO - | epoch  27 | 1100/3602 batches | lr 0.0000 | ms/batch 91.16 | loss 10.90 | mse 10.90 | mre  0.00 |
scGPT - INFO - | epoch  27 | 1200/3602 batches | lr 0.0000 | ms/batch 91.24 | loss 11.94 | mse 11.94 | mre  0.00 |
scGPT - INFO - | epoch  27 | 1300/3602 batches | lr 0.0000 | ms/batch 91.68 | loss 11.44 | mse 11.44 | mre  0.00 |
scGPT - INFO - | epoch  27 | 1400/3602 batches | lr 0.0000 | ms/batch 91.36 | loss 13.54 | mse 13.54 | mre  0.00 |
scGPT - INFO - | epoch  27 | 1500/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 14.17 | mse 14.17 | mre  0.00 |
scGPT - INFO - | epoch  27 | 1600/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 10.17 | mse 10.17 | mre  0.00 |
scGPT - INFO - | epoch  27 | 1700/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 11.19 | mse 11.19 | mre  0.00 |
scGPT - INFO - | epoch  27 | 1800/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 11.04 | mse 11.04 | mre  0.00 |
scGPT - INFO - | epoch  27 | 1900/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 13.77 | mse 13.77 | mre  0.00 |
scGPT - INFO - | epoch  27 | 2000/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 11.75 | mse 11.75 | mre  0.00 |
scGPT - INFO - | epoch  27 | 2100/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 13.01 | mse 13.01 | mre  0.00 |
scGPT - INFO - | epoch  27 | 2200/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 11.50 | mse 11.50 | mre  0.00 |
scGPT - INFO - | epoch  27 | 2300/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 11.54 | mse 11.54 | mre  0.00 |
scGPT - INFO - | epoch  27 | 2400/3602 batches | lr 0.0000 | ms/batch 92.46 | loss 11.85 | mse 11.85 | mre  0.00 |
scGPT - INFO - | epoch  27 | 2500/3602 batches | lr 0.0000 | ms/batch 91.64 | loss 11.03 | mse 11.03 | mre  0.00 |
scGPT - INFO - | epoch  27 | 2600/3602 batches | lr 0.0000 | ms/batch 92.10 | loss 14.63 | mse 14.63 | mre  0.00 |
scGPT - INFO - | epoch  27 | 2700/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 11.33 | mse 11.33 | mre  0.00 |
scGPT - INFO - | epoch  27 | 2800/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 12.43 | mse 12.43 | mre  0.00 |
scGPT - INFO - | epoch  27 | 2900/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 12.07 | mse 12.07 | mre  0.00 |
scGPT - INFO - | epoch  27 | 3000/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 12.05 | mse 12.05 | mre  0.00 |
scGPT - INFO - | epoch  27 | 3100/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 11.52 | mse 11.52 | mre  0.00 |
scGPT - INFO - | epoch  27 | 3200/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 11.60 | mse 11.60 | mre  0.00 |
scGPT - INFO - | epoch  27 | 3300/3602 batches | lr 0.0000 | ms/batch 90.62 | loss 10.02 | mse 10.02 | mre  0.00 |
scGPT - INFO - | epoch  27 | 3400/3602 batches | lr 0.0000 | ms/batch 91.16 | loss 11.22 | mse 11.22 | mre  0.00 |
scGPT - INFO - | epoch  27 | 3500/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 10.68 | mse 10.68 | mre  0.00 |
scGPT - INFO - | epoch  27 | 3600/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 12.40 | mse 12.40 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  27 | time: 340.18s | valid loss/mse 12.7848 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.7848
best_loss: 12.875542593136263, min_delta 0.0001, val_loss 12.784754122985287
Loss error: 0.0907884701509758
epoch:  27
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.836 3.537 3.309 ... 3.514 3.639 3.916]
 [7.043 3.604 3.371 ... 3.486 3.682 3.941]
 [4.668 3.615 3.842 ... 4.074 3.703 3.574]
 ...
 [4.793 3.781 3.895 ... 4.05  3.748 3.586]
 [4.934 3.865 4.008 ... 4.137 3.857 3.65 ]
 [6.832 3.65  3.287 ... 3.543 3.61  3.918]]
(801, 11) (801, 11)
By feature:  MSE:  0.5812¬±0.0 MAE:  0.4979¬±0.0 R2:  0.2295¬±0.0 PCC:  0.798¬±0.0 Cosine Similarity:  0.9897¬±0.0
By sample:  MSE:  0.5812¬±0.0 MAE:  0.4979¬±0.0 R2:  0.2745¬±0.0 PCC:  0.5092¬±0.0 Cosine Similarity:  0.9853¬±0.0
scGPT - INFO - By feature: MSE: 0.5812000036239624¬±0.0 MAE: 0.49790000915527344¬±0.0 R2: 0.2295¬±0.0 PCC: 0.798¬±0.0 Cosine Similarity: 0.9897¬±0.0
scGPT - INFO - By sample: MSE: 0.5812000036239624¬±0.0 MAE: 0.49790000915527344¬±0.0 R2: 0.2745¬±0.0 PCC: 0.5092¬±0.0 Cosine Similarity: 0.9853¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  28
Training model
scGPT - INFO - | epoch  28 | 100/3602 batches | lr 0.0000 | ms/batch 91.93 | loss 11.30 | mse 11.30 | mre  0.00 |
scGPT - INFO - | epoch  28 | 200/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 12.57 | mse 12.57 | mre  0.00 |
scGPT - INFO - | epoch  28 | 300/3602 batches | lr 0.0000 | ms/batch 90.66 | loss  9.85 | mse  9.85 | mre  0.00 |
scGPT - INFO - | epoch  28 | 400/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 11.24 | mse 11.24 | mre  0.00 |
scGPT - INFO - | epoch  28 | 500/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 10.03 | mse 10.03 | mre  0.00 |
scGPT - INFO - | epoch  28 | 600/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 11.08 | mse 11.08 | mre  0.00 |
scGPT - INFO - | epoch  28 | 700/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 11.82 | mse 11.82 | mre  0.00 |
scGPT - INFO - | epoch  28 | 800/3602 batches | lr 0.0000 | ms/batch 91.22 | loss 14.66 | mse 14.66 | mre  0.00 |
scGPT - INFO - | epoch  28 | 900/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 11.58 | mse 11.58 | mre  0.00 |
scGPT - INFO - | epoch  28 | 1000/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 12.97 | mse 12.97 | mre  0.00 |
scGPT - INFO - | epoch  28 | 1100/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 11.40 | mse 11.40 | mre  0.00 |
scGPT - INFO - | epoch  28 | 1200/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 11.53 | mse 11.53 | mre  0.00 |
scGPT - INFO - | epoch  28 | 1300/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 11.57 | mse 11.57 | mre  0.00 |
scGPT - INFO - | epoch  28 | 1400/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 13.60 | mse 13.60 | mre  0.00 |
scGPT - INFO - | epoch  28 | 1500/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 14.07 | mse 14.07 | mre  0.00 |
scGPT - INFO - | epoch  28 | 1600/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 10.14 | mse 10.14 | mre  0.00 |
scGPT - INFO - | epoch  28 | 1700/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 10.89 | mse 10.89 | mre  0.00 |
scGPT - INFO - | epoch  28 | 1800/3602 batches | lr 0.0000 | ms/batch 91.18 | loss 11.21 | mse 11.21 | mre  0.00 |
scGPT - INFO - | epoch  28 | 1900/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 13.97 | mse 13.97 | mre  0.00 |
scGPT - INFO - | epoch  28 | 2000/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 11.87 | mse 11.87 | mre  0.00 |
scGPT - INFO - | epoch  28 | 2100/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 13.05 | mse 13.05 | mre  0.00 |
scGPT - INFO - | epoch  28 | 2200/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 11.36 | mse 11.36 | mre  0.00 |
scGPT - INFO - | epoch  28 | 2300/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 11.41 | mse 11.41 | mre  0.00 |
scGPT - INFO - | epoch  28 | 2400/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 11.95 | mse 11.95 | mre  0.00 |
scGPT - INFO - | epoch  28 | 2500/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 10.87 | mse 10.87 | mre  0.00 |
scGPT - INFO - | epoch  28 | 2600/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 14.75 | mse 14.75 | mre  0.00 |
scGPT - INFO - | epoch  28 | 2700/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 11.17 | mse 11.17 | mre  0.00 |
scGPT - INFO - | epoch  28 | 2800/3602 batches | lr 0.0000 | ms/batch 91.18 | loss 12.40 | mse 12.40 | mre  0.00 |
scGPT - INFO - | epoch  28 | 2900/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 11.82 | mse 11.82 | mre  0.00 |
scGPT - INFO - | epoch  28 | 3000/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 11.86 | mse 11.86 | mre  0.00 |
scGPT - INFO - | epoch  28 | 3100/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 10.95 | mse 10.95 | mre  0.00 |
scGPT - INFO - | epoch  28 | 3200/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 11.71 | mse 11.71 | mre  0.00 |
scGPT - INFO - | epoch  28 | 3300/3602 batches | lr 0.0000 | ms/batch 90.70 | loss  9.93 | mse  9.93 | mre  0.00 |
scGPT - INFO - | epoch  28 | 3400/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 11.09 | mse 11.09 | mre  0.00 |
scGPT - INFO - | epoch  28 | 3500/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 10.82 | mse 10.82 | mre  0.00 |
scGPT - INFO - | epoch  28 | 3600/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 12.32 | mse 12.32 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  28 | time: 339.27s | valid loss/mse 12.7711 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.7711
best_loss: 12.784754122985287, min_delta 0.0001, val_loss 12.771075850494494
Loss error: 0.01367827249079312
epoch:  28
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.812 3.55  3.314 ... 3.506 3.64  3.908]
 [7.008 3.617 3.375 ... 3.488 3.68  3.932]
 [4.66  3.627 3.836 ... 4.07  3.68  3.557]
 ...
 [4.805 3.803 3.9   ... 4.055 3.746 3.582]
 [4.93  3.873 4.    ... 4.133 3.844 3.639]
 [6.94  3.648 3.293 ... 3.508 3.621 3.928]]
(801, 11) (801, 11)
By feature:  MSE:  0.5805¬±0.0 MAE:  0.4969¬±0.0 R2:  0.2248¬±0.0 PCC:  0.7986¬±0.0 Cosine Similarity:  0.9897¬±0.0
By sample:  MSE:  0.5805¬±0.0 MAE:  0.4969¬±0.0 R2:  0.275¬±0.0 PCC:  0.5104¬±0.0 Cosine Similarity:  0.9853¬±0.0
scGPT - INFO - By feature: MSE: 0.5805000066757202¬±0.0 MAE: 0.4968999922275543¬±0.0 R2: 0.2248¬±0.0 PCC: 0.7986¬±0.0 Cosine Similarity: 0.9897¬±0.0
scGPT - INFO - By sample: MSE: 0.5805000066757202¬±0.0 MAE: 0.4968999922275543¬±0.0 R2: 0.275¬±0.0 PCC: 0.5104¬±0.0 Cosine Similarity: 0.9853¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  29
Training model
scGPT - INFO - | epoch  29 | 100/3602 batches | lr 0.0000 | ms/batch 92.00 | loss 11.14 | mse 11.14 | mre  0.00 |
scGPT - INFO - | epoch  29 | 200/3602 batches | lr 0.0000 | ms/batch 91.36 | loss 12.54 | mse 12.54 | mre  0.00 |
scGPT - INFO - | epoch  29 | 300/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 10.32 | mse 10.32 | mre  0.00 |
scGPT - INFO - | epoch  29 | 400/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 11.09 | mse 11.09 | mre  0.00 |
scGPT - INFO - | epoch  29 | 500/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.00 | mse 10.00 | mre  0.00 |
scGPT - INFO - | epoch  29 | 600/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 10.94 | mse 10.94 | mre  0.00 |
scGPT - INFO - | epoch  29 | 700/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 11.65 | mse 11.65 | mre  0.00 |
scGPT - INFO - | epoch  29 | 800/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 14.18 | mse 14.18 | mre  0.00 |
scGPT - INFO - | epoch  29 | 900/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 11.44 | mse 11.44 | mre  0.00 |
scGPT - INFO - | epoch  29 | 1000/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 12.83 | mse 12.83 | mre  0.00 |
scGPT - INFO - | epoch  29 | 1100/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 10.77 | mse 10.77 | mre  0.00 |
scGPT - INFO - | epoch  29 | 1200/3602 batches | lr 0.0000 | ms/batch 91.29 | loss 11.24 | mse 11.24 | mre  0.00 |
scGPT - INFO - | epoch  29 | 1300/3602 batches | lr 0.0000 | ms/batch 90.95 | loss 10.92 | mse 10.92 | mre  0.00 |
scGPT - INFO - | epoch  29 | 1400/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 13.40 | mse 13.40 | mre  0.00 |
scGPT - INFO - | epoch  29 | 1500/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 13.58 | mse 13.58 | mre  0.00 |
scGPT - INFO - | epoch  29 | 1600/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 10.09 | mse 10.09 | mre  0.00 |
scGPT - INFO - | epoch  29 | 1700/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 11.05 | mse 11.05 | mre  0.00 |
scGPT - INFO - | epoch  29 | 1800/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 10.96 | mse 10.96 | mre  0.00 |
scGPT - INFO - | epoch  29 | 1900/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 13.69 | mse 13.69 | mre  0.00 |
scGPT - INFO - | epoch  29 | 2000/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 11.25 | mse 11.25 | mre  0.00 |
scGPT - INFO - | epoch  29 | 2100/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 13.60 | mse 13.60 | mre  0.00 |
scGPT - INFO - | epoch  29 | 2200/3602 batches | lr 0.0000 | ms/batch 91.21 | loss 11.91 | mse 11.91 | mre  0.00 |
scGPT - INFO - | epoch  29 | 2300/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 11.28 | mse 11.28 | mre  0.00 |
scGPT - INFO - | epoch  29 | 2400/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 12.04 | mse 12.04 | mre  0.00 |
scGPT - INFO - | epoch  29 | 2500/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 10.78 | mse 10.78 | mre  0.00 |
scGPT - INFO - | epoch  29 | 2600/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 14.67 | mse 14.67 | mre  0.00 |
scGPT - INFO - | epoch  29 | 2700/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 11.07 | mse 11.07 | mre  0.00 |
scGPT - INFO - | epoch  29 | 2800/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 12.37 | mse 12.37 | mre  0.00 |
scGPT - INFO - | epoch  29 | 2900/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 12.27 | mse 12.27 | mre  0.00 |
scGPT - INFO - | epoch  29 | 3000/3602 batches | lr 0.0000 | ms/batch 90.59 | loss 11.92 | mse 11.92 | mre  0.00 |
scGPT - INFO - | epoch  29 | 3100/3602 batches | lr 0.0000 | ms/batch 90.61 | loss 10.78 | mse 10.78 | mre  0.00 |
scGPT - INFO - | epoch  29 | 3200/3602 batches | lr 0.0000 | ms/batch 91.33 | loss 11.36 | mse 11.36 | mre  0.00 |
scGPT - INFO - | epoch  29 | 3300/3602 batches | lr 0.0000 | ms/batch 90.97 | loss  9.84 | mse  9.84 | mre  0.00 |
scGPT - INFO - | epoch  29 | 3400/3602 batches | lr 0.0000 | ms/batch 90.99 | loss 10.87 | mse 10.87 | mre  0.00 |
scGPT - INFO - | epoch  29 | 3500/3602 batches | lr 0.0000 | ms/batch 90.93 | loss 11.02 | mse 11.02 | mre  0.00 |
scGPT - INFO - | epoch  29 | 3600/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 12.10 | mse 12.10 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  29 | time: 339.54s | valid loss/mse 12.7425 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.7425
best_loss: 12.771075850494494, min_delta 0.0001, val_loss 12.742471939764368
Loss error: 0.02860391073012636
epoch:  29
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.81  3.531 3.303 ... 3.512 3.64  3.906]
 [7.03  3.604 3.373 ... 3.494 3.686 3.936]
 [4.664 3.615 3.834 ... 4.082 3.688 3.559]
 ...
 [4.81  3.799 3.9   ... 4.07  3.754 3.586]
 [4.93  3.87  4.004 ... 4.145 3.852 3.639]
 [6.926 3.633 3.281 ... 3.514 3.613 3.92 ]]
(801, 11) (801, 11)
By feature:  MSE:  0.5792¬±0.0 MAE:  0.4983¬±0.0 R2:  0.2293¬±0.0 PCC:  0.7958¬±0.0 Cosine Similarity:  0.9897¬±0.0
By sample:  MSE:  0.5792¬±0.0 MAE:  0.4983¬±0.0 R2:  0.2749¬±0.0 PCC:  0.5101¬±0.0 Cosine Similarity:  0.9853¬±0.0
scGPT - INFO - By feature: MSE: 0.579200029373169¬±0.0 MAE: 0.4982999861240387¬±0.0 R2: 0.2293¬±0.0 PCC: 0.7958¬±0.0 Cosine Similarity: 0.9897¬±0.0
scGPT - INFO - By sample: MSE: 0.579200029373169¬±0.0 MAE: 0.4982999861240387¬±0.0 R2: 0.2749¬±0.0 PCC: 0.5101¬±0.0 Cosine Similarity: 0.9853¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  30
Training model
scGPT - INFO - | epoch  30 | 100/3602 batches | lr 0.0000 | ms/batch 92.16 | loss 11.03 | mse 11.03 | mre  0.00 |
scGPT - INFO - | epoch  30 | 200/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 12.96 | mse 12.96 | mre  0.00 |
scGPT - INFO - | epoch  30 | 300/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 10.17 | mse 10.17 | mre  0.00 |
scGPT - INFO - | epoch  30 | 400/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 11.12 | mse 11.12 | mre  0.00 |
scGPT - INFO - | epoch  30 | 500/3602 batches | lr 0.0000 | ms/batch 90.93 | loss  9.78 | mse  9.78 | mre  0.00 |
scGPT - INFO - | epoch  30 | 600/3602 batches | lr 0.0000 | ms/batch 91.57 | loss 11.21 | mse 11.21 | mre  0.00 |
scGPT - INFO - | epoch  30 | 700/3602 batches | lr 0.0000 | ms/batch 91.15 | loss 11.55 | mse 11.55 | mre  0.00 |
scGPT - INFO - | epoch  30 | 800/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 14.38 | mse 14.38 | mre  0.00 |
scGPT - INFO - | epoch  30 | 900/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 11.75 | mse 11.75 | mre  0.00 |
scGPT - INFO - | epoch  30 | 1000/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 12.71 | mse 12.71 | mre  0.00 |
scGPT - INFO - | epoch  30 | 1100/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 11.17 | mse 11.17 | mre  0.00 |
scGPT - INFO - | epoch  30 | 1200/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 11.11 | mse 11.11 | mre  0.00 |
scGPT - INFO - | epoch  30 | 1300/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 11.47 | mse 11.47 | mre  0.00 |
scGPT - INFO - | epoch  30 | 1400/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 13.62 | mse 13.62 | mre  0.00 |
scGPT - INFO - | epoch  30 | 1500/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 13.83 | mse 13.83 | mre  0.00 |
scGPT - INFO - | epoch  30 | 1600/3602 batches | lr 0.0000 | ms/batch 91.57 | loss  9.70 | mse  9.70 | mre  0.00 |
scGPT - INFO - | epoch  30 | 1700/3602 batches | lr 0.0000 | ms/batch 91.17 | loss 10.81 | mse 10.81 | mre  0.00 |
scGPT - INFO - | epoch  30 | 1800/3602 batches | lr 0.0000 | ms/batch 90.93 | loss 11.07 | mse 11.07 | mre  0.00 |
scGPT - INFO - | epoch  30 | 1900/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 13.36 | mse 13.36 | mre  0.00 |
scGPT - INFO - | epoch  30 | 2000/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 11.81 | mse 11.81 | mre  0.00 |
scGPT - INFO - | epoch  30 | 2100/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 12.65 | mse 12.65 | mre  0.00 |
scGPT - INFO - | epoch  30 | 2200/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 11.44 | mse 11.44 | mre  0.00 |
scGPT - INFO - | epoch  30 | 2300/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 11.01 | mse 11.01 | mre  0.00 |
scGPT - INFO - | epoch  30 | 2400/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 12.32 | mse 12.32 | mre  0.00 |
scGPT - INFO - | epoch  30 | 2500/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 10.86 | mse 10.86 | mre  0.00 |
scGPT - INFO - | epoch  30 | 2600/3602 batches | lr 0.0000 | ms/batch 91.33 | loss 14.19 | mse 14.19 | mre  0.00 |
scGPT - INFO - | epoch  30 | 2700/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 10.95 | mse 10.95 | mre  0.00 |
scGPT - INFO - | epoch  30 | 2800/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 11.82 | mse 11.82 | mre  0.00 |
scGPT - INFO - | epoch  30 | 2900/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 11.66 | mse 11.66 | mre  0.00 |
scGPT - INFO - | epoch  30 | 3000/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 11.88 | mse 11.88 | mre  0.00 |
scGPT - INFO - | epoch  30 | 3100/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 11.46 | mse 11.46 | mre  0.00 |
scGPT - INFO - | epoch  30 | 3200/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 11.41 | mse 11.41 | mre  0.00 |
scGPT - INFO - | epoch  30 | 3300/3602 batches | lr 0.0000 | ms/batch 90.66 | loss  9.88 | mse  9.88 | mre  0.00 |
scGPT - INFO - | epoch  30 | 3400/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 10.89 | mse 10.89 | mre  0.00 |
scGPT - INFO - | epoch  30 | 3500/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 10.43 | mse 10.43 | mre  0.00 |
scGPT - INFO - | epoch  30 | 3600/3602 batches | lr 0.0000 | ms/batch 91.16 | loss 12.35 | mse 12.35 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  30 | time: 339.72s | valid loss/mse 12.6671 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.6671
best_loss: 12.742471939764368, min_delta 0.0001, val_loss 12.667089693852876
Loss error: 0.07538224591149145
epoch:  30
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.863 3.541 3.299 ... 3.473 3.63  3.904]
 [7.01  3.605 3.371 ... 3.475 3.678 3.928]
 [4.668 3.635 3.836 ... 4.066 3.693 3.55 ]
 ...
 [4.81  3.814 3.9   ... 4.05  3.752 3.576]
 [4.918 3.875 3.992 ... 4.125 3.84  3.623]
 [6.96  3.633 3.281 ... 3.484 3.61  3.916]]
(801, 11) (801, 11)
By feature:  MSE:  0.5758¬±0.0 MAE:  0.4942¬±0.0 R2:  0.2342¬±0.0 PCC:  0.8011¬±0.0 Cosine Similarity:  0.9898¬±0.0
By sample:  MSE:  0.5758¬±0.0 MAE:  0.4942¬±0.0 R2:  0.2791¬±0.0 PCC:  0.514¬±0.0 Cosine Similarity:  0.9853¬±0.0
scGPT - INFO - By feature: MSE: 0.5758000016212463¬±0.0 MAE: 0.4941999912261963¬±0.0 R2: 0.2342¬±0.0 PCC: 0.8011¬±0.0 Cosine Similarity: 0.9898¬±0.0
scGPT - INFO - By sample: MSE: 0.5758000016212463¬±0.0 MAE: 0.4941999912261963¬±0.0 R2: 0.2791¬±0.0 PCC: 0.514¬±0.0 Cosine Similarity: 0.9853¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  31
Training model
scGPT - INFO - | epoch  31 | 100/3602 batches | lr 0.0000 | ms/batch 92.04 | loss 11.21 | mse 11.21 | mre  0.00 |
scGPT - INFO - | epoch  31 | 200/3602 batches | lr 0.0000 | ms/batch 90.59 | loss 12.50 | mse 12.50 | mre  0.00 |
scGPT - INFO - | epoch  31 | 300/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 10.08 | mse 10.08 | mre  0.00 |
scGPT - INFO - | epoch  31 | 400/3602 batches | lr 0.0000 | ms/batch 90.62 | loss 11.11 | mse 11.11 | mre  0.00 |
scGPT - INFO - | epoch  31 | 500/3602 batches | lr 0.0000 | ms/batch 90.58 | loss  9.72 | mse  9.72 | mre  0.00 |
scGPT - INFO - | epoch  31 | 600/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 11.04 | mse 11.04 | mre  0.00 |
scGPT - INFO - | epoch  31 | 700/3602 batches | lr 0.0000 | ms/batch 90.61 | loss 11.86 | mse 11.86 | mre  0.00 |
scGPT - INFO - | epoch  31 | 800/3602 batches | lr 0.0000 | ms/batch 90.59 | loss 14.43 | mse 14.43 | mre  0.00 |
scGPT - INFO - | epoch  31 | 900/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 11.64 | mse 11.64 | mre  0.00 |
scGPT - INFO - | epoch  31 | 1000/3602 batches | lr 0.0000 | ms/batch 91.19 | loss 12.99 | mse 12.99 | mre  0.00 |
scGPT - INFO - | epoch  31 | 1100/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 11.14 | mse 11.14 | mre  0.00 |
scGPT - INFO - | epoch  31 | 1200/3602 batches | lr 0.0000 | ms/batch 90.62 | loss 11.37 | mse 11.37 | mre  0.00 |
scGPT - INFO - | epoch  31 | 1300/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 10.98 | mse 10.98 | mre  0.00 |
scGPT - INFO - | epoch  31 | 1400/3602 batches | lr 0.0000 | ms/batch 90.60 | loss 13.65 | mse 13.65 | mre  0.00 |
scGPT - INFO - | epoch  31 | 1500/3602 batches | lr 0.0000 | ms/batch 90.62 | loss 13.64 | mse 13.64 | mre  0.00 |
scGPT - INFO - | epoch  31 | 1600/3602 batches | lr 0.0000 | ms/batch 90.65 | loss  9.86 | mse  9.86 | mre  0.00 |
scGPT - INFO - | epoch  31 | 1700/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 10.84 | mse 10.84 | mre  0.00 |
scGPT - INFO - | epoch  31 | 1800/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 11.08 | mse 11.08 | mre  0.00 |
scGPT - INFO - | epoch  31 | 1900/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 13.36 | mse 13.36 | mre  0.00 |
scGPT - INFO - | epoch  31 | 2000/3602 batches | lr 0.0000 | ms/batch 91.17 | loss 11.89 | mse 11.89 | mre  0.00 |
scGPT - INFO - | epoch  31 | 2100/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 13.46 | mse 13.46 | mre  0.00 |
scGPT - INFO - | epoch  31 | 2200/3602 batches | lr 0.0000 | ms/batch 91.01 | loss 10.98 | mse 10.98 | mre  0.00 |
scGPT - INFO - | epoch  31 | 2300/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 11.00 | mse 11.00 | mre  0.00 |
scGPT - INFO - | epoch  31 | 2400/3602 batches | lr 0.0000 | ms/batch 90.99 | loss 11.97 | mse 11.97 | mre  0.00 |
scGPT - INFO - | epoch  31 | 2500/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 11.01 | mse 11.01 | mre  0.00 |
scGPT - INFO - | epoch  31 | 2600/3602 batches | lr 0.0000 | ms/batch 91.03 | loss 14.65 | mse 14.65 | mre  0.00 |
scGPT - INFO - | epoch  31 | 2700/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 10.51 | mse 10.51 | mre  0.00 |
scGPT - INFO - | epoch  31 | 2800/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 11.89 | mse 11.89 | mre  0.00 |
scGPT - INFO - | epoch  31 | 2900/3602 batches | lr 0.0000 | ms/batch 90.91 | loss 11.95 | mse 11.95 | mre  0.00 |
scGPT - INFO - | epoch  31 | 3000/3602 batches | lr 0.0000 | ms/batch 91.34 | loss 12.04 | mse 12.04 | mre  0.00 |
scGPT - INFO - | epoch  31 | 3100/3602 batches | lr 0.0000 | ms/batch 90.91 | loss 10.74 | mse 10.74 | mre  0.00 |
scGPT - INFO - | epoch  31 | 3200/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 11.63 | mse 11.63 | mre  0.00 |
scGPT - INFO - | epoch  31 | 3300/3602 batches | lr 0.0000 | ms/batch 90.86 | loss  9.68 | mse  9.68 | mre  0.00 |
scGPT - INFO - | epoch  31 | 3400/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 10.97 | mse 10.97 | mre  0.00 |
scGPT - INFO - | epoch  31 | 3500/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 10.81 | mse 10.81 | mre  0.00 |
scGPT - INFO - | epoch  31 | 3600/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 11.94 | mse 11.94 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  31 | time: 339.49s | valid loss/mse 12.6787 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.667089693852876, min_delta 0.0001, val_loss 12.678700172276086
Loss error: -0.011610478423209614
scGPT - INFO - INFO: Early stopping counter 1 of 10
epoch:  31
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.836 3.533 3.285 ... 3.488 3.63  3.9  ]
 [7.04  3.598 3.355 ... 3.473 3.676 3.926]
 [4.676 3.635 3.828 ... 4.07  3.691 3.549]
 ...
 [4.816 3.812 3.895 ... 4.055 3.756 3.576]
 [4.926 3.87  3.984 ... 4.12  3.84  3.62 ]
 [6.85  3.66  3.266 ... 3.516 3.594 3.902]]
(801, 11) (801, 11)
By feature:  MSE:  0.5763¬±0.0 MAE:  0.4961¬±0.0 R2:  0.2338¬±0.0 PCC:  0.7998¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5763¬±0.0 MAE:  0.4961¬±0.0 R2:  0.2774¬±0.0 PCC:  0.5129¬±0.0 Cosine Similarity:  0.9853¬±0.0
scGPT - INFO - By feature: MSE: 0.5763000249862671¬±0.0 MAE: 0.4961000084877014¬±0.0 R2: 0.2338¬±0.0 PCC: 0.7998¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5763000249862671¬±0.0 MAE: 0.4961000084877014¬±0.0 R2: 0.2774¬±0.0 PCC: 0.5129¬±0.0 Cosine Similarity: 0.9853¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  32
Training model
scGPT - INFO - | epoch  32 | 100/3602 batches | lr 0.0000 | ms/batch 92.16 | loss 11.44 | mse 11.44 | mre  0.00 |
scGPT - INFO - | epoch  32 | 200/3602 batches | lr 0.0000 | ms/batch 90.91 | loss 12.27 | mse 12.27 | mre  0.00 |
scGPT - INFO - | epoch  32 | 300/3602 batches | lr 0.0000 | ms/batch 90.93 | loss 10.07 | mse 10.07 | mre  0.00 |
scGPT - INFO - | epoch  32 | 400/3602 batches | lr 0.0000 | ms/batch 91.55 | loss 11.03 | mse 11.03 | mre  0.00 |
scGPT - INFO - | epoch  32 | 500/3602 batches | lr 0.0000 | ms/batch 90.91 | loss  9.91 | mse  9.91 | mre  0.00 |
scGPT - INFO - | epoch  32 | 600/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 11.00 | mse 11.00 | mre  0.00 |
scGPT - INFO - | epoch  32 | 700/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 11.76 | mse 11.76 | mre  0.00 |
scGPT - INFO - | epoch  32 | 800/3602 batches | lr 0.0000 | ms/batch 90.93 | loss 14.38 | mse 14.38 | mre  0.00 |
scGPT - INFO - | epoch  32 | 900/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 11.63 | mse 11.63 | mre  0.00 |
scGPT - INFO - | epoch  32 | 1000/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 12.88 | mse 12.88 | mre  0.00 |
scGPT - INFO - | epoch  32 | 1100/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 10.86 | mse 10.86 | mre  0.00 |
scGPT - INFO - | epoch  32 | 1200/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 11.26 | mse 11.26 | mre  0.00 |
scGPT - INFO - | epoch  32 | 1300/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 11.09 | mse 11.09 | mre  0.00 |
scGPT - INFO - | epoch  32 | 1400/3602 batches | lr 0.0000 | ms/batch 91.39 | loss 13.66 | mse 13.66 | mre  0.00 |
scGPT - INFO - | epoch  32 | 1500/3602 batches | lr 0.0000 | ms/batch 90.99 | loss 13.72 | mse 13.72 | mre  0.00 |
scGPT - INFO - | epoch  32 | 1600/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 10.02 | mse 10.02 | mre  0.00 |
scGPT - INFO - | epoch  32 | 1700/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 11.03 | mse 11.03 | mre  0.00 |
scGPT - INFO - | epoch  32 | 1800/3602 batches | lr 0.0000 | ms/batch 90.57 | loss 11.09 | mse 11.09 | mre  0.00 |
scGPT - INFO - | epoch  32 | 1900/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 13.37 | mse 13.37 | mre  0.00 |
scGPT - INFO - | epoch  32 | 2000/3602 batches | lr 0.0000 | ms/batch 91.08 | loss 11.77 | mse 11.77 | mre  0.00 |
scGPT - INFO - | epoch  32 | 2100/3602 batches | lr 0.0000 | ms/batch 91.14 | loss 13.37 | mse 13.37 | mre  0.00 |
scGPT - INFO - | epoch  32 | 2200/3602 batches | lr 0.0000 | ms/batch 91.17 | loss 11.39 | mse 11.39 | mre  0.00 |
scGPT - INFO - | epoch  32 | 2300/3602 batches | lr 0.0000 | ms/batch 91.17 | loss 11.23 | mse 11.23 | mre  0.00 |
scGPT - INFO - | epoch  32 | 2400/3602 batches | lr 0.0000 | ms/batch 91.50 | loss 11.99 | mse 11.99 | mre  0.00 |
scGPT - INFO - | epoch  32 | 2500/3602 batches | lr 0.0000 | ms/batch 91.16 | loss 10.70 | mse 10.70 | mre  0.00 |
scGPT - INFO - | epoch  32 | 2600/3602 batches | lr 0.0000 | ms/batch 91.02 | loss 14.82 | mse 14.82 | mre  0.00 |
scGPT - INFO - | epoch  32 | 2700/3602 batches | lr 0.0000 | ms/batch 91.04 | loss 11.06 | mse 11.06 | mre  0.00 |
scGPT - INFO - | epoch  32 | 2800/3602 batches | lr 0.0000 | ms/batch 91.06 | loss 11.86 | mse 11.86 | mre  0.00 |
scGPT - INFO - | epoch  32 | 2900/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 11.46 | mse 11.46 | mre  0.00 |
scGPT - INFO - | epoch  32 | 3000/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 11.68 | mse 11.68 | mre  0.00 |
scGPT - INFO - | epoch  32 | 3100/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 11.06 | mse 11.06 | mre  0.00 |
scGPT - INFO - | epoch  32 | 3200/3602 batches | lr 0.0000 | ms/batch 91.01 | loss 11.46 | mse 11.46 | mre  0.00 |
scGPT - INFO - | epoch  32 | 3300/3602 batches | lr 0.0000 | ms/batch 90.96 | loss  9.86 | mse  9.86 | mre  0.00 |
scGPT - INFO - | epoch  32 | 3400/3602 batches | lr 0.0000 | ms/batch 91.47 | loss 10.76 | mse 10.76 | mre  0.00 |
scGPT - INFO - | epoch  32 | 3500/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 10.99 | mse 10.99 | mre  0.00 |
scGPT - INFO - | epoch  32 | 3600/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 12.24 | mse 12.24 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  32 | time: 340.20s | valid loss/mse 12.6088 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.6088
best_loss: 12.667089693852876, min_delta 0.0001, val_loss 12.608776481038474
Loss error: 0.058313212814402604
epoch:  32
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.88  3.537 3.287 ... 3.47  3.625 3.902]
 [7.027 3.604 3.36  ... 3.469 3.668 3.924]
 [4.703 3.648 3.85  ... 4.086 3.713 3.564]
 ...
 [4.83  3.82  3.906 ... 4.062 3.764 3.58 ]
 [4.95  3.883 4.004 ... 4.133 3.855 3.63 ]
 [6.867 3.662 3.273 ... 3.508 3.594 3.902]]
(801, 11) (801, 11)
By feature:  MSE:  0.5732¬±0.0 MAE:  0.4955¬±0.0 R2:  0.2395¬±0.0 PCC:  0.8008¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5732¬±0.0 MAE:  0.4955¬±0.0 R2:  0.2816¬±0.0 PCC:  0.5164¬±0.0 Cosine Similarity:  0.9854¬±0.0
scGPT - INFO - By feature: MSE: 0.573199987411499¬±0.0 MAE: 0.49549999833106995¬±0.0 R2: 0.2395¬±0.0 PCC: 0.8008¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.573199987411499¬±0.0 MAE: 0.49549999833106995¬±0.0 R2: 0.2816¬±0.0 PCC: 0.5164¬±0.0 Cosine Similarity: 0.9854¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  33
Training model
scGPT - INFO - | epoch  33 | 100/3602 batches | lr 0.0000 | ms/batch 92.26 | loss 10.53 | mse 10.53 | mre  0.00 |
scGPT - INFO - | epoch  33 | 200/3602 batches | lr 0.0000 | ms/batch 91.18 | loss 12.38 | mse 12.38 | mre  0.00 |
scGPT - INFO - | epoch  33 | 300/3602 batches | lr 0.0000 | ms/batch 91.18 | loss  9.91 | mse  9.91 | mre  0.00 |
scGPT - INFO - | epoch  33 | 400/3602 batches | lr 0.0000 | ms/batch 91.08 | loss 10.88 | mse 10.88 | mre  0.00 |
scGPT - INFO - | epoch  33 | 500/3602 batches | lr 0.0000 | ms/batch 90.88 | loss  9.56 | mse  9.56 | mre  0.00 |
scGPT - INFO - | epoch  33 | 600/3602 batches | lr 0.0000 | ms/batch 91.06 | loss 11.01 | mse 11.01 | mre  0.00 |
scGPT - INFO - | epoch  33 | 700/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 11.20 | mse 11.20 | mre  0.00 |
scGPT - INFO - | epoch  33 | 800/3602 batches | lr 0.0000 | ms/batch 91.57 | loss 14.14 | mse 14.14 | mre  0.00 |
scGPT - INFO - | epoch  33 | 900/3602 batches | lr 0.0000 | ms/batch 91.06 | loss 11.29 | mse 11.29 | mre  0.00 |
scGPT - INFO - | epoch  33 | 1000/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 12.95 | mse 12.95 | mre  0.00 |
scGPT - INFO - | epoch  33 | 1100/3602 batches | lr 0.0000 | ms/batch 90.91 | loss 10.99 | mse 10.99 | mre  0.00 |
scGPT - INFO - | epoch  33 | 1200/3602 batches | lr 0.0000 | ms/batch 91.02 | loss 11.27 | mse 11.27 | mre  0.00 |
scGPT - INFO - | epoch  33 | 1300/3602 batches | lr 0.0000 | ms/batch 90.98 | loss 10.94 | mse 10.94 | mre  0.00 |
scGPT - INFO - | epoch  33 | 1400/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 13.56 | mse 13.56 | mre  0.00 |
scGPT - INFO - | epoch  33 | 1500/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 13.65 | mse 13.65 | mre  0.00 |
scGPT - INFO - | epoch  33 | 1600/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 10.10 | mse 10.10 | mre  0.00 |
scGPT - INFO - | epoch  33 | 1700/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 10.48 | mse 10.48 | mre  0.00 |
scGPT - INFO - | epoch  33 | 1800/3602 batches | lr 0.0000 | ms/batch 91.47 | loss 10.82 | mse 10.82 | mre  0.00 |
scGPT - INFO - | epoch  33 | 1900/3602 batches | lr 0.0000 | ms/batch 90.98 | loss 13.47 | mse 13.47 | mre  0.00 |
scGPT - INFO - | epoch  33 | 2000/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 11.82 | mse 11.82 | mre  0.00 |
scGPT - INFO - | epoch  33 | 2100/3602 batches | lr 0.0000 | ms/batch 91.13 | loss 12.67 | mse 12.67 | mre  0.00 |
scGPT - INFO - | epoch  33 | 2200/3602 batches | lr 0.0000 | ms/batch 91.09 | loss 11.07 | mse 11.07 | mre  0.00 |
scGPT - INFO - | epoch  33 | 2300/3602 batches | lr 0.0000 | ms/batch 91.07 | loss 11.50 | mse 11.50 | mre  0.00 |
scGPT - INFO - | epoch  33 | 2400/3602 batches | lr 0.0000 | ms/batch 91.02 | loss 11.91 | mse 11.91 | mre  0.00 |
scGPT - INFO - | epoch  33 | 2500/3602 batches | lr 0.0000 | ms/batch 91.07 | loss 10.52 | mse 10.52 | mre  0.00 |
scGPT - INFO - | epoch  33 | 2600/3602 batches | lr 0.0000 | ms/batch 91.36 | loss 14.48 | mse 14.48 | mre  0.00 |
scGPT - INFO - | epoch  33 | 2700/3602 batches | lr 0.0000 | ms/batch 91.06 | loss 10.73 | mse 10.73 | mre  0.00 |
scGPT - INFO - | epoch  33 | 2800/3602 batches | lr 0.0000 | ms/batch 91.68 | loss 12.10 | mse 12.10 | mre  0.00 |
scGPT - INFO - | epoch  33 | 2900/3602 batches | lr 0.0000 | ms/batch 91.14 | loss 12.07 | mse 12.07 | mre  0.00 |
scGPT - INFO - | epoch  33 | 3000/3602 batches | lr 0.0000 | ms/batch 91.12 | loss 11.82 | mse 11.82 | mre  0.00 |
scGPT - INFO - | epoch  33 | 3100/3602 batches | lr 0.0000 | ms/batch 91.10 | loss 11.09 | mse 11.09 | mre  0.00 |
scGPT - INFO - | epoch  33 | 3200/3602 batches | lr 0.0000 | ms/batch 91.06 | loss 11.43 | mse 11.43 | mre  0.00 |
scGPT - INFO - | epoch  33 | 3300/3602 batches | lr 0.0000 | ms/batch 91.11 | loss 10.04 | mse 10.04 | mre  0.00 |
scGPT - INFO - | epoch  33 | 3400/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 10.83 | mse 10.83 | mre  0.00 |
scGPT - INFO - | epoch  33 | 3500/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 10.52 | mse 10.52 | mre  0.00 |
scGPT - INFO - | epoch  33 | 3600/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 12.17 | mse 12.17 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  33 | time: 340.37s | valid loss/mse 12.5987 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.5987
best_loss: 12.608776481038474, min_delta 0.0001, val_loss 12.598681693815262
Loss error: 0.010094787223211554
epoch:  33
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.883 3.535 3.281 ... 3.47  3.629 3.902]
 [7.04  3.602 3.35  ... 3.469 3.67  3.928]
 [4.7   3.652 3.838 ... 4.086 3.709 3.566]
 ...
 [4.83  3.826 3.9   ... 4.062 3.768 3.588]
 [4.945 3.883 3.992 ... 4.133 3.855 3.635]
 [6.887 3.645 3.26  ... 3.502 3.594 3.902]]
(801, 11) (801, 11)
By feature:  MSE:  0.5727¬±0.0 MAE:  0.4946¬±0.0 R2:  0.2376¬±0.0 PCC:  0.802¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5727¬±0.0 MAE:  0.4946¬±0.0 R2:  0.2826¬±0.0 PCC:  0.5177¬±0.0 Cosine Similarity:  0.9854¬±0.0
scGPT - INFO - By feature: MSE: 0.572700023651123¬±0.0 MAE: 0.49459999799728394¬±0.0 R2: 0.2376¬±0.0 PCC: 0.802¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.572700023651123¬±0.0 MAE: 0.49459999799728394¬±0.0 R2: 0.2826¬±0.0 PCC: 0.5177¬±0.0 Cosine Similarity: 0.9854¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  34
Training model
scGPT - INFO - | epoch  34 | 100/3602 batches | lr 0.0000 | ms/batch 92.36 | loss 10.62 | mse 10.62 | mre  0.00 |
scGPT - INFO - | epoch  34 | 200/3602 batches | lr 0.0000 | ms/batch 91.37 | loss 12.78 | mse 12.78 | mre  0.00 |
scGPT - INFO - | epoch  34 | 300/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 10.30 | mse 10.30 | mre  0.00 |
scGPT - INFO - | epoch  34 | 400/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 11.02 | mse 11.02 | mre  0.00 |
scGPT - INFO - | epoch  34 | 500/3602 batches | lr 0.0000 | ms/batch 91.15 | loss  9.70 | mse  9.70 | mre  0.00 |
scGPT - INFO - | epoch  34 | 600/3602 batches | lr 0.0000 | ms/batch 91.23 | loss 10.89 | mse 10.89 | mre  0.00 |
scGPT - INFO - | epoch  34 | 700/3602 batches | lr 0.0000 | ms/batch 91.08 | loss 11.82 | mse 11.82 | mre  0.00 |
scGPT - INFO - | epoch  34 | 800/3602 batches | lr 0.0000 | ms/batch 91.09 | loss 14.40 | mse 14.40 | mre  0.00 |
scGPT - INFO - | epoch  34 | 900/3602 batches | lr 0.0000 | ms/batch 91.02 | loss 11.47 | mse 11.47 | mre  0.00 |
scGPT - INFO - | epoch  34 | 1000/3602 batches | lr 0.0000 | ms/batch 91.82 | loss 12.98 | mse 12.98 | mre  0.00 |
scGPT - INFO - | epoch  34 | 1100/3602 batches | lr 0.0000 | ms/batch 96.22 | loss 11.08 | mse 11.08 | mre  0.00 |
scGPT - INFO - | epoch  34 | 1200/3602 batches | lr 0.0000 | ms/batch 96.75 | loss 11.44 | mse 11.44 | mre  0.00 |
scGPT - INFO - | epoch  34 | 1300/3602 batches | lr 0.0000 | ms/batch 96.24 | loss 10.61 | mse 10.61 | mre  0.00 |
scGPT - INFO - | epoch  34 | 1400/3602 batches | lr 0.0000 | ms/batch 96.25 | loss 13.22 | mse 13.22 | mre  0.00 |
scGPT - INFO - | epoch  34 | 1500/3602 batches | lr 0.0000 | ms/batch 96.26 | loss 13.67 | mse 13.67 | mre  0.00 |
scGPT - INFO - | epoch  34 | 1600/3602 batches | lr 0.0000 | ms/batch 96.22 | loss  9.77 | mse  9.77 | mre  0.00 |
scGPT - INFO - | epoch  34 | 1700/3602 batches | lr 0.0000 | ms/batch 96.07 | loss 11.02 | mse 11.02 | mre  0.00 |
scGPT - INFO - | epoch  34 | 1800/3602 batches | lr 0.0000 | ms/batch 96.12 | loss 10.89 | mse 10.89 | mre  0.00 |
scGPT - INFO - | epoch  34 | 1900/3602 batches | lr 0.0000 | ms/batch 96.28 | loss 13.40 | mse 13.40 | mre  0.00 |
scGPT - INFO - | epoch  34 | 2000/3602 batches | lr 0.0000 | ms/batch 96.03 | loss 11.95 | mse 11.95 | mre  0.00 |
scGPT - INFO - | epoch  34 | 2100/3602 batches | lr 0.0000 | ms/batch 96.07 | loss 12.85 | mse 12.85 | mre  0.00 |
scGPT - INFO - | epoch  34 | 2200/3602 batches | lr 0.0000 | ms/batch 96.72 | loss 11.28 | mse 11.28 | mre  0.00 |
scGPT - INFO - | epoch  34 | 2300/3602 batches | lr 0.0000 | ms/batch 96.46 | loss 11.09 | mse 11.09 | mre  0.00 |
scGPT - INFO - | epoch  34 | 2400/3602 batches | lr 0.0000 | ms/batch 96.23 | loss 11.67 | mse 11.67 | mre  0.00 |
scGPT - INFO - | epoch  34 | 2500/3602 batches | lr 0.0000 | ms/batch 96.36 | loss 10.48 | mse 10.48 | mre  0.00 |
scGPT - INFO - | epoch  34 | 2600/3602 batches | lr 0.0000 | ms/batch 96.18 | loss 14.65 | mse 14.65 | mre  0.00 |
scGPT - INFO - | epoch  34 | 2700/3602 batches | lr 0.0000 | ms/batch 96.17 | loss 10.44 | mse 10.44 | mre  0.00 |
scGPT - INFO - | epoch  34 | 2800/3602 batches | lr 0.0000 | ms/batch 96.29 | loss 11.93 | mse 11.93 | mre  0.00 |
scGPT - INFO - | epoch  34 | 2900/3602 batches | lr 0.0000 | ms/batch 96.11 | loss 11.92 | mse 11.92 | mre  0.00 |
scGPT - INFO - | epoch  34 | 3000/3602 batches | lr 0.0000 | ms/batch 96.23 | loss 11.40 | mse 11.40 | mre  0.00 |
scGPT - INFO - | epoch  34 | 3100/3602 batches | lr 0.0000 | ms/batch 96.19 | loss 10.95 | mse 10.95 | mre  0.00 |
scGPT - INFO - | epoch  34 | 3200/3602 batches | lr 0.0000 | ms/batch 94.63 | loss 11.15 | mse 11.15 | mre  0.00 |
scGPT - INFO - | epoch  34 | 3300/3602 batches | lr 0.0000 | ms/batch 90.78 | loss  9.43 | mse  9.43 | mre  0.00 |
scGPT - INFO - | epoch  34 | 3400/3602 batches | lr 0.0000 | ms/batch 91.05 | loss 11.03 | mse 11.03 | mre  0.00 |
scGPT - INFO - | epoch  34 | 3500/3602 batches | lr 0.0000 | ms/batch 90.98 | loss 10.30 | mse 10.30 | mre  0.00 |
scGPT - INFO - | epoch  34 | 3600/3602 batches | lr 0.0000 | ms/batch 91.12 | loss 11.81 | mse 11.81 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  34 | time: 351.78s | valid loss/mse 12.6138 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.598681693815262, min_delta 0.0001, val_loss 12.613807879286014
Loss error: -0.015126185470752063
scGPT - INFO - INFO: Early stopping counter 1 of 10
epoch:  34
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.863 3.535 3.287 ... 3.465 3.633 3.9  ]
 [7.023 3.604 3.355 ... 3.473 3.676 3.928]
 [4.68  3.639 3.828 ... 4.082 3.7   3.557]
 ...
 [4.816 3.822 3.898 ... 4.062 3.758 3.58 ]
 [4.938 3.879 3.99  ... 4.133 3.852 3.629]
 [6.934 3.63  3.262 ... 3.482 3.602 3.908]]
(801, 11) (801, 11)
By feature:  MSE:  0.5734¬±0.0 MAE:  0.4936¬±0.0 R2:  0.2399¬±0.0 PCC:  0.8019¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5734¬±0.0 MAE:  0.4936¬±0.0 R2:  0.2821¬±0.0 PCC:  0.5173¬±0.0 Cosine Similarity:  0.9854¬±0.0
scGPT - INFO - By feature: MSE: 0.5734000205993652¬±0.0 MAE: 0.4936000108718872¬±0.0 R2: 0.2399¬±0.0 PCC: 0.8019¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5734000205993652¬±0.0 MAE: 0.4936000108718872¬±0.0 R2: 0.2821¬±0.0 PCC: 0.5173¬±0.0 Cosine Similarity: 0.9854¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  35
Training model
scGPT - INFO - | epoch  35 | 100/3602 batches | lr 0.0000 | ms/batch 92.20 | loss 11.03 | mse 11.03 | mre  0.00 |
scGPT - INFO - | epoch  35 | 200/3602 batches | lr 0.0000 | ms/batch 91.14 | loss 12.44 | mse 12.44 | mre  0.00 |
scGPT - INFO - | epoch  35 | 300/3602 batches | lr 0.0000 | ms/batch 91.20 | loss 10.15 | mse 10.15 | mre  0.00 |
scGPT - INFO - | epoch  35 | 400/3602 batches | lr 0.0000 | ms/batch 91.17 | loss 11.20 | mse 11.20 | mre  0.00 |
scGPT - INFO - | epoch  35 | 500/3602 batches | lr 0.0000 | ms/batch 91.21 | loss  9.72 | mse  9.72 | mre  0.00 |
scGPT - INFO - | epoch  35 | 600/3602 batches | lr 0.0000 | ms/batch 91.32 | loss 11.34 | mse 11.34 | mre  0.00 |
scGPT - INFO - | epoch  35 | 700/3602 batches | lr 0.0000 | ms/batch 90.91 | loss 11.57 | mse 11.57 | mre  0.00 |
scGPT - INFO - | epoch  35 | 800/3602 batches | lr 0.0000 | ms/batch 90.91 | loss 14.41 | mse 14.41 | mre  0.00 |
scGPT - INFO - | epoch  35 | 900/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 11.35 | mse 11.35 | mre  0.00 |
scGPT - INFO - | epoch  35 | 1000/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 12.82 | mse 12.82 | mre  0.00 |
scGPT - INFO - | epoch  35 | 1100/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 10.63 | mse 10.63 | mre  0.00 |
scGPT - INFO - | epoch  35 | 1200/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 11.06 | mse 11.06 | mre  0.00 |
scGPT - INFO - | epoch  35 | 1300/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 10.59 | mse 10.59 | mre  0.00 |
scGPT - INFO - | epoch  35 | 1400/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 13.63 | mse 13.63 | mre  0.00 |
scGPT - INFO - | epoch  35 | 1500/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 13.33 | mse 13.33 | mre  0.00 |
scGPT - INFO - | epoch  35 | 1600/3602 batches | lr 0.0000 | ms/batch 91.37 | loss  9.75 | mse  9.75 | mre  0.00 |
scGPT - INFO - | epoch  35 | 1700/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 11.14 | mse 11.14 | mre  0.00 |
scGPT - INFO - | epoch  35 | 1800/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 10.47 | mse 10.47 | mre  0.00 |
scGPT - INFO - | epoch  35 | 1900/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 13.44 | mse 13.44 | mre  0.00 |
scGPT - INFO - | epoch  35 | 2000/3602 batches | lr 0.0000 | ms/batch 90.97 | loss 11.85 | mse 11.85 | mre  0.00 |
scGPT - INFO - | epoch  35 | 2100/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 12.73 | mse 12.73 | mre  0.00 |
scGPT - INFO - | epoch  35 | 2200/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 11.48 | mse 11.48 | mre  0.00 |
scGPT - INFO - | epoch  35 | 2300/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 11.03 | mse 11.03 | mre  0.00 |
scGPT - INFO - | epoch  35 | 2400/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 11.77 | mse 11.77 | mre  0.00 |
scGPT - INFO - | epoch  35 | 2500/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 10.57 | mse 10.57 | mre  0.00 |
scGPT - INFO - | epoch  35 | 2600/3602 batches | lr 0.0000 | ms/batch 91.40 | loss 14.33 | mse 14.33 | mre  0.00 |
scGPT - INFO - | epoch  35 | 2700/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 10.63 | mse 10.63 | mre  0.00 |
scGPT - INFO - | epoch  35 | 2800/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 12.00 | mse 12.00 | mre  0.00 |
scGPT - INFO - | epoch  35 | 2900/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 11.73 | mse 11.73 | mre  0.00 |
scGPT - INFO - | epoch  35 | 3000/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 11.35 | mse 11.35 | mre  0.00 |
scGPT - INFO - | epoch  35 | 3100/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 10.31 | mse 10.31 | mre  0.00 |
scGPT - INFO - | epoch  35 | 3200/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 11.20 | mse 11.20 | mre  0.00 |
scGPT - INFO - | epoch  35 | 3300/3602 batches | lr 0.0000 | ms/batch 90.96 | loss  9.81 | mse  9.81 | mre  0.00 |
scGPT - INFO - | epoch  35 | 3400/3602 batches | lr 0.0000 | ms/batch 90.95 | loss 11.03 | mse 11.03 | mre  0.00 |
scGPT - INFO - | epoch  35 | 3500/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 10.48 | mse 10.48 | mre  0.00 |
scGPT - INFO - | epoch  35 | 3600/3602 batches | lr 0.0000 | ms/batch 91.36 | loss 11.69 | mse 11.69 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  35 | time: 340.06s | valid loss/mse 12.6501 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.598681693815262, min_delta 0.0001, val_loss 12.65008388655016
Loss error: -0.05140219273489777
scGPT - INFO - INFO: Early stopping counter 2 of 10
epoch:  35
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.883 3.537 3.293 ... 3.47  3.639 3.906]
 [7.035 3.602 3.361 ... 3.48  3.686 3.934]
 [4.676 3.643 3.832 ... 4.086 3.7   3.559]
 ...
 [4.82  3.828 3.908 ... 4.07  3.766 3.59 ]
 [4.938 3.88  3.994 ... 4.137 3.852 3.633]
 [6.93  3.643 3.271 ... 3.496 3.607 3.914]]
(801, 11) (801, 11)
By feature:  MSE:  0.575¬±0.0 MAE:  0.4942¬±0.0 R2:  0.2375¬±0.0 PCC:  0.8017¬±0.0 Cosine Similarity:  0.9898¬±0.0
By sample:  MSE:  0.575¬±0.0 MAE:  0.4942¬±0.0 R2:  0.2813¬±0.0 PCC:  0.5173¬±0.0 Cosine Similarity:  0.9854¬±0.0
scGPT - INFO - By feature: MSE: 0.574999988079071¬±0.0 MAE: 0.4941999912261963¬±0.0 R2: 0.2375¬±0.0 PCC: 0.8017¬±0.0 Cosine Similarity: 0.9898¬±0.0
scGPT - INFO - By sample: MSE: 0.574999988079071¬±0.0 MAE: 0.4941999912261963¬±0.0 R2: 0.2813¬±0.0 PCC: 0.5173¬±0.0 Cosine Similarity: 0.9854¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  36
Training model
scGPT - INFO - | epoch  36 | 100/3602 batches | lr 0.0000 | ms/batch 92.03 | loss 10.67 | mse 10.67 | mre  0.00 |
scGPT - INFO - | epoch  36 | 200/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 12.34 | mse 12.34 | mre  0.00 |
scGPT - INFO - | epoch  36 | 300/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 10.33 | mse 10.33 | mre  0.00 |
scGPT - INFO - | epoch  36 | 400/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 11.07 | mse 11.07 | mre  0.00 |
scGPT - INFO - | epoch  36 | 500/3602 batches | lr 0.0000 | ms/batch 91.22 | loss  9.78 | mse  9.78 | mre  0.00 |
scGPT - INFO - | epoch  36 | 600/3602 batches | lr 0.0000 | ms/batch 90.97 | loss 10.98 | mse 10.98 | mre  0.00 |
scGPT - INFO - | epoch  36 | 700/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 11.69 | mse 11.69 | mre  0.00 |
scGPT - INFO - | epoch  36 | 800/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 14.36 | mse 14.36 | mre  0.00 |
scGPT - INFO - | epoch  36 | 900/3602 batches | lr 0.0000 | ms/batch 90.58 | loss 11.61 | mse 11.61 | mre  0.00 |
scGPT - INFO - | epoch  36 | 1000/3602 batches | lr 0.0000 | ms/batch 91.07 | loss 12.46 | mse 12.46 | mre  0.00 |
scGPT - INFO - | epoch  36 | 1100/3602 batches | lr 0.0000 | ms/batch 90.53 | loss 11.10 | mse 11.10 | mre  0.00 |
scGPT - INFO - | epoch  36 | 1200/3602 batches | lr 0.0000 | ms/batch 90.54 | loss 11.31 | mse 11.31 | mre  0.00 |
scGPT - INFO - | epoch  36 | 1300/3602 batches | lr 0.0000 | ms/batch 90.60 | loss 10.88 | mse 10.88 | mre  0.00 |
scGPT - INFO - | epoch  36 | 1400/3602 batches | lr 0.0000 | ms/batch 90.55 | loss 13.15 | mse 13.15 | mre  0.00 |
scGPT - INFO - | epoch  36 | 1500/3602 batches | lr 0.0000 | ms/batch 90.58 | loss 13.67 | mse 13.67 | mre  0.00 |
scGPT - INFO - | epoch  36 | 1600/3602 batches | lr 0.0000 | ms/batch 90.63 | loss  9.64 | mse  9.64 | mre  0.00 |
scGPT - INFO - | epoch  36 | 1700/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 10.82 | mse 10.82 | mre  0.00 |
scGPT - INFO - | epoch  36 | 1800/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.68 | mse 10.68 | mre  0.00 |
scGPT - INFO - | epoch  36 | 1900/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 13.24 | mse 13.24 | mre  0.00 |
scGPT - INFO - | epoch  36 | 2000/3602 batches | lr 0.0000 | ms/batch 91.14 | loss 11.52 | mse 11.52 | mre  0.00 |
scGPT - INFO - | epoch  36 | 2100/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 12.57 | mse 12.57 | mre  0.00 |
scGPT - INFO - | epoch  36 | 2200/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 11.36 | mse 11.36 | mre  0.00 |
scGPT - INFO - | epoch  36 | 2300/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 10.95 | mse 10.95 | mre  0.00 |
scGPT - INFO - | epoch  36 | 2400/3602 batches | lr 0.0000 | ms/batch 90.58 | loss 11.76 | mse 11.76 | mre  0.00 |
scGPT - INFO - | epoch  36 | 2500/3602 batches | lr 0.0000 | ms/batch 90.62 | loss 10.88 | mse 10.88 | mre  0.00 |
scGPT - INFO - | epoch  36 | 2600/3602 batches | lr 0.0000 | ms/batch 90.57 | loss 14.32 | mse 14.32 | mre  0.00 |
scGPT - INFO - | epoch  36 | 2700/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 10.48 | mse 10.48 | mre  0.00 |
scGPT - INFO - | epoch  36 | 2800/3602 batches | lr 0.0000 | ms/batch 90.62 | loss 11.86 | mse 11.86 | mre  0.00 |
scGPT - INFO - | epoch  36 | 2900/3602 batches | lr 0.0000 | ms/batch 90.60 | loss 11.38 | mse 11.38 | mre  0.00 |
scGPT - INFO - | epoch  36 | 3000/3602 batches | lr 0.0000 | ms/batch 91.15 | loss 11.40 | mse 11.40 | mre  0.00 |
scGPT - INFO - | epoch  36 | 3100/3602 batches | lr 0.0000 | ms/batch 90.62 | loss 10.95 | mse 10.95 | mre  0.00 |
scGPT - INFO - | epoch  36 | 3200/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 11.41 | mse 11.41 | mre  0.00 |
scGPT - INFO - | epoch  36 | 3300/3602 batches | lr 0.0000 | ms/batch 90.58 | loss  9.60 | mse  9.60 | mre  0.00 |
scGPT - INFO - | epoch  36 | 3400/3602 batches | lr 0.0000 | ms/batch 90.57 | loss 10.82 | mse 10.82 | mre  0.00 |
scGPT - INFO - | epoch  36 | 3500/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 10.33 | mse 10.33 | mre  0.00 |
scGPT - INFO - | epoch  36 | 3600/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 11.61 | mse 11.61 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  36 | time: 339.12s | valid loss/mse 12.6091 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.598681693815262, min_delta 0.0001, val_loss 12.609086501464414
Loss error: -0.010404807649152303
scGPT - INFO - INFO: Early stopping counter 3 of 10
epoch:  36
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.85  3.54  3.285 ... 3.473 3.635 3.896]
 [7.035 3.607 3.355 ... 3.475 3.68  3.926]
 [4.67  3.652 3.828 ... 4.082 3.695 3.55 ]
 ...
 [4.816 3.836 3.906 ... 4.07  3.764 3.584]
 [4.93  3.887 3.99  ... 4.133 3.848 3.625]
 [6.895 3.656 3.264 ... 3.498 3.598 3.9  ]]
(801, 11) (801, 11)
By feature:  MSE:  0.5732¬±0.0 MAE:  0.495¬±0.0 R2:  0.2386¬±0.0 PCC:  0.8007¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5732¬±0.0 MAE:  0.495¬±0.0 R2:  0.2827¬±0.0 PCC:  0.5184¬±0.0 Cosine Similarity:  0.9854¬±0.0
scGPT - INFO - By feature: MSE: 0.573199987411499¬±0.0 MAE: 0.4950000047683716¬±0.0 R2: 0.2386¬±0.0 PCC: 0.8007¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.573199987411499¬±0.0 MAE: 0.4950000047683716¬±0.0 R2: 0.2827¬±0.0 PCC: 0.5184¬±0.0 Cosine Similarity: 0.9854¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  37
Training model
scGPT - INFO - | epoch  37 | 100/3602 batches | lr 0.0000 | ms/batch 92.04 | loss 10.59 | mse 10.59 | mre  0.00 |
scGPT - INFO - | epoch  37 | 200/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 12.52 | mse 12.52 | mre  0.00 |
scGPT - INFO - | epoch  37 | 300/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 10.11 | mse 10.11 | mre  0.00 |
scGPT - INFO - | epoch  37 | 400/3602 batches | lr 0.0000 | ms/batch 91.28 | loss 10.90 | mse 10.90 | mre  0.00 |
scGPT - INFO - | epoch  37 | 500/3602 batches | lr 0.0000 | ms/batch 90.76 | loss  9.59 | mse  9.59 | mre  0.00 |
scGPT - INFO - | epoch  37 | 600/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 10.79 | mse 10.79 | mre  0.00 |
scGPT - INFO - | epoch  37 | 700/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 11.44 | mse 11.44 | mre  0.00 |
scGPT - INFO - | epoch  37 | 800/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 14.03 | mse 14.03 | mre  0.00 |
scGPT - INFO - | epoch  37 | 900/3602 batches | lr 0.0000 | ms/batch 91.02 | loss 11.17 | mse 11.17 | mre  0.00 |
scGPT - INFO - | epoch  37 | 1000/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 12.44 | mse 12.44 | mre  0.00 |
scGPT - INFO - | epoch  37 | 1100/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 10.68 | mse 10.68 | mre  0.00 |
scGPT - INFO - | epoch  37 | 1200/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 11.08 | mse 11.08 | mre  0.00 |
scGPT - INFO - | epoch  37 | 1300/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 10.85 | mse 10.85 | mre  0.00 |
scGPT - INFO - | epoch  37 | 1400/3602 batches | lr 0.0000 | ms/batch 91.25 | loss 13.19 | mse 13.19 | mre  0.00 |
scGPT - INFO - | epoch  37 | 1500/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 13.91 | mse 13.91 | mre  0.00 |
scGPT - INFO - | epoch  37 | 1600/3602 batches | lr 0.0000 | ms/batch 90.79 | loss  9.87 | mse  9.87 | mre  0.00 |
scGPT - INFO - | epoch  37 | 1700/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 10.80 | mse 10.80 | mre  0.00 |
scGPT - INFO - | epoch  37 | 1800/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 10.67 | mse 10.67 | mre  0.00 |
scGPT - INFO - | epoch  37 | 1900/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 12.92 | mse 12.92 | mre  0.00 |
scGPT - INFO - | epoch  37 | 2000/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 11.56 | mse 11.56 | mre  0.00 |
scGPT - INFO - | epoch  37 | 2100/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 13.16 | mse 13.16 | mre  0.00 |
scGPT - INFO - | epoch  37 | 2200/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 11.16 | mse 11.16 | mre  0.00 |
scGPT - INFO - | epoch  37 | 2300/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 11.34 | mse 11.34 | mre  0.00 |
scGPT - INFO - | epoch  37 | 2400/3602 batches | lr 0.0000 | ms/batch 91.32 | loss 11.99 | mse 11.99 | mre  0.00 |
scGPT - INFO - | epoch  37 | 2500/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 10.48 | mse 10.48 | mre  0.00 |
scGPT - INFO - | epoch  37 | 2600/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 14.47 | mse 14.47 | mre  0.00 |
scGPT - INFO - | epoch  37 | 2700/3602 batches | lr 0.0000 | ms/batch 90.97 | loss 10.70 | mse 10.70 | mre  0.00 |
scGPT - INFO - | epoch  37 | 2800/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 11.75 | mse 11.75 | mre  0.00 |
scGPT - INFO - | epoch  37 | 2900/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 11.47 | mse 11.47 | mre  0.00 |
scGPT - INFO - | epoch  37 | 3000/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 11.77 | mse 11.77 | mre  0.00 |
scGPT - INFO - | epoch  37 | 3100/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 10.28 | mse 10.28 | mre  0.00 |
scGPT - INFO - | epoch  37 | 3200/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 11.13 | mse 11.13 | mre  0.00 |
scGPT - INFO - | epoch  37 | 3300/3602 batches | lr 0.0000 | ms/batch 90.85 | loss  9.81 | mse  9.81 | mre  0.00 |
scGPT - INFO - | epoch  37 | 3400/3602 batches | lr 0.0000 | ms/batch 91.27 | loss 10.74 | mse 10.74 | mre  0.00 |
scGPT - INFO - | epoch  37 | 3500/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 10.65 | mse 10.65 | mre  0.00 |
scGPT - INFO - | epoch  37 | 3600/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 11.92 | mse 11.92 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  37 | time: 339.63s | valid loss/mse 12.6151 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.598681693815262, min_delta 0.0001, val_loss 12.615076728527912
Loss error: -0.016395034712649803
scGPT - INFO - INFO: Early stopping counter 4 of 10
epoch:  37
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.863 3.54  3.287 ... 3.469 3.635 3.896]
 [7.023 3.602 3.354 ... 3.475 3.678 3.926]
 [4.68  3.654 3.828 ... 4.082 3.7   3.555]
 ...
 [4.816 3.834 3.902 ... 4.066 3.766 3.584]
 [4.934 3.885 3.986 ... 4.13  3.85  3.627]
 [6.92  3.637 3.264 ... 3.488 3.602 3.9  ]]
(801, 11) (801, 11)
By feature:  MSE:  0.5735¬±0.0 MAE:  0.4942¬±0.0 R2:  0.2379¬±0.0 PCC:  0.8015¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5735¬±0.0 MAE:  0.4942¬±0.0 R2:  0.283¬±0.0 PCC:  0.5184¬±0.0 Cosine Similarity:  0.9854¬±0.0
scGPT - INFO - By feature: MSE: 0.5734999775886536¬±0.0 MAE: 0.4941999912261963¬±0.0 R2: 0.2379¬±0.0 PCC: 0.8015¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5734999775886536¬±0.0 MAE: 0.4941999912261963¬±0.0 R2: 0.283¬±0.0 PCC: 0.5184¬±0.0 Cosine Similarity: 0.9854¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  38
Training model
scGPT - INFO - | epoch  38 | 100/3602 batches | lr 0.0000 | ms/batch 91.81 | loss 10.70 | mse 10.70 | mre  0.00 |
scGPT - INFO - | epoch  38 | 200/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 12.37 | mse 12.37 | mre  0.00 |
scGPT - INFO - | epoch  38 | 300/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 10.01 | mse 10.01 | mre  0.00 |
scGPT - INFO - | epoch  38 | 400/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.85 | mse 10.85 | mre  0.00 |
scGPT - INFO - | epoch  38 | 500/3602 batches | lr 0.0000 | ms/batch 90.69 | loss  9.46 | mse  9.46 | mre  0.00 |
scGPT - INFO - | epoch  38 | 600/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 11.06 | mse 11.06 | mre  0.00 |
scGPT - INFO - | epoch  38 | 700/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 11.79 | mse 11.79 | mre  0.00 |
scGPT - INFO - | epoch  38 | 800/3602 batches | lr 0.0000 | ms/batch 91.24 | loss 14.16 | mse 14.16 | mre  0.00 |
scGPT - INFO - | epoch  38 | 900/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 11.08 | mse 11.08 | mre  0.00 |
scGPT - INFO - | epoch  38 | 1000/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 12.56 | mse 12.56 | mre  0.00 |
scGPT - INFO - | epoch  38 | 1100/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 10.57 | mse 10.57 | mre  0.00 |
scGPT - INFO - | epoch  38 | 1200/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 11.05 | mse 11.05 | mre  0.00 |
scGPT - INFO - | epoch  38 | 1300/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 10.60 | mse 10.60 | mre  0.00 |
scGPT - INFO - | epoch  38 | 1400/3602 batches | lr 0.0000 | ms/batch 90.97 | loss 13.44 | mse 13.44 | mre  0.00 |
scGPT - INFO - | epoch  38 | 1500/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 13.77 | mse 13.77 | mre  0.00 |
scGPT - INFO - | epoch  38 | 1600/3602 batches | lr 0.0000 | ms/batch 90.71 | loss  9.60 | mse  9.60 | mre  0.00 |
scGPT - INFO - | epoch  38 | 1700/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 10.66 | mse 10.66 | mre  0.00 |
scGPT - INFO - | epoch  38 | 1800/3602 batches | lr 0.0000 | ms/batch 91.23 | loss 10.78 | mse 10.78 | mre  0.00 |
scGPT - INFO - | epoch  38 | 1900/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 13.22 | mse 13.22 | mre  0.00 |
scGPT - INFO - | epoch  38 | 2000/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 11.34 | mse 11.34 | mre  0.00 |
scGPT - INFO - | epoch  38 | 2100/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 12.57 | mse 12.57 | mre  0.00 |
scGPT - INFO - | epoch  38 | 2200/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 11.30 | mse 11.30 | mre  0.00 |
scGPT - INFO - | epoch  38 | 2300/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 10.92 | mse 10.92 | mre  0.00 |
scGPT - INFO - | epoch  38 | 2400/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 11.22 | mse 11.22 | mre  0.00 |
scGPT - INFO - | epoch  38 | 2500/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 10.88 | mse 10.88 | mre  0.00 |
scGPT - INFO - | epoch  38 | 2600/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 13.94 | mse 13.94 | mre  0.00 |
scGPT - INFO - | epoch  38 | 2700/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 10.43 | mse 10.43 | mre  0.00 |
scGPT - INFO - | epoch  38 | 2800/3602 batches | lr 0.0000 | ms/batch 91.23 | loss 12.06 | mse 12.06 | mre  0.00 |
scGPT - INFO - | epoch  38 | 2900/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 11.82 | mse 11.82 | mre  0.00 |
scGPT - INFO - | epoch  38 | 3000/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 11.56 | mse 11.56 | mre  0.00 |
scGPT - INFO - | epoch  38 | 3100/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 10.23 | mse 10.23 | mre  0.00 |
scGPT - INFO - | epoch  38 | 3200/3602 batches | lr 0.0000 | ms/batch 91.04 | loss 11.38 | mse 11.38 | mre  0.00 |
scGPT - INFO - | epoch  38 | 3300/3602 batches | lr 0.0000 | ms/batch 90.92 | loss  9.72 | mse  9.72 | mre  0.00 |
scGPT - INFO - | epoch  38 | 3400/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.94 | mse 10.94 | mre  0.00 |
scGPT - INFO - | epoch  38 | 3500/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 10.64 | mse 10.64 | mre  0.00 |
scGPT - INFO - | epoch  38 | 3600/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 12.05 | mse 12.05 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  38 | time: 339.52s | valid loss/mse 12.6128 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.598681693815262, min_delta 0.0001, val_loss 12.612770800138085
Loss error: -0.014089106322822786
scGPT - INFO - INFO: Early stopping counter 5 of 10
epoch:  38
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.88  3.535 3.28  ... 3.469 3.63  3.902]
 [7.03  3.598 3.348 ... 3.475 3.676 3.928]
 [4.7   3.652 3.826 ... 4.086 3.701 3.559]
 ...
 [4.832 3.83  3.898 ... 4.066 3.768 3.588]
 [4.95  3.879 3.982 ... 4.13  3.852 3.629]
 [6.926 3.637 3.26  ... 3.494 3.598 3.904]]
(801, 11) (801, 11)
By feature:  MSE:  0.5734¬±0.0 MAE:  0.4942¬±0.0 R2:  0.2371¬±0.0 PCC:  0.8015¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5734¬±0.0 MAE:  0.4942¬±0.0 R2:  0.2833¬±0.0 PCC:  0.5189¬±0.0 Cosine Similarity:  0.9854¬±0.0
scGPT - INFO - By feature: MSE: 0.5734000205993652¬±0.0 MAE: 0.4941999912261963¬±0.0 R2: 0.2371¬±0.0 PCC: 0.8015¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5734000205993652¬±0.0 MAE: 0.4941999912261963¬±0.0 R2: 0.2833¬±0.0 PCC: 0.5189¬±0.0 Cosine Similarity: 0.9854¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  39
Training model
scGPT - INFO - | epoch  39 | 100/3602 batches | lr 0.0000 | ms/batch 92.13 | loss 11.19 | mse 11.19 | mre  0.00 |
scGPT - INFO - | epoch  39 | 200/3602 batches | lr 0.0000 | ms/batch 91.38 | loss 12.62 | mse 12.62 | mre  0.00 |
scGPT - INFO - | epoch  39 | 300/3602 batches | lr 0.0000 | ms/batch 91.10 | loss  9.98 | mse  9.98 | mre  0.00 |
scGPT - INFO - | epoch  39 | 400/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 11.00 | mse 11.00 | mre  0.00 |
scGPT - INFO - | epoch  39 | 500/3602 batches | lr 0.0000 | ms/batch 90.76 | loss  9.57 | mse  9.57 | mre  0.00 |
scGPT - INFO - | epoch  39 | 600/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 10.87 | mse 10.87 | mre  0.00 |
scGPT - INFO - | epoch  39 | 700/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 11.26 | mse 11.26 | mre  0.00 |
scGPT - INFO - | epoch  39 | 800/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 14.23 | mse 14.23 | mre  0.00 |
scGPT - INFO - | epoch  39 | 900/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 11.19 | mse 11.19 | mre  0.00 |
scGPT - INFO - | epoch  39 | 1000/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 12.51 | mse 12.51 | mre  0.00 |
scGPT - INFO - | epoch  39 | 1100/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 11.38 | mse 11.38 | mre  0.00 |
scGPT - INFO - | epoch  39 | 1200/3602 batches | lr 0.0000 | ms/batch 91.24 | loss 11.44 | mse 11.44 | mre  0.00 |
scGPT - INFO - | epoch  39 | 1300/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 10.90 | mse 10.90 | mre  0.00 |
scGPT - INFO - | epoch  39 | 1400/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 13.51 | mse 13.51 | mre  0.00 |
scGPT - INFO - | epoch  39 | 1500/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 13.83 | mse 13.83 | mre  0.00 |
scGPT - INFO - | epoch  39 | 1600/3602 batches | lr 0.0000 | ms/batch 90.75 | loss  9.77 | mse  9.77 | mre  0.00 |
scGPT - INFO - | epoch  39 | 1700/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 10.57 | mse 10.57 | mre  0.00 |
scGPT - INFO - | epoch  39 | 1800/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 10.51 | mse 10.51 | mre  0.00 |
scGPT - INFO - | epoch  39 | 1900/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 13.65 | mse 13.65 | mre  0.00 |
scGPT - INFO - | epoch  39 | 2000/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 11.57 | mse 11.57 | mre  0.00 |
scGPT - INFO - | epoch  39 | 2100/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 13.03 | mse 13.03 | mre  0.00 |
scGPT - INFO - | epoch  39 | 2200/3602 batches | lr 0.0000 | ms/batch 91.32 | loss 11.11 | mse 11.11 | mre  0.00 |
scGPT - INFO - | epoch  39 | 2300/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 11.20 | mse 11.20 | mre  0.00 |
scGPT - INFO - | epoch  39 | 2400/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 11.83 | mse 11.83 | mre  0.00 |
scGPT - INFO - | epoch  39 | 2500/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 10.38 | mse 10.38 | mre  0.00 |
scGPT - INFO - | epoch  39 | 2600/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 14.21 | mse 14.21 | mre  0.00 |
scGPT - INFO - | epoch  39 | 2700/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.57 | mse 10.57 | mre  0.00 |
scGPT - INFO - | epoch  39 | 2800/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 11.92 | mse 11.92 | mre  0.00 |
scGPT - INFO - | epoch  39 | 2900/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 11.77 | mse 11.77 | mre  0.00 |
scGPT - INFO - | epoch  39 | 3000/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 11.56 | mse 11.56 | mre  0.00 |
scGPT - INFO - | epoch  39 | 3100/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 10.56 | mse 10.56 | mre  0.00 |
scGPT - INFO - | epoch  39 | 3200/3602 batches | lr 0.0000 | ms/batch 91.32 | loss 11.31 | mse 11.31 | mre  0.00 |
scGPT - INFO - | epoch  39 | 3300/3602 batches | lr 0.0000 | ms/batch 91.12 | loss  9.84 | mse  9.84 | mre  0.00 |
scGPT - INFO - | epoch  39 | 3400/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 10.47 | mse 10.47 | mre  0.00 |
scGPT - INFO - | epoch  39 | 3500/3602 batches | lr 0.0000 | ms/batch 90.91 | loss 10.24 | mse 10.24 | mre  0.00 |
scGPT - INFO - | epoch  39 | 3600/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 11.78 | mse 11.78 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  39 | time: 339.68s | valid loss/mse 12.6075 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.598681693815262, min_delta 0.0001, val_loss 12.607470590300327
Loss error: -0.008788896485064868
scGPT - INFO - INFO: Early stopping counter 6 of 10
epoch:  39
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.867 3.553 3.291 ... 3.473 3.646 3.902]
 [7.023 3.617 3.357 ... 3.479 3.69  3.932]
 [4.684 3.666 3.832 ... 4.09  3.713 3.56 ]
 ...
 [4.82  3.848 3.906 ... 4.07  3.781 3.59 ]
 [4.93  3.89  3.988 ... 4.133 3.861 3.629]
 [6.926 3.65  3.27  ... 3.494 3.613 3.908]]
(801, 11) (801, 11)
By feature:  MSE:  0.5731¬±0.0 MAE:  0.4944¬±0.0 R2:  0.2388¬±0.0 PCC:  0.8015¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5731¬±0.0 MAE:  0.4944¬±0.0 R2:  0.2837¬±0.0 PCC:  0.5187¬±0.0 Cosine Similarity:  0.9854¬±0.0
scGPT - INFO - By feature: MSE: 0.5730999708175659¬±0.0 MAE: 0.4943999946117401¬±0.0 R2: 0.2388¬±0.0 PCC: 0.8015¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5730999708175659¬±0.0 MAE: 0.4943999946117401¬±0.0 R2: 0.2837¬±0.0 PCC: 0.5187¬±0.0 Cosine Similarity: 0.9854¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  40
Training model
scGPT - INFO - | epoch  40 | 100/3602 batches | lr 0.0000 | ms/batch 92.00 | loss 10.79 | mse 10.79 | mre  0.00 |
scGPT - INFO - | epoch  40 | 200/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 12.47 | mse 12.47 | mre  0.00 |
scGPT - INFO - | epoch  40 | 300/3602 batches | lr 0.0000 | ms/batch 90.73 | loss  9.79 | mse  9.79 | mre  0.00 |
scGPT - INFO - | epoch  40 | 400/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 10.60 | mse 10.60 | mre  0.00 |
scGPT - INFO - | epoch  40 | 500/3602 batches | lr 0.0000 | ms/batch 90.76 | loss  9.50 | mse  9.50 | mre  0.00 |
scGPT - INFO - | epoch  40 | 600/3602 batches | lr 0.0000 | ms/batch 91.35 | loss 10.97 | mse 10.97 | mre  0.00 |
scGPT - INFO - | epoch  40 | 700/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 11.32 | mse 11.32 | mre  0.00 |
scGPT - INFO - | epoch  40 | 800/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 13.94 | mse 13.94 | mre  0.00 |
scGPT - INFO - | epoch  40 | 900/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 11.22 | mse 11.22 | mre  0.00 |
scGPT - INFO - | epoch  40 | 1000/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 12.64 | mse 12.64 | mre  0.00 |
scGPT - INFO - | epoch  40 | 1100/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 10.82 | mse 10.82 | mre  0.00 |
scGPT - INFO - | epoch  40 | 1200/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 11.24 | mse 11.24 | mre  0.00 |
scGPT - INFO - | epoch  40 | 1300/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 11.11 | mse 11.11 | mre  0.00 |
scGPT - INFO - | epoch  40 | 1400/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 13.65 | mse 13.65 | mre  0.00 |
scGPT - INFO - | epoch  40 | 1500/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 13.76 | mse 13.76 | mre  0.00 |
scGPT - INFO - | epoch  40 | 1600/3602 batches | lr 0.0000 | ms/batch 91.31 | loss  9.96 | mse  9.96 | mre  0.00 |
scGPT - INFO - | epoch  40 | 1700/3602 batches | lr 0.0000 | ms/batch 91.00 | loss 10.79 | mse 10.79 | mre  0.00 |
scGPT - INFO - | epoch  40 | 1800/3602 batches | lr 0.0000 | ms/batch 90.98 | loss 10.59 | mse 10.59 | mre  0.00 |
scGPT - INFO - | epoch  40 | 1900/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 13.06 | mse 13.06 | mre  0.00 |
scGPT - INFO - | epoch  40 | 2000/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 11.62 | mse 11.62 | mre  0.00 |
scGPT - INFO - | epoch  40 | 2100/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 13.14 | mse 13.14 | mre  0.00 |
scGPT - INFO - | epoch  40 | 2200/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 11.11 | mse 11.11 | mre  0.00 |
scGPT - INFO - | epoch  40 | 2300/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 10.97 | mse 10.97 | mre  0.00 |
scGPT - INFO - | epoch  40 | 2400/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 11.79 | mse 11.79 | mre  0.00 |
scGPT - INFO - | epoch  40 | 2500/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 10.51 | mse 10.51 | mre  0.00 |
scGPT - INFO - | epoch  40 | 2600/3602 batches | lr 0.0000 | ms/batch 91.40 | loss 14.57 | mse 14.57 | mre  0.00 |
scGPT - INFO - | epoch  40 | 2700/3602 batches | lr 0.0000 | ms/batch 90.93 | loss 10.50 | mse 10.50 | mre  0.00 |
scGPT - INFO - | epoch  40 | 2800/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 12.15 | mse 12.15 | mre  0.00 |
scGPT - INFO - | epoch  40 | 2900/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 11.70 | mse 11.70 | mre  0.00 |
scGPT - INFO - | epoch  40 | 3000/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 11.67 | mse 11.67 | mre  0.00 |
scGPT - INFO - | epoch  40 | 3100/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 10.74 | mse 10.74 | mre  0.00 |
scGPT - INFO - | epoch  40 | 3200/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 11.19 | mse 11.19 | mre  0.00 |
scGPT - INFO - | epoch  40 | 3300/3602 batches | lr 0.0000 | ms/batch 91.07 | loss  9.76 | mse  9.76 | mre  0.00 |
scGPT - INFO - | epoch  40 | 3400/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.81 | mse 10.81 | mre  0.00 |
scGPT - INFO - | epoch  40 | 3500/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.21 | mse 10.21 | mre  0.00 |
scGPT - INFO - | epoch  40 | 3600/3602 batches | lr 0.0000 | ms/batch 91.24 | loss 11.65 | mse 11.65 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  40 | time: 340.08s | valid loss/mse 12.6166 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.598681693815262, min_delta 0.0001, val_loss 12.616591191916877
Loss error: -0.017909498101614574
scGPT - INFO - INFO: Early stopping counter 7 of 10
epoch:  40
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.863 3.549 3.29  ... 3.467 3.64  3.896]
 [7.023 3.611 3.352 ... 3.47  3.682 3.924]
 [4.688 3.658 3.824 ... 4.086 3.701 3.555]
 ...
 [4.82  3.84  3.898 ... 4.066 3.771 3.582]
 [4.938 3.887 3.982 ... 4.13  3.854 3.625]
 [6.92  3.645 3.266 ... 3.488 3.605 3.9  ]]
(801, 11) (801, 11)
By feature:  MSE:  0.5735¬±0.0 MAE:  0.4943¬±0.0 R2:  0.2391¬±0.0 PCC:  0.8016¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5735¬±0.0 MAE:  0.4943¬±0.0 R2:  0.2836¬±0.0 PCC:  0.5192¬±0.0 Cosine Similarity:  0.9854¬±0.0
scGPT - INFO - By feature: MSE: 0.5734999775886536¬±0.0 MAE: 0.4943000078201294¬±0.0 R2: 0.2391¬±0.0 PCC: 0.8016¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5734999775886536¬±0.0 MAE: 0.4943000078201294¬±0.0 R2: 0.2836¬±0.0 PCC: 0.5192¬±0.0 Cosine Similarity: 0.9854¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  41
Training model
scGPT - INFO - | epoch  41 | 100/3602 batches | lr 0.0000 | ms/batch 92.34 | loss 10.62 | mse 10.62 | mre  0.00 |
scGPT - INFO - | epoch  41 | 200/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 12.47 | mse 12.47 | mre  0.00 |
scGPT - INFO - | epoch  41 | 300/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 10.11 | mse 10.11 | mre  0.00 |
scGPT - INFO - | epoch  41 | 400/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 11.05 | mse 11.05 | mre  0.00 |
scGPT - INFO - | epoch  41 | 500/3602 batches | lr 0.0000 | ms/batch 90.75 | loss  9.49 | mse  9.49 | mre  0.00 |
scGPT - INFO - | epoch  41 | 600/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 10.84 | mse 10.84 | mre  0.00 |
scGPT - INFO - | epoch  41 | 700/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 11.49 | mse 11.49 | mre  0.00 |
scGPT - INFO - | epoch  41 | 800/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 13.95 | mse 13.95 | mre  0.00 |
scGPT - INFO - | epoch  41 | 900/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 11.29 | mse 11.29 | mre  0.00 |
scGPT - INFO - | epoch  41 | 1000/3602 batches | lr 0.0000 | ms/batch 91.32 | loss 12.58 | mse 12.58 | mre  0.00 |
scGPT - INFO - | epoch  41 | 1100/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 10.47 | mse 10.47 | mre  0.00 |
scGPT - INFO - | epoch  41 | 1200/3602 batches | lr 0.0000 | ms/batch 91.21 | loss 11.35 | mse 11.35 | mre  0.00 |
scGPT - INFO - | epoch  41 | 1300/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 10.84 | mse 10.84 | mre  0.00 |
scGPT - INFO - | epoch  41 | 1400/3602 batches | lr 0.0000 | ms/batch 90.93 | loss 13.66 | mse 13.66 | mre  0.00 |
scGPT - INFO - | epoch  41 | 1500/3602 batches | lr 0.0000 | ms/batch 90.99 | loss 13.97 | mse 13.97 | mre  0.00 |
scGPT - INFO - | epoch  41 | 1600/3602 batches | lr 0.0000 | ms/batch 90.89 | loss  9.66 | mse  9.66 | mre  0.00 |
scGPT - INFO - | epoch  41 | 1700/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 10.40 | mse 10.40 | mre  0.00 |
scGPT - INFO - | epoch  41 | 1800/3602 batches | lr 0.0000 | ms/batch 90.91 | loss 10.42 | mse 10.42 | mre  0.00 |
scGPT - INFO - | epoch  41 | 1900/3602 batches | lr 0.0000 | ms/batch 90.98 | loss 13.18 | mse 13.18 | mre  0.00 |
scGPT - INFO - | epoch  41 | 2000/3602 batches | lr 0.0000 | ms/batch 91.33 | loss 11.10 | mse 11.10 | mre  0.00 |
scGPT - INFO - | epoch  41 | 2100/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 12.81 | mse 12.81 | mre  0.00 |
scGPT - INFO - | epoch  41 | 2200/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 10.99 | mse 10.99 | mre  0.00 |
scGPT - INFO - | epoch  41 | 2300/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 11.00 | mse 11.00 | mre  0.00 |
scGPT - INFO - | epoch  41 | 2400/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 11.73 | mse 11.73 | mre  0.00 |
scGPT - INFO - | epoch  41 | 2500/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 10.61 | mse 10.61 | mre  0.00 |
scGPT - INFO - | epoch  41 | 2600/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 14.24 | mse 14.24 | mre  0.00 |
scGPT - INFO - | epoch  41 | 2700/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 10.57 | mse 10.57 | mre  0.00 |
scGPT - INFO - | epoch  41 | 2800/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 11.97 | mse 11.97 | mre  0.00 |
scGPT - INFO - | epoch  41 | 2900/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 11.54 | mse 11.54 | mre  0.00 |
scGPT - INFO - | epoch  41 | 3000/3602 batches | lr 0.0000 | ms/batch 91.29 | loss 11.39 | mse 11.39 | mre  0.00 |
scGPT - INFO - | epoch  41 | 3100/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 10.66 | mse 10.66 | mre  0.00 |
scGPT - INFO - | epoch  41 | 3200/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 11.06 | mse 11.06 | mre  0.00 |
scGPT - INFO - | epoch  41 | 3300/3602 batches | lr 0.0000 | ms/batch 90.75 | loss  9.69 | mse  9.69 | mre  0.00 |
scGPT - INFO - | epoch  41 | 3400/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 10.79 | mse 10.79 | mre  0.00 |
scGPT - INFO - | epoch  41 | 3500/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 10.46 | mse 10.46 | mre  0.00 |
scGPT - INFO - | epoch  41 | 3600/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 12.24 | mse 12.24 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  41 | time: 339.79s | valid loss/mse 12.6059 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.598681693815262, min_delta 0.0001, val_loss 12.605855954720287
Loss error: -0.00717426090502471
scGPT - INFO - INFO: Early stopping counter 8 of 10
epoch:  41
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.875 3.553 3.29  ... 3.463 3.637 3.896]
 [7.027 3.615 3.348 ... 3.469 3.678 3.926]
 [4.695 3.662 3.822 ... 4.086 3.7   3.555]
 ...
 [4.824 3.844 3.896 ... 4.066 3.771 3.582]
 [4.945 3.893 3.982 ... 4.13  3.857 3.627]
 [6.94  3.637 3.264 ... 3.479 3.605 3.9  ]]
(801, 11) (801, 11)
By feature:  MSE:  0.573¬±0.0 MAE:  0.4937¬±0.0 R2:  0.24¬±0.0 PCC:  0.8029¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.573¬±0.0 MAE:  0.4937¬±0.0 R2:  0.2839¬±0.0 PCC:  0.5196¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5730000138282776¬±0.0 MAE: 0.4936999976634979¬±0.0 R2: 0.24¬±0.0 PCC: 0.8029¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5730000138282776¬±0.0 MAE: 0.4936999976634979¬±0.0 R2: 0.2839¬±0.0 PCC: 0.5196¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  42
Training model
scGPT - INFO - | epoch  42 | 100/3602 batches | lr 0.0000 | ms/batch 91.99 | loss 11.06 | mse 11.06 | mre  0.00 |
scGPT - INFO - | epoch  42 | 200/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 12.62 | mse 12.62 | mre  0.00 |
scGPT - INFO - | epoch  42 | 300/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 10.03 | mse 10.03 | mre  0.00 |
scGPT - INFO - | epoch  42 | 400/3602 batches | lr 0.0000 | ms/batch 91.19 | loss 10.90 | mse 10.90 | mre  0.00 |
scGPT - INFO - | epoch  42 | 500/3602 batches | lr 0.0000 | ms/batch 90.66 | loss  9.53 | mse  9.53 | mre  0.00 |
scGPT - INFO - | epoch  42 | 600/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 10.48 | mse 10.48 | mre  0.00 |
scGPT - INFO - | epoch  42 | 700/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 11.66 | mse 11.66 | mre  0.00 |
scGPT - INFO - | epoch  42 | 800/3602 batches | lr 0.0000 | ms/batch 91.24 | loss 14.35 | mse 14.35 | mre  0.00 |
scGPT - INFO - | epoch  42 | 900/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 11.21 | mse 11.21 | mre  0.00 |
scGPT - INFO - | epoch  42 | 1000/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 12.63 | mse 12.63 | mre  0.00 |
scGPT - INFO - | epoch  42 | 1100/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 10.68 | mse 10.68 | mre  0.00 |
scGPT - INFO - | epoch  42 | 1200/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 11.56 | mse 11.56 | mre  0.00 |
scGPT - INFO - | epoch  42 | 1300/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 10.34 | mse 10.34 | mre  0.00 |
scGPT - INFO - | epoch  42 | 1400/3602 batches | lr 0.0000 | ms/batch 91.29 | loss 13.37 | mse 13.37 | mre  0.00 |
scGPT - INFO - | epoch  42 | 1500/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 13.79 | mse 13.79 | mre  0.00 |
scGPT - INFO - | epoch  42 | 1600/3602 batches | lr 0.0000 | ms/batch 90.80 | loss  9.98 | mse  9.98 | mre  0.00 |
scGPT - INFO - | epoch  42 | 1700/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 10.72 | mse 10.72 | mre  0.00 |
scGPT - INFO - | epoch  42 | 1800/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 10.83 | mse 10.83 | mre  0.00 |
scGPT - INFO - | epoch  42 | 1900/3602 batches | lr 0.0000 | ms/batch 90.93 | loss 13.52 | mse 13.52 | mre  0.00 |
scGPT - INFO - | epoch  42 | 2000/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 11.37 | mse 11.37 | mre  0.00 |
scGPT - INFO - | epoch  42 | 2100/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 12.82 | mse 12.82 | mre  0.00 |
scGPT - INFO - | epoch  42 | 2200/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 11.18 | mse 11.18 | mre  0.00 |
scGPT - INFO - | epoch  42 | 2300/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 10.87 | mse 10.87 | mre  0.00 |
scGPT - INFO - | epoch  42 | 2400/3602 batches | lr 0.0000 | ms/batch 91.33 | loss 11.63 | mse 11.63 | mre  0.00 |
scGPT - INFO - | epoch  42 | 2500/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 10.73 | mse 10.73 | mre  0.00 |
scGPT - INFO - | epoch  42 | 2600/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 14.53 | mse 14.53 | mre  0.00 |
scGPT - INFO - | epoch  42 | 2700/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 10.87 | mse 10.87 | mre  0.00 |
scGPT - INFO - | epoch  42 | 2800/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 12.22 | mse 12.22 | mre  0.00 |
scGPT - INFO - | epoch  42 | 2900/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 11.64 | mse 11.64 | mre  0.00 |
scGPT - INFO - | epoch  42 | 3000/3602 batches | lr 0.0000 | ms/batch 91.20 | loss 12.03 | mse 12.03 | mre  0.00 |
scGPT - INFO - | epoch  42 | 3100/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.77 | mse 10.77 | mre  0.00 |
scGPT - INFO - | epoch  42 | 3200/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 11.16 | mse 11.16 | mre  0.00 |
scGPT - INFO - | epoch  42 | 3300/3602 batches | lr 0.0000 | ms/batch 90.71 | loss  9.64 | mse  9.64 | mre  0.00 |
scGPT - INFO - | epoch  42 | 3400/3602 batches | lr 0.0000 | ms/batch 91.30 | loss 10.84 | mse 10.84 | mre  0.00 |
scGPT - INFO - | epoch  42 | 3500/3602 batches | lr 0.0000 | ms/batch 90.95 | loss 10.58 | mse 10.58 | mre  0.00 |
scGPT - INFO - | epoch  42 | 3600/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 11.86 | mse 11.86 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  42 | time: 339.86s | valid loss/mse 12.5928 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.5928
best_loss: 12.598681693815262, min_delta 0.0001, val_loss 12.592818982592236
Loss error: 0.0058627112230258405
epoch:  42
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.863 3.55  3.283 ... 3.463 3.64  3.895]
 [7.03  3.615 3.344 ... 3.469 3.684 3.924]
 [4.688 3.666 3.818 ... 4.09  3.703 3.555]
 ...
 [4.82  3.846 3.895 ... 4.066 3.777 3.584]
 [4.938 3.895 3.979 ... 4.13  3.86  3.625]
 [6.94  3.64  3.26  ... 3.479 3.61  3.9  ]]
(801, 11) (801, 11)
By feature:  MSE:  0.5724¬±0.0 MAE:  0.4937¬±0.0 R2:  0.2398¬±0.0 PCC:  0.8028¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5724¬±0.0 MAE:  0.4937¬±0.0 R2:  0.2846¬±0.0 PCC:  0.5201¬±0.0 Cosine Similarity:  0.9854¬±0.0
scGPT - INFO - By feature: MSE: 0.5723999738693237¬±0.0 MAE: 0.4936999976634979¬±0.0 R2: 0.2398¬±0.0 PCC: 0.8028¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5723999738693237¬±0.0 MAE: 0.4936999976634979¬±0.0 R2: 0.2846¬±0.0 PCC: 0.5201¬±0.0 Cosine Similarity: 0.9854¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  43
Training model
scGPT - INFO - | epoch  43 | 100/3602 batches | lr 0.0000 | ms/batch 92.15 | loss 10.41 | mse 10.41 | mre  0.00 |
scGPT - INFO - | epoch  43 | 200/3602 batches | lr 0.0000 | ms/batch 91.00 | loss 12.39 | mse 12.39 | mre  0.00 |
scGPT - INFO - | epoch  43 | 300/3602 batches | lr 0.0000 | ms/batch 90.87 | loss  9.78 | mse  9.78 | mre  0.00 |
scGPT - INFO - | epoch  43 | 400/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 10.87 | mse 10.87 | mre  0.00 |
scGPT - INFO - | epoch  43 | 500/3602 batches | lr 0.0000 | ms/batch 91.12 | loss  9.33 | mse  9.33 | mre  0.00 |
scGPT - INFO - | epoch  43 | 600/3602 batches | lr 0.0000 | ms/batch 91.33 | loss 11.10 | mse 11.10 | mre  0.00 |
scGPT - INFO - | epoch  43 | 700/3602 batches | lr 0.0000 | ms/batch 91.15 | loss 11.30 | mse 11.30 | mre  0.00 |
scGPT - INFO - | epoch  43 | 800/3602 batches | lr 0.0000 | ms/batch 91.45 | loss 14.01 | mse 14.01 | mre  0.00 |
scGPT - INFO - | epoch  43 | 900/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 11.20 | mse 11.20 | mre  0.00 |
scGPT - INFO - | epoch  43 | 1000/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 12.54 | mse 12.54 | mre  0.00 |
scGPT - INFO - | epoch  43 | 1100/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 10.89 | mse 10.89 | mre  0.00 |
scGPT - INFO - | epoch  43 | 1200/3602 batches | lr 0.0000 | ms/batch 91.09 | loss 11.01 | mse 11.01 | mre  0.00 |
scGPT - INFO - | epoch  43 | 1300/3602 batches | lr 0.0000 | ms/batch 90.97 | loss 10.73 | mse 10.73 | mre  0.00 |
scGPT - INFO - | epoch  43 | 1400/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 13.51 | mse 13.51 | mre  0.00 |
scGPT - INFO - | epoch  43 | 1500/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 14.41 | mse 14.41 | mre  0.00 |
scGPT - INFO - | epoch  43 | 1600/3602 batches | lr 0.0000 | ms/batch 90.75 | loss  9.60 | mse  9.60 | mre  0.00 |
scGPT - INFO - | epoch  43 | 1700/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 10.50 | mse 10.50 | mre  0.00 |
scGPT - INFO - | epoch  43 | 1800/3602 batches | lr 0.0000 | ms/batch 91.29 | loss 10.74 | mse 10.74 | mre  0.00 |
scGPT - INFO - | epoch  43 | 1900/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 13.33 | mse 13.33 | mre  0.00 |
scGPT - INFO - | epoch  43 | 2000/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 11.25 | mse 11.25 | mre  0.00 |
scGPT - INFO - | epoch  43 | 2100/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 12.83 | mse 12.83 | mre  0.00 |
scGPT - INFO - | epoch  43 | 2200/3602 batches | lr 0.0000 | ms/batch 91.09 | loss 11.03 | mse 11.03 | mre  0.00 |
scGPT - INFO - | epoch  43 | 2300/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 11.07 | mse 11.07 | mre  0.00 |
scGPT - INFO - | epoch  43 | 2400/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 11.56 | mse 11.56 | mre  0.00 |
scGPT - INFO - | epoch  43 | 2500/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 10.49 | mse 10.49 | mre  0.00 |
scGPT - INFO - | epoch  43 | 2600/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 14.22 | mse 14.22 | mre  0.00 |
scGPT - INFO - | epoch  43 | 2700/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 10.48 | mse 10.48 | mre  0.00 |
scGPT - INFO - | epoch  43 | 2800/3602 batches | lr 0.0000 | ms/batch 91.46 | loss 11.78 | mse 11.78 | mre  0.00 |
scGPT - INFO - | epoch  43 | 2900/3602 batches | lr 0.0000 | ms/batch 90.91 | loss 12.05 | mse 12.05 | mre  0.00 |
scGPT - INFO - | epoch  43 | 3000/3602 batches | lr 0.0000 | ms/batch 91.00 | loss 11.51 | mse 11.51 | mre  0.00 |
scGPT - INFO - | epoch  43 | 3100/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 11.05 | mse 11.05 | mre  0.00 |
scGPT - INFO - | epoch  43 | 3200/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 11.41 | mse 11.41 | mre  0.00 |
scGPT - INFO - | epoch  43 | 3300/3602 batches | lr 0.0000 | ms/batch 90.92 | loss  9.75 | mse  9.75 | mre  0.00 |
scGPT - INFO - | epoch  43 | 3400/3602 batches | lr 0.0000 | ms/batch 90.98 | loss 10.68 | mse 10.68 | mre  0.00 |
scGPT - INFO - | epoch  43 | 3500/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 10.66 | mse 10.66 | mre  0.00 |
scGPT - INFO - | epoch  43 | 3600/3602 batches | lr 0.0000 | ms/batch 90.91 | loss 12.20 | mse 12.20 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  43 | time: 341.83s | valid loss/mse 12.5823 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.5823
best_loss: 12.592818982592236, min_delta 0.0001, val_loss 12.582348015870942
Loss error: 0.010470966721294772
epoch:  43
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.848 3.553 3.293 ... 3.467 3.639 3.893]
 [7.008 3.615 3.354 ... 3.473 3.682 3.924]
 [4.676 3.668 3.822 ... 4.094 3.7   3.553]
 ...
 [4.81  3.852 3.9   ... 4.07  3.773 3.584]
 [4.92  3.896 3.982 ... 4.133 3.855 3.625]
 [6.934 3.64  3.27  ... 3.48  3.61  3.9  ]]
(801, 11) (801, 11)
By feature:  MSE:  0.572¬±0.0 MAE:  0.4931¬±0.0 R2:  0.2398¬±0.0 PCC:  0.8032¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.572¬±0.0 MAE:  0.4931¬±0.0 R2:  0.2854¬±0.0 PCC:  0.5209¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5720000267028809¬±0.0 MAE: 0.49309998750686646¬±0.0 R2: 0.2398¬±0.0 PCC: 0.8032¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5720000267028809¬±0.0 MAE: 0.49309998750686646¬±0.0 R2: 0.2854¬±0.0 PCC: 0.5209¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  44
Training model
scGPT - INFO - | epoch  44 | 100/3602 batches | lr 0.0000 | ms/batch 92.00 | loss 10.61 | mse 10.61 | mre  0.00 |
scGPT - INFO - | epoch  44 | 200/3602 batches | lr 0.0000 | ms/batch 91.56 | loss 12.26 | mse 12.26 | mre  0.00 |
scGPT - INFO - | epoch  44 | 300/3602 batches | lr 0.0000 | ms/batch 90.91 | loss  9.90 | mse  9.90 | mre  0.00 |
scGPT - INFO - | epoch  44 | 400/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 10.46 | mse 10.46 | mre  0.00 |
scGPT - INFO - | epoch  44 | 500/3602 batches | lr 0.0000 | ms/batch 90.99 | loss  9.20 | mse  9.20 | mre  0.00 |
scGPT - INFO - | epoch  44 | 600/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 10.65 | mse 10.65 | mre  0.00 |
scGPT - INFO - | epoch  44 | 700/3602 batches | lr 0.0000 | ms/batch 91.06 | loss 11.41 | mse 11.41 | mre  0.00 |
scGPT - INFO - | epoch  44 | 800/3602 batches | lr 0.0000 | ms/batch 91.22 | loss 14.20 | mse 14.20 | mre  0.00 |
scGPT - INFO - | epoch  44 | 900/3602 batches | lr 0.0000 | ms/batch 91.05 | loss 11.18 | mse 11.18 | mre  0.00 |
scGPT - INFO - | epoch  44 | 1000/3602 batches | lr 0.0000 | ms/batch 90.97 | loss 12.71 | mse 12.71 | mre  0.00 |
scGPT - INFO - | epoch  44 | 1100/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 10.39 | mse 10.39 | mre  0.00 |
scGPT - INFO - | epoch  44 | 1200/3602 batches | lr 0.0000 | ms/batch 91.64 | loss 11.01 | mse 11.01 | mre  0.00 |
scGPT - INFO - | epoch  44 | 1300/3602 batches | lr 0.0000 | ms/batch 91.24 | loss 10.83 | mse 10.83 | mre  0.00 |
scGPT - INFO - | epoch  44 | 1400/3602 batches | lr 0.0000 | ms/batch 91.04 | loss 13.12 | mse 13.12 | mre  0.00 |
scGPT - INFO - | epoch  44 | 1500/3602 batches | lr 0.0000 | ms/batch 91.14 | loss 14.16 | mse 14.16 | mre  0.00 |
scGPT - INFO - | epoch  44 | 1600/3602 batches | lr 0.0000 | ms/batch 91.20 | loss  9.86 | mse  9.86 | mre  0.00 |
scGPT - INFO - | epoch  44 | 1700/3602 batches | lr 0.0000 | ms/batch 91.01 | loss 10.51 | mse 10.51 | mre  0.00 |
scGPT - INFO - | epoch  44 | 1800/3602 batches | lr 0.0000 | ms/batch 91.13 | loss 10.48 | mse 10.48 | mre  0.00 |
scGPT - INFO - | epoch  44 | 1900/3602 batches | lr 0.0000 | ms/batch 91.15 | loss 12.88 | mse 12.88 | mre  0.00 |
scGPT - INFO - | epoch  44 | 2000/3602 batches | lr 0.0000 | ms/batch 91.12 | loss 11.39 | mse 11.39 | mre  0.00 |
scGPT - INFO - | epoch  44 | 2100/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 12.70 | mse 12.70 | mre  0.00 |
scGPT - INFO - | epoch  44 | 2200/3602 batches | lr 0.0000 | ms/batch 91.35 | loss 11.06 | mse 11.06 | mre  0.00 |
scGPT - INFO - | epoch  44 | 2300/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 10.96 | mse 10.96 | mre  0.00 |
scGPT - INFO - | epoch  44 | 2400/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 11.54 | mse 11.54 | mre  0.00 |
scGPT - INFO - | epoch  44 | 2500/3602 batches | lr 0.0000 | ms/batch 90.98 | loss 10.65 | mse 10.65 | mre  0.00 |
scGPT - INFO - | epoch  44 | 2600/3602 batches | lr 0.0000 | ms/batch 90.91 | loss 13.77 | mse 13.77 | mre  0.00 |
scGPT - INFO - | epoch  44 | 2700/3602 batches | lr 0.0000 | ms/batch 91.03 | loss 10.79 | mse 10.79 | mre  0.00 |
scGPT - INFO - | epoch  44 | 2800/3602 batches | lr 0.0000 | ms/batch 91.18 | loss 11.91 | mse 11.91 | mre  0.00 |
scGPT - INFO - | epoch  44 | 2900/3602 batches | lr 0.0000 | ms/batch 90.99 | loss 11.43 | mse 11.43 | mre  0.00 |
scGPT - INFO - | epoch  44 | 3000/3602 batches | lr 0.0000 | ms/batch 91.11 | loss 11.34 | mse 11.34 | mre  0.00 |
scGPT - INFO - | epoch  44 | 3100/3602 batches | lr 0.0000 | ms/batch 91.03 | loss 10.50 | mse 10.50 | mre  0.00 |
scGPT - INFO - | epoch  44 | 3200/3602 batches | lr 0.0000 | ms/batch 91.55 | loss 11.32 | mse 11.32 | mre  0.00 |
scGPT - INFO - | epoch  44 | 3300/3602 batches | lr 0.0000 | ms/batch 91.00 | loss  9.34 | mse  9.34 | mre  0.00 |
scGPT - INFO - | epoch  44 | 3400/3602 batches | lr 0.0000 | ms/batch 90.98 | loss 10.66 | mse 10.66 | mre  0.00 |
scGPT - INFO - | epoch  44 | 3500/3602 batches | lr 0.0000 | ms/batch 90.99 | loss 10.51 | mse 10.51 | mre  0.00 |
scGPT - INFO - | epoch  44 | 3600/3602 batches | lr 0.0000 | ms/batch 90.97 | loss 12.31 | mse 12.31 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  44 | time: 340.47s | valid loss/mse 12.5998 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.582348015870942, min_delta 0.0001, val_loss 12.599833783064591
Loss error: -0.017485767193649693
scGPT - INFO - INFO: Early stopping counter 1 of 10
epoch:  44
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.86  3.553 3.283 ... 3.463 3.637 3.89 ]
 [7.023 3.62  3.346 ... 3.47  3.68  3.924]
 [4.676 3.662 3.81  ... 4.086 3.69  3.545]
 ...
 [4.81  3.844 3.889 ... 4.066 3.766 3.576]
 [4.92  3.889 3.969 ... 4.125 3.844 3.615]
 [6.93  3.648 3.26  ... 3.482 3.604 3.898]]
(801, 11) (801, 11)
By feature:  MSE:  0.5728¬±0.0 MAE:  0.4934¬±0.0 R2:  0.2384¬±0.0 PCC:  0.8029¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5728¬±0.0 MAE:  0.4934¬±0.0 R2:  0.2846¬±0.0 PCC:  0.5209¬±0.0 Cosine Similarity:  0.9854¬±0.0
scGPT - INFO - By feature: MSE: 0.5727999806404114¬±0.0 MAE: 0.4934000074863434¬±0.0 R2: 0.2384¬±0.0 PCC: 0.8029¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5727999806404114¬±0.0 MAE: 0.4934000074863434¬±0.0 R2: 0.2846¬±0.0 PCC: 0.5209¬±0.0 Cosine Similarity: 0.9854¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  45
Training model
scGPT - INFO - | epoch  45 | 100/3602 batches | lr 0.0000 | ms/batch 92.25 | loss 10.83 | mse 10.83 | mre  0.00 |
scGPT - INFO - | epoch  45 | 200/3602 batches | lr 0.0000 | ms/batch 91.12 | loss 12.69 | mse 12.69 | mre  0.00 |
scGPT - INFO - | epoch  45 | 300/3602 batches | lr 0.0000 | ms/batch 91.12 | loss 10.12 | mse 10.12 | mre  0.00 |
scGPT - INFO - | epoch  45 | 400/3602 batches | lr 0.0000 | ms/batch 91.09 | loss 10.77 | mse 10.77 | mre  0.00 |
scGPT - INFO - | epoch  45 | 500/3602 batches | lr 0.0000 | ms/batch 91.05 | loss  9.58 | mse  9.58 | mre  0.00 |
scGPT - INFO - | epoch  45 | 600/3602 batches | lr 0.0000 | ms/batch 91.96 | loss 10.86 | mse 10.86 | mre  0.00 |
scGPT - INFO - | epoch  45 | 700/3602 batches | lr 0.0000 | ms/batch 91.23 | loss 11.07 | mse 11.07 | mre  0.00 |
scGPT - INFO - | epoch  45 | 800/3602 batches | lr 0.0000 | ms/batch 91.31 | loss 14.03 | mse 14.03 | mre  0.00 |
scGPT - INFO - | epoch  45 | 900/3602 batches | lr 0.0000 | ms/batch 91.08 | loss 11.14 | mse 11.14 | mre  0.00 |
scGPT - INFO - | epoch  45 | 1000/3602 batches | lr 0.0000 | ms/batch 91.01 | loss 12.73 | mse 12.73 | mre  0.00 |
scGPT - INFO - | epoch  45 | 1100/3602 batches | lr 0.0000 | ms/batch 91.04 | loss 10.54 | mse 10.54 | mre  0.00 |
scGPT - INFO - | epoch  45 | 1200/3602 batches | lr 0.0000 | ms/batch 90.95 | loss 11.07 | mse 11.07 | mre  0.00 |
scGPT - INFO - | epoch  45 | 1300/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 10.37 | mse 10.37 | mre  0.00 |
scGPT - INFO - | epoch  45 | 1400/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 13.37 | mse 13.37 | mre  0.00 |
scGPT - INFO - | epoch  45 | 1500/3602 batches | lr 0.0000 | ms/batch 91.23 | loss 13.62 | mse 13.62 | mre  0.00 |
scGPT - INFO - | epoch  45 | 1600/3602 batches | lr 0.0000 | ms/batch 91.54 | loss 10.08 | mse 10.08 | mre  0.00 |
scGPT - INFO - | epoch  45 | 1700/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 10.73 | mse 10.73 | mre  0.00 |
scGPT - INFO - | epoch  45 | 1800/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 10.50 | mse 10.50 | mre  0.00 |
scGPT - INFO - | epoch  45 | 1900/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 13.12 | mse 13.12 | mre  0.00 |
scGPT - INFO - | epoch  45 | 2000/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 11.80 | mse 11.80 | mre  0.00 |
scGPT - INFO - | epoch  45 | 2100/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 13.09 | mse 13.09 | mre  0.00 |
scGPT - INFO - | epoch  45 | 2200/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 10.81 | mse 10.81 | mre  0.00 |
scGPT - INFO - | epoch  45 | 2300/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 10.97 | mse 10.97 | mre  0.00 |
scGPT - INFO - | epoch  45 | 2400/3602 batches | lr 0.0000 | ms/batch 91.31 | loss 11.66 | mse 11.66 | mre  0.00 |
scGPT - INFO - | epoch  45 | 2500/3602 batches | lr 0.0000 | ms/batch 91.26 | loss 10.43 | mse 10.43 | mre  0.00 |
scGPT - INFO - | epoch  45 | 2600/3602 batches | lr 0.0000 | ms/batch 91.66 | loss 14.29 | mse 14.29 | mre  0.00 |
scGPT - INFO - | epoch  45 | 2700/3602 batches | lr 0.0000 | ms/batch 91.01 | loss 10.41 | mse 10.41 | mre  0.00 |
scGPT - INFO - | epoch  45 | 2800/3602 batches | lr 0.0000 | ms/batch 91.09 | loss 11.51 | mse 11.51 | mre  0.00 |
scGPT - INFO - | epoch  45 | 2900/3602 batches | lr 0.0000 | ms/batch 90.95 | loss 11.23 | mse 11.23 | mre  0.00 |
scGPT - INFO - | epoch  45 | 3000/3602 batches | lr 0.0000 | ms/batch 91.08 | loss 11.44 | mse 11.44 | mre  0.00 |
scGPT - INFO - | epoch  45 | 3100/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 10.30 | mse 10.30 | mre  0.00 |
scGPT - INFO - | epoch  45 | 3200/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 10.83 | mse 10.83 | mre  0.00 |
scGPT - INFO - | epoch  45 | 3300/3602 batches | lr 0.0000 | ms/batch 90.88 | loss  9.60 | mse  9.60 | mre  0.00 |
scGPT - INFO - | epoch  45 | 3400/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 10.69 | mse 10.69 | mre  0.00 |
scGPT - INFO - | epoch  45 | 3500/3602 batches | lr 0.0000 | ms/batch 90.83 | loss  9.88 | mse  9.88 | mre  0.00 |
scGPT - INFO - | epoch  45 | 3600/3602 batches | lr 0.0000 | ms/batch 91.45 | loss 11.59 | mse 11.59 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  45 | time: 340.47s | valid loss/mse 12.5923 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.582348015870942, min_delta 0.0001, val_loss 12.592263567016069
Loss error: -0.00991555114512721
scGPT - INFO - INFO: Early stopping counter 2 of 10
epoch:  45
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.863 3.553 3.28  ... 3.46  3.639 3.893]
 [7.02  3.615 3.34  ... 3.467 3.682 3.922]
 [4.688 3.674 3.818 ... 4.09  3.7   3.55 ]
 ...
 [4.824 3.857 3.896 ... 4.07  3.773 3.58 ]
 [4.938 3.902 3.979 ... 4.13  3.855 3.621]
 [6.934 3.648 3.256 ... 3.479 3.604 3.896]]
(801, 11) (801, 11)
By feature:  MSE:  0.5724¬±0.0 MAE:  0.4939¬±0.0 R2:  0.2386¬±0.0 PCC:  0.8028¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5724¬±0.0 MAE:  0.4939¬±0.0 R2:  0.2846¬±0.0 PCC:  0.5209¬±0.0 Cosine Similarity:  0.9854¬±0.0
scGPT - INFO - By feature: MSE: 0.5723999738693237¬±0.0 MAE: 0.49390000104904175¬±0.0 R2: 0.2386¬±0.0 PCC: 0.8028¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5723999738693237¬±0.0 MAE: 0.49390000104904175¬±0.0 R2: 0.2846¬±0.0 PCC: 0.5209¬±0.0 Cosine Similarity: 0.9854¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  46
Training model
scGPT - INFO - | epoch  46 | 100/3602 batches | lr 0.0000 | ms/batch 92.01 | loss 10.98 | mse 10.98 | mre  0.00 |
scGPT - INFO - | epoch  46 | 200/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 12.35 | mse 12.35 | mre  0.00 |
scGPT - INFO - | epoch  46 | 300/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 10.04 | mse 10.04 | mre  0.00 |
scGPT - INFO - | epoch  46 | 400/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 11.00 | mse 11.00 | mre  0.00 |
scGPT - INFO - | epoch  46 | 500/3602 batches | lr 0.0000 | ms/batch 90.85 | loss  9.62 | mse  9.62 | mre  0.00 |
scGPT - INFO - | epoch  46 | 600/3602 batches | lr 0.0000 | ms/batch 91.00 | loss 10.85 | mse 10.85 | mre  0.00 |
scGPT - INFO - | epoch  46 | 700/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 11.21 | mse 11.21 | mre  0.00 |
scGPT - INFO - | epoch  46 | 800/3602 batches | lr 0.0000 | ms/batch 91.02 | loss 14.00 | mse 14.00 | mre  0.00 |
scGPT - INFO - | epoch  46 | 900/3602 batches | lr 0.0000 | ms/batch 91.00 | loss 11.20 | mse 11.20 | mre  0.00 |
scGPT - INFO - | epoch  46 | 1000/3602 batches | lr 0.0000 | ms/batch 91.69 | loss 12.67 | mse 12.67 | mre  0.00 |
scGPT - INFO - | epoch  46 | 1100/3602 batches | lr 0.0000 | ms/batch 91.22 | loss 10.91 | mse 10.91 | mre  0.00 |
scGPT - INFO - | epoch  46 | 1200/3602 batches | lr 0.0000 | ms/batch 91.10 | loss 11.32 | mse 11.32 | mre  0.00 |
scGPT - INFO - | epoch  46 | 1300/3602 batches | lr 0.0000 | ms/batch 91.21 | loss 10.76 | mse 10.76 | mre  0.00 |
scGPT - INFO - | epoch  46 | 1400/3602 batches | lr 0.0000 | ms/batch 91.13 | loss 13.40 | mse 13.40 | mre  0.00 |
scGPT - INFO - | epoch  46 | 1500/3602 batches | lr 0.0000 | ms/batch 91.06 | loss 13.44 | mse 13.44 | mre  0.00 |
scGPT - INFO - | epoch  46 | 1600/3602 batches | lr 0.0000 | ms/batch 91.06 | loss  9.69 | mse  9.69 | mre  0.00 |
scGPT - INFO - | epoch  46 | 1700/3602 batches | lr 0.0000 | ms/batch 91.04 | loss 10.62 | mse 10.62 | mre  0.00 |
scGPT - INFO - | epoch  46 | 1800/3602 batches | lr 0.0000 | ms/batch 91.06 | loss 10.67 | mse 10.67 | mre  0.00 |
scGPT - INFO - | epoch  46 | 1900/3602 batches | lr 0.0000 | ms/batch 90.99 | loss 13.09 | mse 13.09 | mre  0.00 |
scGPT - INFO - | epoch  46 | 2000/3602 batches | lr 0.0000 | ms/batch 91.42 | loss 11.13 | mse 11.13 | mre  0.00 |
scGPT - INFO - | epoch  46 | 2100/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 13.18 | mse 13.18 | mre  0.00 |
scGPT - INFO - | epoch  46 | 2200/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 10.88 | mse 10.88 | mre  0.00 |
scGPT - INFO - | epoch  46 | 2300/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 10.77 | mse 10.77 | mre  0.00 |
scGPT - INFO - | epoch  46 | 2400/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 11.63 | mse 11.63 | mre  0.00 |
scGPT - INFO - | epoch  46 | 2500/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 10.50 | mse 10.50 | mre  0.00 |
scGPT - INFO - | epoch  46 | 2600/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 14.26 | mse 14.26 | mre  0.00 |
scGPT - INFO - | epoch  46 | 2700/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 10.43 | mse 10.43 | mre  0.00 |
scGPT - INFO - | epoch  46 | 2800/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 11.72 | mse 11.72 | mre  0.00 |
scGPT - INFO - | epoch  46 | 2900/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 11.88 | mse 11.88 | mre  0.00 |
scGPT - INFO - | epoch  46 | 3000/3602 batches | lr 0.0000 | ms/batch 91.31 | loss 11.25 | mse 11.25 | mre  0.00 |
scGPT - INFO - | epoch  46 | 3100/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 10.52 | mse 10.52 | mre  0.00 |
scGPT - INFO - | epoch  46 | 3200/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 11.15 | mse 11.15 | mre  0.00 |
scGPT - INFO - | epoch  46 | 3300/3602 batches | lr 0.0000 | ms/batch 90.76 | loss  9.70 | mse  9.70 | mre  0.00 |
scGPT - INFO - | epoch  46 | 3400/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 10.53 | mse 10.53 | mre  0.00 |
scGPT - INFO - | epoch  46 | 3500/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 10.59 | mse 10.59 | mre  0.00 |
scGPT - INFO - | epoch  46 | 3600/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 12.07 | mse 12.07 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  46 | time: 339.97s | valid loss/mse 12.5826 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.582348015870942, min_delta 0.0001, val_loss 12.582645275173116
Loss error: -0.00029725930217416874
scGPT - INFO - INFO: Early stopping counter 3 of 10
epoch:  46
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.86  3.555 3.281 ... 3.463 3.64  3.89 ]
 [7.023 3.62  3.342 ... 3.47  3.684 3.924]
 [4.688 3.676 3.816 ... 4.09  3.701 3.549]
 ...
 [4.82  3.857 3.896 ... 4.07  3.777 3.582]
 [4.934 3.9   3.975 ... 4.13  3.855 3.62 ]
 [6.95  3.646 3.258 ... 3.479 3.61  3.9  ]]
(801, 11) (801, 11)
By feature:  MSE:  0.572¬±0.0 MAE:  0.4934¬±0.0 R2:  0.2382¬±0.0 PCC:  0.8035¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.572¬±0.0 MAE:  0.4934¬±0.0 R2:  0.2856¬±0.0 PCC:  0.5218¬±0.0 Cosine Similarity:  0.9854¬±0.0
scGPT - INFO - By feature: MSE: 0.5720000267028809¬±0.0 MAE: 0.4934000074863434¬±0.0 R2: 0.2382¬±0.0 PCC: 0.8035¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5720000267028809¬±0.0 MAE: 0.4934000074863434¬±0.0 R2: 0.2856¬±0.0 PCC: 0.5218¬±0.0 Cosine Similarity: 0.9854¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  47
Training model
scGPT - INFO - | epoch  47 | 100/3602 batches | lr 0.0000 | ms/batch 91.85 | loss 10.49 | mse 10.49 | mre  0.00 |
scGPT - INFO - | epoch  47 | 200/3602 batches | lr 0.0000 | ms/batch 90.61 | loss 12.63 | mse 12.63 | mre  0.00 |
scGPT - INFO - | epoch  47 | 300/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 10.00 | mse 10.00 | mre  0.00 |
scGPT - INFO - | epoch  47 | 400/3602 batches | lr 0.0000 | ms/batch 91.12 | loss 11.22 | mse 11.22 | mre  0.00 |
scGPT - INFO - | epoch  47 | 500/3602 batches | lr 0.0000 | ms/batch 90.68 | loss  9.47 | mse  9.47 | mre  0.00 |
scGPT - INFO - | epoch  47 | 600/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 11.03 | mse 11.03 | mre  0.00 |
scGPT - INFO - | epoch  47 | 700/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 11.40 | mse 11.40 | mre  0.00 |
scGPT - INFO - | epoch  47 | 800/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 14.02 | mse 14.02 | mre  0.00 |
scGPT - INFO - | epoch  47 | 900/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 11.08 | mse 11.08 | mre  0.00 |
scGPT - INFO - | epoch  47 | 1000/3602 batches | lr 0.0000 | ms/batch 91.20 | loss 12.36 | mse 12.36 | mre  0.00 |
scGPT - INFO - | epoch  47 | 1100/3602 batches | lr 0.0000 | ms/batch 90.61 | loss 10.41 | mse 10.41 | mre  0.00 |
scGPT - INFO - | epoch  47 | 1200/3602 batches | lr 0.0000 | ms/batch 90.59 | loss 11.49 | mse 11.49 | mre  0.00 |
scGPT - INFO - | epoch  47 | 1300/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 10.59 | mse 10.59 | mre  0.00 |
scGPT - INFO - | epoch  47 | 1400/3602 batches | lr 0.0000 | ms/batch 91.19 | loss 13.44 | mse 13.44 | mre  0.00 |
scGPT - INFO - | epoch  47 | 1500/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 13.27 | mse 13.27 | mre  0.00 |
scGPT - INFO - | epoch  47 | 1600/3602 batches | lr 0.0000 | ms/batch 90.81 | loss  9.70 | mse  9.70 | mre  0.00 |
scGPT - INFO - | epoch  47 | 1700/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 10.34 | mse 10.34 | mre  0.00 |
scGPT - INFO - | epoch  47 | 1800/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 10.67 | mse 10.67 | mre  0.00 |
scGPT - INFO - | epoch  47 | 1900/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 13.10 | mse 13.10 | mre  0.00 |
scGPT - INFO - | epoch  47 | 2000/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 11.27 | mse 11.27 | mre  0.00 |
scGPT - INFO - | epoch  47 | 2100/3602 batches | lr 0.0000 | ms/batch 90.59 | loss 12.60 | mse 12.60 | mre  0.00 |
scGPT - INFO - | epoch  47 | 2200/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 10.84 | mse 10.84 | mre  0.00 |
scGPT - INFO - | epoch  47 | 2300/3602 batches | lr 0.0000 | ms/batch 90.60 | loss 10.65 | mse 10.65 | mre  0.00 |
scGPT - INFO - | epoch  47 | 2400/3602 batches | lr 0.0000 | ms/batch 91.24 | loss 11.67 | mse 11.67 | mre  0.00 |
scGPT - INFO - | epoch  47 | 2500/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 10.35 | mse 10.35 | mre  0.00 |
scGPT - INFO - | epoch  47 | 2600/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 14.08 | mse 14.08 | mre  0.00 |
scGPT - INFO - | epoch  47 | 2700/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 10.55 | mse 10.55 | mre  0.00 |
scGPT - INFO - | epoch  47 | 2800/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 11.69 | mse 11.69 | mre  0.00 |
scGPT - INFO - | epoch  47 | 2900/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 11.42 | mse 11.42 | mre  0.00 |
scGPT - INFO - | epoch  47 | 3000/3602 batches | lr 0.0000 | ms/batch 91.03 | loss 11.27 | mse 11.27 | mre  0.00 |
scGPT - INFO - | epoch  47 | 3100/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 10.39 | mse 10.39 | mre  0.00 |
scGPT - INFO - | epoch  47 | 3200/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 10.82 | mse 10.82 | mre  0.00 |
scGPT - INFO - | epoch  47 | 3300/3602 batches | lr 0.0000 | ms/batch 90.73 | loss  9.31 | mse  9.31 | mre  0.00 |
scGPT - INFO - | epoch  47 | 3400/3602 batches | lr 0.0000 | ms/batch 91.21 | loss 10.71 | mse 10.71 | mre  0.00 |
scGPT - INFO - | epoch  47 | 3500/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 10.66 | mse 10.66 | mre  0.00 |
scGPT - INFO - | epoch  47 | 3600/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 11.44 | mse 11.44 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  47 | time: 339.35s | valid loss/mse 12.5703 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.5703
best_loss: 12.582348015870942, min_delta 0.0001, val_loss 12.57034478867694
Loss error: 0.012003227194002264
epoch:  47
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.848 3.56  3.283 ... 3.465 3.643 3.889]
 [7.016 3.625 3.342 ... 3.47  3.686 3.922]
 [4.676 3.68  3.82  ... 4.094 3.703 3.55 ]
 ...
 [4.812 3.861 3.898 ... 4.074 3.78  3.582]
 [4.92  3.904 3.977 ... 4.13  3.857 3.62 ]
 [6.93  3.654 3.258 ... 3.479 3.61  3.896]]
(801, 11) (801, 11)
By feature:  MSE:  0.5714¬±0.0 MAE:  0.4935¬±0.0 R2:  0.2404¬±0.0 PCC:  0.8033¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5714¬±0.0 MAE:  0.4935¬±0.0 R2:  0.2856¬±0.0 PCC:  0.5216¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.571399986743927¬±0.0 MAE: 0.4934999942779541¬±0.0 R2: 0.2404¬±0.0 PCC: 0.8033¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.571399986743927¬±0.0 MAE: 0.4934999942779541¬±0.0 R2: 0.2856¬±0.0 PCC: 0.5216¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  48
Training model
scGPT - INFO - | epoch  48 | 100/3602 batches | lr 0.0000 | ms/batch 91.98 | loss 10.96 | mse 10.96 | mre  0.00 |
scGPT - INFO - | epoch  48 | 200/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 12.35 | mse 12.35 | mre  0.00 |
scGPT - INFO - | epoch  48 | 300/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 10.04 | mse 10.04 | mre  0.00 |
scGPT - INFO - | epoch  48 | 400/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 10.97 | mse 10.97 | mre  0.00 |
scGPT - INFO - | epoch  48 | 500/3602 batches | lr 0.0000 | ms/batch 90.72 | loss  9.76 | mse  9.76 | mre  0.00 |
scGPT - INFO - | epoch  48 | 600/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 10.86 | mse 10.86 | mre  0.00 |
scGPT - INFO - | epoch  48 | 700/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 11.30 | mse 11.30 | mre  0.00 |
scGPT - INFO - | epoch  48 | 800/3602 batches | lr 0.0000 | ms/batch 91.30 | loss 13.88 | mse 13.88 | mre  0.00 |
scGPT - INFO - | epoch  48 | 900/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 11.36 | mse 11.36 | mre  0.00 |
scGPT - INFO - | epoch  48 | 1000/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 13.15 | mse 13.15 | mre  0.00 |
scGPT - INFO - | epoch  48 | 1100/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 10.70 | mse 10.70 | mre  0.00 |
scGPT - INFO - | epoch  48 | 1200/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 10.93 | mse 10.93 | mre  0.00 |
scGPT - INFO - | epoch  48 | 1300/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.78 | mse 10.78 | mre  0.00 |
scGPT - INFO - | epoch  48 | 1400/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 13.36 | mse 13.36 | mre  0.00 |
scGPT - INFO - | epoch  48 | 1500/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 13.47 | mse 13.47 | mre  0.00 |
scGPT - INFO - | epoch  48 | 1600/3602 batches | lr 0.0000 | ms/batch 90.79 | loss  9.38 | mse  9.38 | mre  0.00 |
scGPT - INFO - | epoch  48 | 1700/3602 batches | lr 0.0000 | ms/batch 91.02 | loss 11.00 | mse 11.00 | mre  0.00 |
scGPT - INFO - | epoch  48 | 1800/3602 batches | lr 0.0000 | ms/batch 91.41 | loss 10.39 | mse 10.39 | mre  0.00 |
scGPT - INFO - | epoch  48 | 1900/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 13.20 | mse 13.20 | mre  0.00 |
scGPT - INFO - | epoch  48 | 2000/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 11.85 | mse 11.85 | mre  0.00 |
scGPT - INFO - | epoch  48 | 2100/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 12.52 | mse 12.52 | mre  0.00 |
scGPT - INFO - | epoch  48 | 2200/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.69 | mse 10.69 | mre  0.00 |
scGPT - INFO - | epoch  48 | 2300/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 10.78 | mse 10.78 | mre  0.00 |
scGPT - INFO - | epoch  48 | 2400/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 11.52 | mse 11.52 | mre  0.00 |
scGPT - INFO - | epoch  48 | 2500/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 10.36 | mse 10.36 | mre  0.00 |
scGPT - INFO - | epoch  48 | 2600/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 14.24 | mse 14.24 | mre  0.00 |
scGPT - INFO - | epoch  48 | 2700/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 10.85 | mse 10.85 | mre  0.00 |
scGPT - INFO - | epoch  48 | 2800/3602 batches | lr 0.0000 | ms/batch 91.43 | loss 11.25 | mse 11.25 | mre  0.00 |
scGPT - INFO - | epoch  48 | 2900/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 12.00 | mse 12.00 | mre  0.00 |
scGPT - INFO - | epoch  48 | 3000/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 11.58 | mse 11.58 | mre  0.00 |
scGPT - INFO - | epoch  48 | 3100/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 10.50 | mse 10.50 | mre  0.00 |
scGPT - INFO - | epoch  48 | 3200/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 10.84 | mse 10.84 | mre  0.00 |
scGPT - INFO - | epoch  48 | 3300/3602 batches | lr 0.0000 | ms/batch 90.72 | loss  9.80 | mse  9.80 | mre  0.00 |
scGPT - INFO - | epoch  48 | 3400/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 10.65 | mse 10.65 | mre  0.00 |
scGPT - INFO - | epoch  48 | 3500/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 10.28 | mse 10.28 | mre  0.00 |
scGPT - INFO - | epoch  48 | 3600/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 11.54 | mse 11.54 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  48 | time: 339.65s | valid loss/mse 12.5704 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.57034478867694, min_delta 0.0001, val_loss 12.570380959022655
Loss error: -3.6170345715902386e-05
scGPT - INFO - INFO: Early stopping counter 1 of 10
epoch:  48
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.863 3.559 3.277 ... 3.46  3.64  3.887]
 [7.02  3.621 3.336 ... 3.469 3.682 3.916]
 [4.684 3.682 3.818 ... 4.09  3.703 3.549]
 ...
 [4.816 3.863 3.896 ... 4.07  3.777 3.578]
 [4.93  3.906 3.975 ... 4.125 3.857 3.617]
 [6.934 3.652 3.254 ... 3.477 3.607 3.893]]
(801, 11) (801, 11)
By feature:  MSE:  0.5714¬±0.0 MAE:  0.493¬±0.0 R2:  0.2408¬±0.0 PCC:  0.8038¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5714¬±0.0 MAE:  0.493¬±0.0 R2:  0.2858¬±0.0 PCC:  0.522¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.571399986743927¬±0.0 MAE: 0.49300000071525574¬±0.0 R2: 0.2408¬±0.0 PCC: 0.8038¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.571399986743927¬±0.0 MAE: 0.49300000071525574¬±0.0 R2: 0.2858¬±0.0 PCC: 0.522¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  49
Training model
scGPT - INFO - | epoch  49 | 100/3602 batches | lr 0.0000 | ms/batch 92.00 | loss 10.71 | mse 10.71 | mre  0.00 |
scGPT - INFO - | epoch  49 | 200/3602 batches | lr 0.0000 | ms/batch 91.28 | loss 12.22 | mse 12.22 | mre  0.00 |
scGPT - INFO - | epoch  49 | 300/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 10.20 | mse 10.20 | mre  0.00 |
scGPT - INFO - | epoch  49 | 400/3602 batches | lr 0.0000 | ms/batch 91.48 | loss 10.82 | mse 10.82 | mre  0.00 |
scGPT - INFO - | epoch  49 | 500/3602 batches | lr 0.0000 | ms/batch 92.78 | loss  9.09 | mse  9.09 | mre  0.00 |
scGPT - INFO - | epoch  49 | 600/3602 batches | lr 0.0000 | ms/batch 92.74 | loss 10.61 | mse 10.61 | mre  0.00 |
scGPT - INFO - | epoch  49 | 700/3602 batches | lr 0.0000 | ms/batch 92.81 | loss 11.50 | mse 11.50 | mre  0.00 |
scGPT - INFO - | epoch  49 | 800/3602 batches | lr 0.0000 | ms/batch 92.68 | loss 13.95 | mse 13.95 | mre  0.00 |
scGPT - INFO - | epoch  49 | 900/3602 batches | lr 0.0000 | ms/batch 92.77 | loss 11.23 | mse 11.23 | mre  0.00 |
scGPT - INFO - | epoch  49 | 1000/3602 batches | lr 0.0000 | ms/batch 92.79 | loss 12.44 | mse 12.44 | mre  0.00 |
scGPT - INFO - | epoch  49 | 1100/3602 batches | lr 0.0000 | ms/batch 92.70 | loss 10.52 | mse 10.52 | mre  0.00 |
scGPT - INFO - | epoch  49 | 1200/3602 batches | lr 0.0000 | ms/batch 93.19 | loss 11.17 | mse 11.17 | mre  0.00 |
scGPT - INFO - | epoch  49 | 1300/3602 batches | lr 0.0000 | ms/batch 92.68 | loss 10.74 | mse 10.74 | mre  0.00 |
scGPT - INFO - | epoch  49 | 1400/3602 batches | lr 0.0000 | ms/batch 92.70 | loss 13.52 | mse 13.52 | mre  0.00 |
scGPT - INFO - | epoch  49 | 1500/3602 batches | lr 0.0000 | ms/batch 92.66 | loss 13.88 | mse 13.88 | mre  0.00 |
scGPT - INFO - | epoch  49 | 1600/3602 batches | lr 0.0000 | ms/batch 92.72 | loss  9.69 | mse  9.69 | mre  0.00 |
scGPT - INFO - | epoch  49 | 1700/3602 batches | lr 0.0000 | ms/batch 92.85 | loss 10.62 | mse 10.62 | mre  0.00 |
scGPT - INFO - | epoch  49 | 1800/3602 batches | lr 0.0000 | ms/batch 92.72 | loss 10.70 | mse 10.70 | mre  0.00 |
scGPT - INFO - | epoch  49 | 1900/3602 batches | lr 0.0000 | ms/batch 92.86 | loss 13.14 | mse 13.14 | mre  0.00 |
scGPT - INFO - | epoch  49 | 2000/3602 batches | lr 0.0000 | ms/batch 92.73 | loss 11.26 | mse 11.26 | mre  0.00 |
scGPT - INFO - | epoch  49 | 2100/3602 batches | lr 0.0000 | ms/batch 92.73 | loss 12.95 | mse 12.95 | mre  0.00 |
scGPT - INFO - | epoch  49 | 2200/3602 batches | lr 0.0000 | ms/batch 93.30 | loss 11.10 | mse 11.10 | mre  0.00 |
scGPT - INFO - | epoch  49 | 2300/3602 batches | lr 0.0000 | ms/batch 92.88 | loss 10.86 | mse 10.86 | mre  0.00 |
scGPT - INFO - | epoch  49 | 2400/3602 batches | lr 0.0000 | ms/batch 92.84 | loss 11.62 | mse 11.62 | mre  0.00 |
scGPT - INFO - | epoch  49 | 2500/3602 batches | lr 0.0000 | ms/batch 92.74 | loss 10.14 | mse 10.14 | mre  0.00 |
scGPT - INFO - | epoch  49 | 2600/3602 batches | lr 0.0000 | ms/batch 92.68 | loss 14.42 | mse 14.42 | mre  0.00 |
scGPT - INFO - | epoch  49 | 2700/3602 batches | lr 0.0000 | ms/batch 92.81 | loss 10.41 | mse 10.41 | mre  0.00 |
scGPT - INFO - | epoch  49 | 2800/3602 batches | lr 0.0000 | ms/batch 92.82 | loss 11.58 | mse 11.58 | mre  0.00 |
scGPT - INFO - | epoch  49 | 2900/3602 batches | lr 0.0000 | ms/batch 92.70 | loss 11.42 | mse 11.42 | mre  0.00 |
scGPT - INFO - | epoch  49 | 3000/3602 batches | lr 0.0000 | ms/batch 92.76 | loss 11.46 | mse 11.46 | mre  0.00 |
scGPT - INFO - | epoch  49 | 3100/3602 batches | lr 0.0000 | ms/batch 92.76 | loss 10.47 | mse 10.47 | mre  0.00 |
scGPT - INFO - | epoch  49 | 3200/3602 batches | lr 0.0000 | ms/batch 92.80 | loss 10.99 | mse 10.99 | mre  0.00 |
scGPT - INFO - | epoch  49 | 3300/3602 batches | lr 0.0000 | ms/batch 90.78 | loss  9.36 | mse  9.36 | mre  0.00 |
scGPT - INFO - | epoch  49 | 3400/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 10.77 | mse 10.77 | mre  0.00 |
scGPT - INFO - | epoch  49 | 3500/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 10.40 | mse 10.40 | mre  0.00 |
scGPT - INFO - | epoch  49 | 3600/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 12.06 | mse 12.06 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  49 | time: 345.20s | valid loss/mse 12.5806 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.57034478867694, min_delta 0.0001, val_loss 12.58064201522558
Loss error: -0.010297226548640737
scGPT - INFO - INFO: Early stopping counter 2 of 10
epoch:  49
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.85  3.559 3.281 ... 3.463 3.64  3.887]
 [7.02  3.625 3.34  ... 3.47  3.682 3.92 ]
 [4.68  3.682 3.82  ... 4.094 3.703 3.55 ]
 ...
 [4.81  3.861 3.898 ... 4.07  3.777 3.58 ]
 [4.92  3.904 3.977 ... 4.125 3.855 3.617]
 [6.918 3.658 3.256 ... 3.482 3.605 3.895]]
(801, 11) (801, 11)
By feature:  MSE:  0.5719¬±0.0 MAE:  0.4937¬±0.0 R2:  0.2398¬±0.0 PCC:  0.8032¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5719¬±0.0 MAE:  0.4937¬±0.0 R2:  0.2853¬±0.0 PCC:  0.5215¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5719000101089478¬±0.0 MAE: 0.4936999976634979¬±0.0 R2: 0.2398¬±0.0 PCC: 0.8032¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5719000101089478¬±0.0 MAE: 0.4936999976634979¬±0.0 R2: 0.2853¬±0.0 PCC: 0.5215¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  50
Training model
scGPT - INFO - | epoch  50 | 100/3602 batches | lr 0.0000 | ms/batch 92.08 | loss 10.70 | mse 10.70 | mre  0.00 |
scGPT - INFO - | epoch  50 | 200/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 12.12 | mse 12.12 | mre  0.00 |
scGPT - INFO - | epoch  50 | 300/3602 batches | lr 0.0000 | ms/batch 90.86 | loss  9.97 | mse  9.97 | mre  0.00 |
scGPT - INFO - | epoch  50 | 400/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 10.74 | mse 10.74 | mre  0.00 |
scGPT - INFO - | epoch  50 | 500/3602 batches | lr 0.0000 | ms/batch 90.91 | loss  9.60 | mse  9.60 | mre  0.00 |
scGPT - INFO - | epoch  50 | 600/3602 batches | lr 0.0000 | ms/batch 91.42 | loss 10.55 | mse 10.55 | mre  0.00 |
scGPT - INFO - | epoch  50 | 700/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 11.34 | mse 11.34 | mre  0.00 |
scGPT - INFO - | epoch  50 | 800/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 14.12 | mse 14.12 | mre  0.00 |
scGPT - INFO - | epoch  50 | 900/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 11.37 | mse 11.37 | mre  0.00 |
scGPT - INFO - | epoch  50 | 1000/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 12.58 | mse 12.58 | mre  0.00 |
scGPT - INFO - | epoch  50 | 1100/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 10.68 | mse 10.68 | mre  0.00 |
scGPT - INFO - | epoch  50 | 1200/3602 batches | lr 0.0000 | ms/batch 90.91 | loss 11.30 | mse 11.30 | mre  0.00 |
scGPT - INFO - | epoch  50 | 1300/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 10.94 | mse 10.94 | mre  0.00 |
scGPT - INFO - | epoch  50 | 1400/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 13.29 | mse 13.29 | mre  0.00 |
scGPT - INFO - | epoch  50 | 1500/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 13.55 | mse 13.55 | mre  0.00 |
scGPT - INFO - | epoch  50 | 1600/3602 batches | lr 0.0000 | ms/batch 91.59 | loss  9.50 | mse  9.50 | mre  0.00 |
scGPT - INFO - | epoch  50 | 1700/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 10.43 | mse 10.43 | mre  0.00 |
scGPT - INFO - | epoch  50 | 1800/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 10.58 | mse 10.58 | mre  0.00 |
scGPT - INFO - | epoch  50 | 1900/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 12.88 | mse 12.88 | mre  0.00 |
scGPT - INFO - | epoch  50 | 2000/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 11.32 | mse 11.32 | mre  0.00 |
scGPT - INFO - | epoch  50 | 2100/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 12.70 | mse 12.70 | mre  0.00 |
scGPT - INFO - | epoch  50 | 2200/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 10.89 | mse 10.89 | mre  0.00 |
scGPT - INFO - | epoch  50 | 2300/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 11.14 | mse 11.14 | mre  0.00 |
scGPT - INFO - | epoch  50 | 2400/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 11.48 | mse 11.48 | mre  0.00 |
scGPT - INFO - | epoch  50 | 2500/3602 batches | lr 0.0000 | ms/batch 90.99 | loss 10.12 | mse 10.12 | mre  0.00 |
scGPT - INFO - | epoch  50 | 2600/3602 batches | lr 0.0000 | ms/batch 91.43 | loss 14.41 | mse 14.41 | mre  0.00 |
scGPT - INFO - | epoch  50 | 2700/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 10.51 | mse 10.51 | mre  0.00 |
scGPT - INFO - | epoch  50 | 2800/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 12.18 | mse 12.18 | mre  0.00 |
scGPT - INFO - | epoch  50 | 2900/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 11.97 | mse 11.97 | mre  0.00 |
scGPT - INFO - | epoch  50 | 3000/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 11.29 | mse 11.29 | mre  0.00 |
scGPT - INFO - | epoch  50 | 3100/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 10.53 | mse 10.53 | mre  0.00 |
scGPT - INFO - | epoch  50 | 3200/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 11.41 | mse 11.41 | mre  0.00 |
scGPT - INFO - | epoch  50 | 3300/3602 batches | lr 0.0000 | ms/batch 90.85 | loss  9.32 | mse  9.32 | mre  0.00 |
scGPT - INFO - | epoch  50 | 3400/3602 batches | lr 0.0000 | ms/batch 90.95 | loss 10.95 | mse 10.95 | mre  0.00 |
scGPT - INFO - | epoch  50 | 3500/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 10.73 | mse 10.73 | mre  0.00 |
scGPT - INFO - | epoch  50 | 3600/3602 batches | lr 0.0000 | ms/batch 91.31 | loss 11.72 | mse 11.72 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  50 | time: 339.92s | valid loss/mse 12.5720 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.57034478867694, min_delta 0.0001, val_loss 12.571951831622368
Loss error: -0.0016070429454284607
scGPT - INFO - INFO: Early stopping counter 3 of 10
epoch:  50
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.85  3.562 3.28  ... 3.46  3.643 3.885]
 [7.01  3.625 3.338 ... 3.467 3.684 3.916]
 [4.688 3.686 3.818 ... 4.09  3.707 3.55 ]
 ...
 [4.816 3.865 3.895 ... 4.07  3.78  3.58 ]
 [4.926 3.906 3.973 ... 4.125 3.857 3.617]
 [6.93  3.654 3.252 ... 3.477 3.607 3.89 ]]
(801, 11) (801, 11)
By feature:  MSE:  0.5715¬±0.0 MAE:  0.4928¬±0.0 R2:  0.2407¬±0.0 PCC:  0.8042¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5715¬±0.0 MAE:  0.4928¬±0.0 R2:  0.286¬±0.0 PCC:  0.522¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5715000033378601¬±0.0 MAE: 0.4927999973297119¬±0.0 R2: 0.2407¬±0.0 PCC: 0.8042¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5715000033378601¬±0.0 MAE: 0.4927999973297119¬±0.0 R2: 0.286¬±0.0 PCC: 0.522¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  51
Training model
scGPT - INFO - | epoch  51 | 100/3602 batches | lr 0.0000 | ms/batch 91.92 | loss 11.08 | mse 11.08 | mre  0.00 |
scGPT - INFO - | epoch  51 | 200/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 12.64 | mse 12.64 | mre  0.00 |
scGPT - INFO - | epoch  51 | 300/3602 batches | lr 0.0000 | ms/batch 90.76 | loss  9.82 | mse  9.82 | mre  0.00 |
scGPT - INFO - | epoch  51 | 400/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 10.59 | mse 10.59 | mre  0.00 |
scGPT - INFO - | epoch  51 | 500/3602 batches | lr 0.0000 | ms/batch 90.68 | loss  9.62 | mse  9.62 | mre  0.00 |
scGPT - INFO - | epoch  51 | 600/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 10.72 | mse 10.72 | mre  0.00 |
scGPT - INFO - | epoch  51 | 700/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 11.41 | mse 11.41 | mre  0.00 |
scGPT - INFO - | epoch  51 | 800/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 14.20 | mse 14.20 | mre  0.00 |
scGPT - INFO - | epoch  51 | 900/3602 batches | lr 0.0000 | ms/batch 91.38 | loss 11.28 | mse 11.28 | mre  0.00 |
scGPT - INFO - | epoch  51 | 1000/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 12.65 | mse 12.65 | mre  0.00 |
scGPT - INFO - | epoch  51 | 1100/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 10.80 | mse 10.80 | mre  0.00 |
scGPT - INFO - | epoch  51 | 1200/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 10.96 | mse 10.96 | mre  0.00 |
scGPT - INFO - | epoch  51 | 1300/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 10.64 | mse 10.64 | mre  0.00 |
scGPT - INFO - | epoch  51 | 1400/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 13.20 | mse 13.20 | mre  0.00 |
scGPT - INFO - | epoch  51 | 1500/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 13.54 | mse 13.54 | mre  0.00 |
scGPT - INFO - | epoch  51 | 1600/3602 batches | lr 0.0000 | ms/batch 90.72 | loss  9.67 | mse  9.67 | mre  0.00 |
scGPT - INFO - | epoch  51 | 1700/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 10.22 | mse 10.22 | mre  0.00 |
scGPT - INFO - | epoch  51 | 1800/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 10.77 | mse 10.77 | mre  0.00 |
scGPT - INFO - | epoch  51 | 1900/3602 batches | lr 0.0000 | ms/batch 91.23 | loss 13.01 | mse 13.01 | mre  0.00 |
scGPT - INFO - | epoch  51 | 2000/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 11.31 | mse 11.31 | mre  0.00 |
scGPT - INFO - | epoch  51 | 2100/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 12.94 | mse 12.94 | mre  0.00 |
scGPT - INFO - | epoch  51 | 2200/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 10.81 | mse 10.81 | mre  0.00 |
scGPT - INFO - | epoch  51 | 2300/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 10.69 | mse 10.69 | mre  0.00 |
scGPT - INFO - | epoch  51 | 2400/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 11.98 | mse 11.98 | mre  0.00 |
scGPT - INFO - | epoch  51 | 2500/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 10.18 | mse 10.18 | mre  0.00 |
scGPT - INFO - | epoch  51 | 2600/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 14.56 | mse 14.56 | mre  0.00 |
scGPT - INFO - | epoch  51 | 2700/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 10.47 | mse 10.47 | mre  0.00 |
scGPT - INFO - | epoch  51 | 2800/3602 batches | lr 0.0000 | ms/batch 91.53 | loss 11.56 | mse 11.56 | mre  0.00 |
scGPT - INFO - | epoch  51 | 2900/3602 batches | lr 0.0000 | ms/batch 92.55 | loss 11.52 | mse 11.52 | mre  0.00 |
scGPT - INFO - | epoch  51 | 3000/3602 batches | lr 0.0000 | ms/batch 91.75 | loss 11.45 | mse 11.45 | mre  0.00 |
scGPT - INFO - | epoch  51 | 3100/3602 batches | lr 0.0000 | ms/batch 91.63 | loss 10.23 | mse 10.23 | mre  0.00 |
scGPT - INFO - | epoch  51 | 3200/3602 batches | lr 0.0000 | ms/batch 91.57 | loss 11.02 | mse 11.02 | mre  0.00 |
scGPT - INFO - | epoch  51 | 3300/3602 batches | lr 0.0000 | ms/batch 91.57 | loss  9.89 | mse  9.89 | mre  0.00 |
scGPT - INFO - | epoch  51 | 3400/3602 batches | lr 0.0000 | ms/batch 91.57 | loss 10.82 | mse 10.82 | mre  0.00 |
scGPT - INFO - | epoch  51 | 3500/3602 batches | lr 0.0000 | ms/batch 91.61 | loss 10.51 | mse 10.51 | mre  0.00 |
scGPT - INFO - | epoch  51 | 3600/3602 batches | lr 0.0000 | ms/batch 91.68 | loss 11.75 | mse 11.75 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  51 | time: 340.18s | valid loss/mse 12.5747 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.57034478867694, min_delta 0.0001, val_loss 12.574654320652565
Loss error: -0.004309531975625447
scGPT - INFO - INFO: Early stopping counter 4 of 10
epoch:  51
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.848 3.562 3.28  ... 3.46  3.643 3.883]
 [7.008 3.625 3.336 ... 3.467 3.684 3.916]
 [4.684 3.688 3.818 ... 4.094 3.705 3.55 ]
 ...
 [4.816 3.867 3.896 ... 4.07  3.781 3.58 ]
 [4.926 3.91  3.975 ... 4.125 3.86  3.617]
 [6.93  3.654 3.252 ... 3.475 3.607 3.89 ]]
(801, 11) (801, 11)
By feature:  MSE:  0.5716¬±0.0 MAE:  0.4929¬±0.0 R2:  0.2405¬±0.0 PCC:  0.8038¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5716¬±0.0 MAE:  0.4929¬±0.0 R2:  0.2859¬±0.0 PCC:  0.5219¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5716000199317932¬±0.0 MAE: 0.492900013923645¬±0.0 R2: 0.2405¬±0.0 PCC: 0.8038¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5716000199317932¬±0.0 MAE: 0.492900013923645¬±0.0 R2: 0.2859¬±0.0 PCC: 0.5219¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  52
Training model
scGPT - INFO - | epoch  52 | 100/3602 batches | lr 0.0000 | ms/batch 92.74 | loss 10.78 | mse 10.78 | mre  0.00 |
scGPT - INFO - | epoch  52 | 200/3602 batches | lr 0.0000 | ms/batch 91.63 | loss 12.21 | mse 12.21 | mre  0.00 |
scGPT - INFO - | epoch  52 | 300/3602 batches | lr 0.0000 | ms/batch 92.46 | loss 10.13 | mse 10.13 | mre  0.00 |
scGPT - INFO - | epoch  52 | 400/3602 batches | lr 0.0000 | ms/batch 91.53 | loss 10.82 | mse 10.82 | mre  0.00 |
scGPT - INFO - | epoch  52 | 500/3602 batches | lr 0.0000 | ms/batch 91.62 | loss  9.24 | mse  9.24 | mre  0.00 |
scGPT - INFO - | epoch  52 | 600/3602 batches | lr 0.0000 | ms/batch 91.53 | loss 10.96 | mse 10.96 | mre  0.00 |
scGPT - INFO - | epoch  52 | 700/3602 batches | lr 0.0000 | ms/batch 91.70 | loss 11.02 | mse 11.02 | mre  0.00 |
scGPT - INFO - | epoch  52 | 800/3602 batches | lr 0.0000 | ms/batch 91.77 | loss 14.16 | mse 14.16 | mre  0.00 |
scGPT - INFO - | epoch  52 | 900/3602 batches | lr 0.0000 | ms/batch 91.67 | loss 11.17 | mse 11.17 | mre  0.00 |
scGPT - INFO - | epoch  52 | 1000/3602 batches | lr 0.0000 | ms/batch 91.71 | loss 12.59 | mse 12.59 | mre  0.00 |
scGPT - INFO - | epoch  52 | 1100/3602 batches | lr 0.0000 | ms/batch 91.69 | loss 10.77 | mse 10.77 | mre  0.00 |
scGPT - INFO - | epoch  52 | 1200/3602 batches | lr 0.0000 | ms/batch 91.71 | loss 10.86 | mse 10.86 | mre  0.00 |
scGPT - INFO - | epoch  52 | 1300/3602 batches | lr 0.0000 | ms/batch 92.52 | loss 10.58 | mse 10.58 | mre  0.00 |
scGPT - INFO - | epoch  52 | 1400/3602 batches | lr 0.0000 | ms/batch 91.70 | loss 13.42 | mse 13.42 | mre  0.00 |
scGPT - INFO - | epoch  52 | 1500/3602 batches | lr 0.0000 | ms/batch 91.72 | loss 13.73 | mse 13.73 | mre  0.00 |
scGPT - INFO - | epoch  52 | 1600/3602 batches | lr 0.0000 | ms/batch 91.69 | loss  9.69 | mse  9.69 | mre  0.00 |
scGPT - INFO - | epoch  52 | 1700/3602 batches | lr 0.0000 | ms/batch 91.66 | loss 10.36 | mse 10.36 | mre  0.00 |
scGPT - INFO - | epoch  52 | 1800/3602 batches | lr 0.0000 | ms/batch 91.62 | loss 10.65 | mse 10.65 | mre  0.00 |
scGPT - INFO - | epoch  52 | 1900/3602 batches | lr 0.0000 | ms/batch 91.68 | loss 12.93 | mse 12.93 | mre  0.00 |
scGPT - INFO - | epoch  52 | 2000/3602 batches | lr 0.0000 | ms/batch 91.69 | loss 10.84 | mse 10.84 | mre  0.00 |
scGPT - INFO - | epoch  52 | 2100/3602 batches | lr 0.0000 | ms/batch 91.67 | loss 12.83 | mse 12.83 | mre  0.00 |
scGPT - INFO - | epoch  52 | 2200/3602 batches | lr 0.0000 | ms/batch 91.70 | loss 11.64 | mse 11.64 | mre  0.00 |
scGPT - INFO - | epoch  52 | 2300/3602 batches | lr 0.0000 | ms/batch 92.47 | loss 10.88 | mse 10.88 | mre  0.00 |
scGPT - INFO - | epoch  52 | 2400/3602 batches | lr 0.0000 | ms/batch 91.62 | loss 11.64 | mse 11.64 | mre  0.00 |
scGPT - INFO - | epoch  52 | 2500/3602 batches | lr 0.0000 | ms/batch 91.65 | loss 10.30 | mse 10.30 | mre  0.00 |
scGPT - INFO - | epoch  52 | 2600/3602 batches | lr 0.0000 | ms/batch 91.65 | loss 14.33 | mse 14.33 | mre  0.00 |
scGPT - INFO - | epoch  52 | 2700/3602 batches | lr 0.0000 | ms/batch 91.79 | loss 10.60 | mse 10.60 | mre  0.00 |
scGPT - INFO - | epoch  52 | 2800/3602 batches | lr 0.0000 | ms/batch 91.91 | loss 11.56 | mse 11.56 | mre  0.00 |
scGPT - INFO - | epoch  52 | 2900/3602 batches | lr 0.0000 | ms/batch 91.86 | loss 11.88 | mse 11.88 | mre  0.00 |
scGPT - INFO - | epoch  52 | 3000/3602 batches | lr 0.0000 | ms/batch 91.95 | loss 11.23 | mse 11.23 | mre  0.00 |
scGPT - INFO - | epoch  52 | 3100/3602 batches | lr 0.0000 | ms/batch 91.87 | loss 10.54 | mse 10.54 | mre  0.00 |
scGPT - INFO - | epoch  52 | 3200/3602 batches | lr 0.0000 | ms/batch 91.88 | loss 11.10 | mse 11.10 | mre  0.00 |
scGPT - INFO - | epoch  52 | 3300/3602 batches | lr 0.0000 | ms/batch 92.70 | loss  9.47 | mse  9.47 | mre  0.00 |
scGPT - INFO - | epoch  52 | 3400/3602 batches | lr 0.0000 | ms/batch 91.71 | loss 10.96 | mse 10.96 | mre  0.00 |
scGPT - INFO - | epoch  52 | 3500/3602 batches | lr 0.0000 | ms/batch 91.71 | loss 10.29 | mse 10.29 | mre  0.00 |
scGPT - INFO - | epoch  52 | 3600/3602 batches | lr 0.0000 | ms/batch 91.75 | loss 11.79 | mse 11.79 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  52 | time: 343.08s | valid loss/mse 12.5697 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.5697
best_loss: 12.57034478867694, min_delta 0.0001, val_loss 12.5697274313735
Loss error: 0.0006173573034384816
epoch:  52
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.848 3.562 3.28  ... 3.459 3.643 3.88 ]
 [7.008 3.627 3.338 ... 3.467 3.684 3.914]
 [4.684 3.686 3.818 ... 4.094 3.705 3.549]
 ...
 [4.812 3.867 3.896 ... 4.07  3.78  3.578]
 [4.926 3.91  3.975 ... 4.125 3.86  3.615]
 [6.926 3.658 3.254 ... 3.477 3.61  3.89 ]]
(801, 11) (801, 11)
By feature:  MSE:  0.5714¬±0.0 MAE:  0.493¬±0.0 R2:  0.2416¬±0.0 PCC:  0.8036¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5714¬±0.0 MAE:  0.493¬±0.0 R2:  0.2859¬±0.0 PCC:  0.5218¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.571399986743927¬±0.0 MAE: 0.49300000071525574¬±0.0 R2: 0.2416¬±0.0 PCC: 0.8036¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.571399986743927¬±0.0 MAE: 0.49300000071525574¬±0.0 R2: 0.2859¬±0.0 PCC: 0.5218¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  53
Training model
scGPT - INFO - | epoch  53 | 100/3602 batches | lr 0.0000 | ms/batch 92.94 | loss 10.84 | mse 10.84 | mre  0.00 |
scGPT - INFO - | epoch  53 | 200/3602 batches | lr 0.0000 | ms/batch 92.02 | loss 12.48 | mse 12.48 | mre  0.00 |
scGPT - INFO - | epoch  53 | 300/3602 batches | lr 0.0000 | ms/batch 91.71 | loss  9.77 | mse  9.77 | mre  0.00 |
scGPT - INFO - | epoch  53 | 400/3602 batches | lr 0.0000 | ms/batch 91.78 | loss 10.87 | mse 10.87 | mre  0.00 |
scGPT - INFO - | epoch  53 | 500/3602 batches | lr 0.0000 | ms/batch 91.99 | loss  9.87 | mse  9.87 | mre  0.00 |
scGPT - INFO - | epoch  53 | 600/3602 batches | lr 0.0000 | ms/batch 91.83 | loss 10.83 | mse 10.83 | mre  0.00 |
scGPT - INFO - | epoch  53 | 700/3602 batches | lr 0.0000 | ms/batch 92.62 | loss 11.27 | mse 11.27 | mre  0.00 |
scGPT - INFO - | epoch  53 | 800/3602 batches | lr 0.0000 | ms/batch 91.82 | loss 13.71 | mse 13.71 | mre  0.00 |
scGPT - INFO - | epoch  53 | 900/3602 batches | lr 0.0000 | ms/batch 91.74 | loss 11.05 | mse 11.05 | mre  0.00 |
scGPT - INFO - | epoch  53 | 1000/3602 batches | lr 0.0000 | ms/batch 91.71 | loss 12.80 | mse 12.80 | mre  0.00 |
scGPT - INFO - | epoch  53 | 1100/3602 batches | lr 0.0000 | ms/batch 91.77 | loss 11.12 | mse 11.12 | mre  0.00 |
scGPT - INFO - | epoch  53 | 1200/3602 batches | lr 0.0000 | ms/batch 91.77 | loss 11.34 | mse 11.34 | mre  0.00 |
scGPT - INFO - | epoch  53 | 1300/3602 batches | lr 0.0000 | ms/batch 91.67 | loss 10.69 | mse 10.69 | mre  0.00 |
scGPT - INFO - | epoch  53 | 1400/3602 batches | lr 0.0000 | ms/batch 91.72 | loss 13.42 | mse 13.42 | mre  0.00 |
scGPT - INFO - | epoch  53 | 1500/3602 batches | lr 0.0000 | ms/batch 91.77 | loss 13.79 | mse 13.79 | mre  0.00 |
scGPT - INFO - | epoch  53 | 1600/3602 batches | lr 0.0000 | ms/batch 91.77 | loss  9.71 | mse  9.71 | mre  0.00 |
scGPT - INFO - | epoch  53 | 1700/3602 batches | lr 0.0000 | ms/batch 92.48 | loss 10.60 | mse 10.60 | mre  0.00 |
scGPT - INFO - | epoch  53 | 1800/3602 batches | lr 0.0000 | ms/batch 91.72 | loss 10.91 | mse 10.91 | mre  0.00 |
scGPT - INFO - | epoch  53 | 1900/3602 batches | lr 0.0000 | ms/batch 91.67 | loss 12.77 | mse 12.77 | mre  0.00 |
scGPT - INFO - | epoch  53 | 2000/3602 batches | lr 0.0000 | ms/batch 91.67 | loss 11.35 | mse 11.35 | mre  0.00 |
scGPT - INFO - | epoch  53 | 2100/3602 batches | lr 0.0000 | ms/batch 91.65 | loss 12.99 | mse 12.99 | mre  0.00 |
scGPT - INFO - | epoch  53 | 2200/3602 batches | lr 0.0000 | ms/batch 91.67 | loss 10.88 | mse 10.88 | mre  0.00 |
scGPT - INFO - | epoch  53 | 2300/3602 batches | lr 0.0000 | ms/batch 91.69 | loss 10.99 | mse 10.99 | mre  0.00 |
scGPT - INFO - | epoch  53 | 2400/3602 batches | lr 0.0000 | ms/batch 92.48 | loss 11.83 | mse 11.83 | mre  0.00 |
scGPT - INFO - | epoch  53 | 2500/3602 batches | lr 0.0000 | ms/batch 92.88 | loss 10.08 | mse 10.08 | mre  0.00 |
scGPT - INFO - | epoch  53 | 2600/3602 batches | lr 0.0000 | ms/batch 92.76 | loss 14.62 | mse 14.62 | mre  0.00 |
scGPT - INFO - | epoch  53 | 2700/3602 batches | lr 0.0000 | ms/batch 93.40 | loss 10.64 | mse 10.64 | mre  0.00 |
scGPT - INFO - | epoch  53 | 2800/3602 batches | lr 0.0000 | ms/batch 92.76 | loss 12.07 | mse 12.07 | mre  0.00 |
scGPT - INFO - | epoch  53 | 2900/3602 batches | lr 0.0000 | ms/batch 92.73 | loss 11.89 | mse 11.89 | mre  0.00 |
scGPT - INFO - | epoch  53 | 3000/3602 batches | lr 0.0000 | ms/batch 92.88 | loss 11.41 | mse 11.41 | mre  0.00 |
scGPT - INFO - | epoch  53 | 3100/3602 batches | lr 0.0000 | ms/batch 92.85 | loss 10.48 | mse 10.48 | mre  0.00 |
scGPT - INFO - | epoch  53 | 3200/3602 batches | lr 0.0000 | ms/batch 92.91 | loss 11.22 | mse 11.22 | mre  0.00 |
scGPT - INFO - | epoch  53 | 3300/3602 batches | lr 0.0000 | ms/batch 92.87 | loss  9.62 | mse  9.62 | mre  0.00 |
scGPT - INFO - | epoch  53 | 3400/3602 batches | lr 0.0000 | ms/batch 92.99 | loss 10.96 | mse 10.96 | mre  0.00 |
scGPT - INFO - | epoch  53 | 3500/3602 batches | lr 0.0000 | ms/batch 92.94 | loss 10.31 | mse 10.31 | mre  0.00 |
scGPT - INFO - | epoch  53 | 3600/3602 batches | lr 0.0000 | ms/batch 92.95 | loss 11.74 | mse 11.74 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  53 | time: 345.13s | valid loss/mse 12.5620 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.5620
best_loss: 12.5697274313735, min_delta 0.0001, val_loss 12.562031832303894
Loss error: 0.007695599069606374
epoch:  53
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.848 3.564 3.281 ... 3.459 3.643 3.883]
 [7.004 3.629 3.34  ... 3.469 3.684 3.914]
 [4.688 3.691 3.822 ... 4.094 3.709 3.55 ]
 ...
 [4.816 3.87  3.898 ... 4.07  3.781 3.578]
 [4.93  3.91  3.977 ... 4.125 3.861 3.615]
 [6.918 3.66  3.256 ... 3.479 3.607 3.889]]
(801, 11) (801, 11)
By feature:  MSE:  0.571¬±0.0 MAE:  0.4929¬±0.0 R2:  0.2414¬±0.0 PCC:  0.8041¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.571¬±0.0 MAE:  0.4929¬±0.0 R2:  0.2862¬±0.0 PCC:  0.5222¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5709999799728394¬±0.0 MAE: 0.492900013923645¬±0.0 R2: 0.2414¬±0.0 PCC: 0.8041¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5709999799728394¬±0.0 MAE: 0.492900013923645¬±0.0 R2: 0.2862¬±0.0 PCC: 0.5222¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  54
Training model
scGPT - INFO - | epoch  54 | 100/3602 batches | lr 0.0000 | ms/batch 94.78 | loss 10.31 | mse 10.31 | mre  0.00 |
scGPT - INFO - | epoch  54 | 200/3602 batches | lr 0.0000 | ms/batch 92.85 | loss 12.24 | mse 12.24 | mre  0.00 |
scGPT - INFO - | epoch  54 | 300/3602 batches | lr 0.0000 | ms/batch 92.86 | loss  9.75 | mse  9.75 | mre  0.00 |
scGPT - INFO - | epoch  54 | 400/3602 batches | lr 0.0000 | ms/batch 92.81 | loss 10.74 | mse 10.74 | mre  0.00 |
scGPT - INFO - | epoch  54 | 500/3602 batches | lr 0.0000 | ms/batch 92.79 | loss  9.36 | mse  9.36 | mre  0.00 |
scGPT - INFO - | epoch  54 | 600/3602 batches | lr 0.0000 | ms/batch 92.72 | loss 10.56 | mse 10.56 | mre  0.00 |
scGPT - INFO - | epoch  54 | 700/3602 batches | lr 0.0000 | ms/batch 92.72 | loss 11.34 | mse 11.34 | mre  0.00 |
scGPT - INFO - | epoch  54 | 800/3602 batches | lr 0.0000 | ms/batch 92.71 | loss 13.99 | mse 13.99 | mre  0.00 |
scGPT - INFO - | epoch  54 | 900/3602 batches | lr 0.0000 | ms/batch 92.82 | loss 10.98 | mse 10.98 | mre  0.00 |
scGPT - INFO - | epoch  54 | 1000/3602 batches | lr 0.0000 | ms/batch 91.37 | loss 12.65 | mse 12.65 | mre  0.00 |
scGPT - INFO - | epoch  54 | 1100/3602 batches | lr 0.0000 | ms/batch 91.22 | loss 10.77 | mse 10.77 | mre  0.00 |
scGPT - INFO - | epoch  54 | 1200/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 11.10 | mse 11.10 | mre  0.00 |
scGPT - INFO - | epoch  54 | 1300/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 10.52 | mse 10.52 | mre  0.00 |
scGPT - INFO - | epoch  54 | 1400/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 13.30 | mse 13.30 | mre  0.00 |
scGPT - INFO - | epoch  54 | 1500/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 13.48 | mse 13.48 | mre  0.00 |
scGPT - INFO - | epoch  54 | 1600/3602 batches | lr 0.0000 | ms/batch 90.69 | loss  9.64 | mse  9.64 | mre  0.00 |
scGPT - INFO - | epoch  54 | 1700/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 10.49 | mse 10.49 | mre  0.00 |
scGPT - INFO - | epoch  54 | 1800/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 10.66 | mse 10.66 | mre  0.00 |
scGPT - INFO - | epoch  54 | 1900/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 13.43 | mse 13.43 | mre  0.00 |
scGPT - INFO - | epoch  54 | 2000/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.79 | mse 10.79 | mre  0.00 |
scGPT - INFO - | epoch  54 | 2100/3602 batches | lr 0.0000 | ms/batch 91.26 | loss 12.66 | mse 12.66 | mre  0.00 |
scGPT - INFO - | epoch  54 | 2200/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 10.75 | mse 10.75 | mre  0.00 |
scGPT - INFO - | epoch  54 | 2300/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 10.84 | mse 10.84 | mre  0.00 |
scGPT - INFO - | epoch  54 | 2400/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 11.84 | mse 11.84 | mre  0.00 |
scGPT - INFO - | epoch  54 | 2500/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 10.20 | mse 10.20 | mre  0.00 |
scGPT - INFO - | epoch  54 | 2600/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 14.33 | mse 14.33 | mre  0.00 |
scGPT - INFO - | epoch  54 | 2700/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 10.43 | mse 10.43 | mre  0.00 |
scGPT - INFO - | epoch  54 | 2800/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 12.25 | mse 12.25 | mre  0.00 |
scGPT - INFO - | epoch  54 | 2900/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 11.81 | mse 11.81 | mre  0.00 |
scGPT - INFO - | epoch  54 | 3000/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 11.30 | mse 11.30 | mre  0.00 |
scGPT - INFO - | epoch  54 | 3100/3602 batches | lr 0.0000 | ms/batch 91.23 | loss 11.25 | mse 11.25 | mre  0.00 |
scGPT - INFO - | epoch  54 | 3200/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 11.28 | mse 11.28 | mre  0.00 |
scGPT - INFO - | epoch  54 | 3300/3602 batches | lr 0.0000 | ms/batch 90.65 | loss  9.52 | mse  9.52 | mre  0.00 |
scGPT - INFO - | epoch  54 | 3400/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 10.50 | mse 10.50 | mre  0.00 |
scGPT - INFO - | epoch  54 | 3500/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 10.04 | mse 10.04 | mre  0.00 |
scGPT - INFO - | epoch  54 | 3600/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 11.80 | mse 11.80 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  54 | time: 341.49s | valid loss/mse 12.5604 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.5604
best_loss: 12.562031832303894, min_delta 0.0001, val_loss 12.560383144612615
Loss error: 0.0016486876912793491
epoch:  54
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.848 3.568 3.283 ... 3.459 3.646 3.885]
 [7.008 3.633 3.342 ... 3.467 3.688 3.92 ]
 [4.684 3.693 3.818 ... 4.094 3.709 3.55 ]
 ...
 [4.816 3.873 3.896 ... 4.07  3.783 3.58 ]
 [4.926 3.914 3.975 ... 4.125 3.861 3.617]
 [6.92  3.666 3.258 ... 3.479 3.611 3.895]]
(801, 11) (801, 11)
By feature:  MSE:  0.571¬±0.0 MAE:  0.4929¬±0.0 R2:  0.2412¬±0.0 PCC:  0.8039¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.571¬±0.0 MAE:  0.4929¬±0.0 R2:  0.2866¬±0.0 PCC:  0.5225¬±0.0 Cosine Similarity:  0.9854¬±0.0
scGPT - INFO - By feature: MSE: 0.5709999799728394¬±0.0 MAE: 0.492900013923645¬±0.0 R2: 0.2412¬±0.0 PCC: 0.8039¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5709999799728394¬±0.0 MAE: 0.492900013923645¬±0.0 R2: 0.2866¬±0.0 PCC: 0.5225¬±0.0 Cosine Similarity: 0.9854¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  55
Training model
scGPT - INFO - | epoch  55 | 100/3602 batches | lr 0.0000 | ms/batch 91.96 | loss 10.55 | mse 10.55 | mre  0.00 |
scGPT - INFO - | epoch  55 | 200/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 12.05 | mse 12.05 | mre  0.00 |
scGPT - INFO - | epoch  55 | 300/3602 batches | lr 0.0000 | ms/batch 90.70 | loss  9.51 | mse  9.51 | mre  0.00 |
scGPT - INFO - | epoch  55 | 400/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 10.84 | mse 10.84 | mre  0.00 |
scGPT - INFO - | epoch  55 | 500/3602 batches | lr 0.0000 | ms/batch 91.30 | loss  9.65 | mse  9.65 | mre  0.00 |
scGPT - INFO - | epoch  55 | 600/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 10.92 | mse 10.92 | mre  0.00 |
scGPT - INFO - | epoch  55 | 700/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 11.52 | mse 11.52 | mre  0.00 |
scGPT - INFO - | epoch  55 | 800/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 13.78 | mse 13.78 | mre  0.00 |
scGPT - INFO - | epoch  55 | 900/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 11.10 | mse 11.10 | mre  0.00 |
scGPT - INFO - | epoch  55 | 1000/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 12.63 | mse 12.63 | mre  0.00 |
scGPT - INFO - | epoch  55 | 1100/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 10.87 | mse 10.87 | mre  0.00 |
scGPT - INFO - | epoch  55 | 1200/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 11.13 | mse 11.13 | mre  0.00 |
scGPT - INFO - | epoch  55 | 1300/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 10.93 | mse 10.93 | mre  0.00 |
scGPT - INFO - | epoch  55 | 1400/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 13.42 | mse 13.42 | mre  0.00 |
scGPT - INFO - | epoch  55 | 1500/3602 batches | lr 0.0000 | ms/batch 91.28 | loss 13.33 | mse 13.33 | mre  0.00 |
scGPT - INFO - | epoch  55 | 1600/3602 batches | lr 0.0000 | ms/batch 90.69 | loss  9.54 | mse  9.54 | mre  0.00 |
scGPT - INFO - | epoch  55 | 1700/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 10.57 | mse 10.57 | mre  0.00 |
scGPT - INFO - | epoch  55 | 1800/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 10.52 | mse 10.52 | mre  0.00 |
scGPT - INFO - | epoch  55 | 1900/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 13.03 | mse 13.03 | mre  0.00 |
scGPT - INFO - | epoch  55 | 2000/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 11.05 | mse 11.05 | mre  0.00 |
scGPT - INFO - | epoch  55 | 2100/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 13.03 | mse 13.03 | mre  0.00 |
scGPT - INFO - | epoch  55 | 2200/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 11.06 | mse 11.06 | mre  0.00 |
scGPT - INFO - | epoch  55 | 2300/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 10.84 | mse 10.84 | mre  0.00 |
scGPT - INFO - | epoch  55 | 2400/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 11.70 | mse 11.70 | mre  0.00 |
scGPT - INFO - | epoch  55 | 2500/3602 batches | lr 0.0000 | ms/batch 91.31 | loss 10.59 | mse 10.59 | mre  0.00 |
scGPT - INFO - | epoch  55 | 2600/3602 batches | lr 0.0000 | ms/batch 92.25 | loss 14.58 | mse 14.58 | mre  0.00 |
scGPT - INFO - | epoch  55 | 2700/3602 batches | lr 0.0000 | ms/batch 91.43 | loss 10.62 | mse 10.62 | mre  0.00 |
scGPT - INFO - | epoch  55 | 2800/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 11.78 | mse 11.78 | mre  0.00 |
scGPT - INFO - | epoch  55 | 2900/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 11.40 | mse 11.40 | mre  0.00 |
scGPT - INFO - | epoch  55 | 3000/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 11.41 | mse 11.41 | mre  0.00 |
scGPT - INFO - | epoch  55 | 3100/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 10.47 | mse 10.47 | mre  0.00 |
scGPT - INFO - | epoch  55 | 3200/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 11.14 | mse 11.14 | mre  0.00 |
scGPT - INFO - | epoch  55 | 3300/3602 batches | lr 0.0000 | ms/batch 90.64 | loss  9.44 | mse  9.44 | mre  0.00 |
scGPT - INFO - | epoch  55 | 3400/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 10.76 | mse 10.76 | mre  0.00 |
scGPT - INFO - | epoch  55 | 3500/3602 batches | lr 0.0000 | ms/batch 91.93 | loss 10.50 | mse 10.50 | mre  0.00 |
scGPT - INFO - | epoch  55 | 3600/3602 batches | lr 0.0000 | ms/batch 92.86 | loss 11.57 | mse 11.57 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  55 | time: 340.04s | valid loss/mse 12.5591 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.5591
best_loss: 12.560383144612615, min_delta 0.0001, val_loss 12.559127085068997
Loss error: 0.001256059543617738
epoch:  55
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.84  3.566 3.285 ... 3.459 3.646 3.883]
 [7.    3.633 3.342 ... 3.469 3.688 3.914]
 [4.68  3.69  3.82  ... 4.094 3.709 3.55 ]
 ...
 [4.812 3.871 3.9   ... 4.07  3.783 3.578]
 [4.926 3.914 3.979 ... 4.125 3.863 3.617]
 [6.918 3.662 3.26  ... 3.475 3.611 3.89 ]]
(801, 11) (801, 11)
By feature:  MSE:  0.5709¬±0.0 MAE:  0.4926¬±0.0 R2:  0.2425¬±0.0 PCC:  0.8039¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5709¬±0.0 MAE:  0.4926¬±0.0 R2:  0.2865¬±0.0 PCC:  0.5223¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.570900022983551¬±0.0 MAE: 0.4925999939441681¬±0.0 R2: 0.2425¬±0.0 PCC: 0.8039¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.570900022983551¬±0.0 MAE: 0.4925999939441681¬±0.0 R2: 0.2865¬±0.0 PCC: 0.5223¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  56
Training model
scGPT - INFO - | epoch  56 | 100/3602 batches | lr 0.0000 | ms/batch 92.83 | loss 10.29 | mse 10.29 | mre  0.00 |
scGPT - INFO - | epoch  56 | 200/3602 batches | lr 0.0000 | ms/batch 91.58 | loss 12.36 | mse 12.36 | mre  0.00 |
scGPT - INFO - | epoch  56 | 300/3602 batches | lr 0.0000 | ms/batch 91.94 | loss  9.54 | mse  9.54 | mre  0.00 |
scGPT - INFO - | epoch  56 | 400/3602 batches | lr 0.0000 | ms/batch 91.91 | loss 10.97 | mse 10.97 | mre  0.00 |
scGPT - INFO - | epoch  56 | 500/3602 batches | lr 0.0000 | ms/batch 91.78 | loss  9.35 | mse  9.35 | mre  0.00 |
scGPT - INFO - | epoch  56 | 600/3602 batches | lr 0.0000 | ms/batch 92.00 | loss 11.06 | mse 11.06 | mre  0.00 |
scGPT - INFO - | epoch  56 | 700/3602 batches | lr 0.0000 | ms/batch 91.92 | loss 11.49 | mse 11.49 | mre  0.00 |
scGPT - INFO - | epoch  56 | 800/3602 batches | lr 0.0000 | ms/batch 91.75 | loss 14.37 | mse 14.37 | mre  0.00 |
scGPT - INFO - | epoch  56 | 900/3602 batches | lr 0.0000 | ms/batch 92.60 | loss 11.25 | mse 11.25 | mre  0.00 |
scGPT - INFO - | epoch  56 | 1000/3602 batches | lr 0.0000 | ms/batch 91.26 | loss 12.46 | mse 12.46 | mre  0.00 |
scGPT - INFO - | epoch  56 | 1100/3602 batches | lr 0.0000 | ms/batch 91.39 | loss 10.80 | mse 10.80 | mre  0.00 |
scGPT - INFO - | epoch  56 | 1200/3602 batches | lr 0.0000 | ms/batch 91.62 | loss 11.01 | mse 11.01 | mre  0.00 |
scGPT - INFO - | epoch  56 | 1300/3602 batches | lr 0.0000 | ms/batch 91.52 | loss 10.41 | mse 10.41 | mre  0.00 |
scGPT - INFO - | epoch  56 | 1400/3602 batches | lr 0.0000 | ms/batch 91.79 | loss 12.99 | mse 12.99 | mre  0.00 |
scGPT - INFO - | epoch  56 | 1500/3602 batches | lr 0.0000 | ms/batch 94.39 | loss 13.87 | mse 13.87 | mre  0.00 |
scGPT - INFO - | epoch  56 | 1600/3602 batches | lr 0.0000 | ms/batch 96.34 | loss  9.62 | mse  9.62 | mre  0.00 |
scGPT - INFO - | epoch  56 | 1700/3602 batches | lr 0.0000 | ms/batch 96.35 | loss 10.46 | mse 10.46 | mre  0.00 |
scGPT - INFO - | epoch  56 | 1800/3602 batches | lr 0.0000 | ms/batch 96.38 | loss 11.01 | mse 11.01 | mre  0.00 |
scGPT - INFO - | epoch  56 | 1900/3602 batches | lr 0.0000 | ms/batch 96.83 | loss 13.25 | mse 13.25 | mre  0.00 |
scGPT - INFO - | epoch  56 | 2000/3602 batches | lr 0.0000 | ms/batch 96.26 | loss 11.20 | mse 11.20 | mre  0.00 |
scGPT - INFO - | epoch  56 | 2100/3602 batches | lr 0.0000 | ms/batch 96.31 | loss 12.88 | mse 12.88 | mre  0.00 |
scGPT - INFO - | epoch  56 | 2200/3602 batches | lr 0.0000 | ms/batch 96.26 | loss 11.07 | mse 11.07 | mre  0.00 |
scGPT - INFO - | epoch  56 | 2300/3602 batches | lr 0.0000 | ms/batch 96.23 | loss 11.18 | mse 11.18 | mre  0.00 |
scGPT - INFO - | epoch  56 | 2400/3602 batches | lr 0.0000 | ms/batch 96.36 | loss 11.45 | mse 11.45 | mre  0.00 |
scGPT - INFO - | epoch  56 | 2500/3602 batches | lr 0.0000 | ms/batch 94.87 | loss  9.77 | mse  9.77 | mre  0.00 |
scGPT - INFO - | epoch  56 | 2600/3602 batches | lr 0.0000 | ms/batch 91.84 | loss 14.48 | mse 14.48 | mre  0.00 |
scGPT - INFO - | epoch  56 | 2700/3602 batches | lr 0.0000 | ms/batch 95.47 | loss 10.17 | mse 10.17 | mre  0.00 |
scGPT - INFO - | epoch  56 | 2800/3602 batches | lr 0.0000 | ms/batch 96.19 | loss 11.87 | mse 11.87 | mre  0.00 |
scGPT - INFO - | epoch  56 | 2900/3602 batches | lr 0.0000 | ms/batch 96.80 | loss 12.00 | mse 12.00 | mre  0.00 |
scGPT - INFO - | epoch  56 | 3000/3602 batches | lr 0.0000 | ms/batch 96.27 | loss 11.15 | mse 11.15 | mre  0.00 |
scGPT - INFO - | epoch  56 | 3100/3602 batches | lr 0.0000 | ms/batch 96.32 | loss 10.66 | mse 10.66 | mre  0.00 |
scGPT - INFO - | epoch  56 | 3200/3602 batches | lr 0.0000 | ms/batch 96.17 | loss 10.49 | mse 10.49 | mre  0.00 |
scGPT - INFO - | epoch  56 | 3300/3602 batches | lr 0.0000 | ms/batch 96.38 | loss  9.81 | mse  9.81 | mre  0.00 |
scGPT - INFO - | epoch  56 | 3400/3602 batches | lr 0.0000 | ms/batch 96.34 | loss 10.75 | mse 10.75 | mre  0.00 |
scGPT - INFO - | epoch  56 | 3500/3602 batches | lr 0.0000 | ms/batch 96.27 | loss 10.31 | mse 10.31 | mre  0.00 |
scGPT - INFO - | epoch  56 | 3600/3602 batches | lr 0.0000 | ms/batch 96.25 | loss 11.83 | mse 11.83 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  56 | time: 353.79s | valid loss/mse 12.5700 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.559127085068997, min_delta 0.0001, val_loss 12.569974095261797
Loss error: -0.010847010192799544
scGPT - INFO - INFO: Early stopping counter 1 of 10
epoch:  56
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.844 3.564 3.283 ... 3.457 3.645 3.883]
 [7.004 3.63  3.338 ... 3.467 3.686 3.914]
 [4.688 3.691 3.818 ... 4.094 3.707 3.549]
 ...
 [4.816 3.871 3.896 ... 4.07  3.781 3.578]
 [4.93  3.916 3.975 ... 4.125 3.861 3.615]
 [6.926 3.662 3.256 ... 3.475 3.611 3.89 ]]
(801, 11) (801, 11)
By feature:  MSE:  0.5714¬±0.0 MAE:  0.493¬±0.0 R2:  0.2409¬±0.0 PCC:  0.8038¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5714¬±0.0 MAE:  0.493¬±0.0 R2:  0.2861¬±0.0 PCC:  0.5222¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.571399986743927¬±0.0 MAE: 0.49300000071525574¬±0.0 R2: 0.2409¬±0.0 PCC: 0.8038¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.571399986743927¬±0.0 MAE: 0.49300000071525574¬±0.0 R2: 0.2861¬±0.0 PCC: 0.5222¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  57
Training model
scGPT - INFO - | epoch  57 | 100/3602 batches | lr 0.0000 | ms/batch 93.01 | loss 10.56 | mse 10.56 | mre  0.00 |
scGPT - INFO - | epoch  57 | 200/3602 batches | lr 0.0000 | ms/batch 91.69 | loss 12.24 | mse 12.24 | mre  0.00 |
scGPT - INFO - | epoch  57 | 300/3602 batches | lr 0.0000 | ms/batch 92.41 | loss 10.13 | mse 10.13 | mre  0.00 |
scGPT - INFO - | epoch  57 | 400/3602 batches | lr 0.0000 | ms/batch 91.95 | loss 10.66 | mse 10.66 | mre  0.00 |
scGPT - INFO - | epoch  57 | 500/3602 batches | lr 0.0000 | ms/batch 92.32 | loss  9.37 | mse  9.37 | mre  0.00 |
scGPT - INFO - | epoch  57 | 600/3602 batches | lr 0.0000 | ms/batch 92.72 | loss 10.88 | mse 10.88 | mre  0.00 |
scGPT - INFO - | epoch  57 | 700/3602 batches | lr 0.0000 | ms/batch 92.76 | loss 11.43 | mse 11.43 | mre  0.00 |
scGPT - INFO - | epoch  57 | 800/3602 batches | lr 0.0000 | ms/batch 93.07 | loss 14.15 | mse 14.15 | mre  0.00 |
scGPT - INFO - | epoch  57 | 900/3602 batches | lr 0.0000 | ms/batch 92.88 | loss 11.38 | mse 11.38 | mre  0.00 |
scGPT - INFO - | epoch  57 | 1000/3602 batches | lr 0.0000 | ms/batch 92.75 | loss 12.79 | mse 12.79 | mre  0.00 |
scGPT - INFO - | epoch  57 | 1100/3602 batches | lr 0.0000 | ms/batch 92.81 | loss 10.77 | mse 10.77 | mre  0.00 |
scGPT - INFO - | epoch  57 | 1200/3602 batches | lr 0.0000 | ms/batch 92.68 | loss 11.11 | mse 11.11 | mre  0.00 |
scGPT - INFO - | epoch  57 | 1300/3602 batches | lr 0.0000 | ms/batch 93.40 | loss 10.72 | mse 10.72 | mre  0.00 |
scGPT - INFO - | epoch  57 | 1400/3602 batches | lr 0.0000 | ms/batch 92.74 | loss 13.53 | mse 13.53 | mre  0.00 |
scGPT - INFO - | epoch  57 | 1500/3602 batches | lr 0.0000 | ms/batch 92.70 | loss 13.73 | mse 13.73 | mre  0.00 |
scGPT - INFO - | epoch  57 | 1600/3602 batches | lr 0.0000 | ms/batch 92.71 | loss  9.70 | mse  9.70 | mre  0.00 |
scGPT - INFO - | epoch  57 | 1700/3602 batches | lr 0.0000 | ms/batch 92.71 | loss 10.73 | mse 10.73 | mre  0.00 |
scGPT - INFO - | epoch  57 | 1800/3602 batches | lr 0.0000 | ms/batch 92.77 | loss 10.58 | mse 10.58 | mre  0.00 |
scGPT - INFO - | epoch  57 | 1900/3602 batches | lr 0.0000 | ms/batch 92.66 | loss 13.16 | mse 13.16 | mre  0.00 |
scGPT - INFO - | epoch  57 | 2000/3602 batches | lr 0.0000 | ms/batch 92.12 | loss 11.15 | mse 11.15 | mre  0.00 |
scGPT - INFO - | epoch  57 | 2100/3602 batches | lr 0.0000 | ms/batch 92.80 | loss 12.55 | mse 12.55 | mre  0.00 |
scGPT - INFO - | epoch  57 | 2200/3602 batches | lr 0.0000 | ms/batch 91.81 | loss 11.38 | mse 11.38 | mre  0.00 |
scGPT - INFO - | epoch  57 | 2300/3602 batches | lr 0.0000 | ms/batch 91.22 | loss 11.10 | mse 11.10 | mre  0.00 |
scGPT - INFO - | epoch  57 | 2400/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 11.71 | mse 11.71 | mre  0.00 |
scGPT - INFO - | epoch  57 | 2500/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 10.33 | mse 10.33 | mre  0.00 |
scGPT - INFO - | epoch  57 | 2600/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 14.56 | mse 14.56 | mre  0.00 |
scGPT - INFO - | epoch  57 | 2700/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 10.36 | mse 10.36 | mre  0.00 |
scGPT - INFO - | epoch  57 | 2800/3602 batches | lr 0.0000 | ms/batch 91.10 | loss 11.75 | mse 11.75 | mre  0.00 |
scGPT - INFO - | epoch  57 | 2900/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 11.58 | mse 11.58 | mre  0.00 |
scGPT - INFO - | epoch  57 | 3000/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 11.48 | mse 11.48 | mre  0.00 |
scGPT - INFO - | epoch  57 | 3100/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 10.60 | mse 10.60 | mre  0.00 |
scGPT - INFO - | epoch  57 | 3200/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 10.69 | mse 10.69 | mre  0.00 |
scGPT - INFO - | epoch  57 | 3300/3602 batches | lr 0.0000 | ms/batch 91.36 | loss 10.08 | mse 10.08 | mre  0.00 |
scGPT - INFO - | epoch  57 | 3400/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 11.00 | mse 11.00 | mre  0.00 |
scGPT - INFO - | epoch  57 | 3500/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 10.27 | mse 10.27 | mre  0.00 |
scGPT - INFO - | epoch  57 | 3600/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 11.92 | mse 11.92 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  57 | time: 343.39s | valid loss/mse 12.5685 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.559127085068997, min_delta 0.0001, val_loss 12.568497842170773
Loss error: -0.009370757101775595
scGPT - INFO - INFO: Early stopping counter 2 of 10
epoch:  57
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.844 3.566 3.281 ... 3.457 3.645 3.88 ]
 [7.004 3.63  3.338 ... 3.465 3.686 3.914]
 [4.684 3.691 3.818 ... 4.094 3.707 3.547]
 ...
 [4.812 3.871 3.896 ... 4.07  3.781 3.576]
 [4.926 3.914 3.975 ... 4.125 3.861 3.615]
 [6.918 3.664 3.256 ... 3.475 3.61  3.889]]
(801, 11) (801, 11)
By feature:  MSE:  0.5713¬±0.0 MAE:  0.4932¬±0.0 R2:  0.2413¬±0.0 PCC:  0.8036¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5713¬±0.0 MAE:  0.4932¬±0.0 R2:  0.286¬±0.0 PCC:  0.5221¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5713000297546387¬±0.0 MAE: 0.49320000410079956¬±0.0 R2: 0.2413¬±0.0 PCC: 0.8036¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5713000297546387¬±0.0 MAE: 0.49320000410079956¬±0.0 R2: 0.286¬±0.0 PCC: 0.5221¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  58
Training model
scGPT - INFO - | epoch  58 | 100/3602 batches | lr 0.0000 | ms/batch 92.02 | loss 10.41 | mse 10.41 | mre  0.00 |
scGPT - INFO - | epoch  58 | 200/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 12.19 | mse 12.19 | mre  0.00 |
scGPT - INFO - | epoch  58 | 300/3602 batches | lr 0.0000 | ms/batch 90.72 | loss  9.96 | mse  9.96 | mre  0.00 |
scGPT - INFO - | epoch  58 | 400/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 10.88 | mse 10.88 | mre  0.00 |
scGPT - INFO - | epoch  58 | 500/3602 batches | lr 0.0000 | ms/batch 90.74 | loss  9.37 | mse  9.37 | mre  0.00 |
scGPT - INFO - | epoch  58 | 600/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 10.76 | mse 10.76 | mre  0.00 |
scGPT - INFO - | epoch  58 | 700/3602 batches | lr 0.0000 | ms/batch 91.29 | loss 11.63 | mse 11.63 | mre  0.00 |
scGPT - INFO - | epoch  58 | 800/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 14.15 | mse 14.15 | mre  0.00 |
scGPT - INFO - | epoch  58 | 900/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 11.13 | mse 11.13 | mre  0.00 |
scGPT - INFO - | epoch  58 | 1000/3602 batches | lr 0.0000 | ms/batch 91.05 | loss 12.77 | mse 12.77 | mre  0.00 |
scGPT - INFO - | epoch  58 | 1100/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 10.74 | mse 10.74 | mre  0.00 |
scGPT - INFO - | epoch  58 | 1200/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 10.89 | mse 10.89 | mre  0.00 |
scGPT - INFO - | epoch  58 | 1300/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 10.66 | mse 10.66 | mre  0.00 |
scGPT - INFO - | epoch  58 | 1400/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 13.41 | mse 13.41 | mre  0.00 |
scGPT - INFO - | epoch  58 | 1500/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 13.60 | mse 13.60 | mre  0.00 |
scGPT - INFO - | epoch  58 | 1600/3602 batches | lr 0.0000 | ms/batch 90.64 | loss  9.71 | mse  9.71 | mre  0.00 |
scGPT - INFO - | epoch  58 | 1700/3602 batches | lr 0.0000 | ms/batch 91.17 | loss 10.22 | mse 10.22 | mre  0.00 |
scGPT - INFO - | epoch  58 | 1800/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 10.37 | mse 10.37 | mre  0.00 |
scGPT - INFO - | epoch  58 | 1900/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 13.29 | mse 13.29 | mre  0.00 |
scGPT - INFO - | epoch  58 | 2000/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 10.93 | mse 10.93 | mre  0.00 |
scGPT - INFO - | epoch  58 | 2100/3602 batches | lr 0.0000 | ms/batch 90.60 | loss 12.95 | mse 12.95 | mre  0.00 |
scGPT - INFO - | epoch  58 | 2200/3602 batches | lr 0.0000 | ms/batch 90.60 | loss 10.91 | mse 10.91 | mre  0.00 |
scGPT - INFO - | epoch  58 | 2300/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 10.71 | mse 10.71 | mre  0.00 |
scGPT - INFO - | epoch  58 | 2400/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 11.56 | mse 11.56 | mre  0.00 |
scGPT - INFO - | epoch  58 | 2500/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 10.39 | mse 10.39 | mre  0.00 |
scGPT - INFO - | epoch  58 | 2600/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 14.51 | mse 14.51 | mre  0.00 |
scGPT - INFO - | epoch  58 | 2700/3602 batches | lr 0.0000 | ms/batch 91.38 | loss 10.57 | mse 10.57 | mre  0.00 |
scGPT - INFO - | epoch  58 | 2800/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 11.58 | mse 11.58 | mre  0.00 |
scGPT - INFO - | epoch  58 | 2900/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 11.60 | mse 11.60 | mre  0.00 |
scGPT - INFO - | epoch  58 | 3000/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 11.33 | mse 11.33 | mre  0.00 |
scGPT - INFO - | epoch  58 | 3100/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 10.26 | mse 10.26 | mre  0.00 |
scGPT - INFO - | epoch  58 | 3200/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 11.03 | mse 11.03 | mre  0.00 |
scGPT - INFO - | epoch  58 | 3300/3602 batches | lr 0.0000 | ms/batch 90.95 | loss  9.33 | mse  9.33 | mre  0.00 |
scGPT - INFO - | epoch  58 | 3400/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 10.78 | mse 10.78 | mre  0.00 |
scGPT - INFO - | epoch  58 | 3500/3602 batches | lr 0.0000 | ms/batch 90.99 | loss 10.45 | mse 10.45 | mre  0.00 |
scGPT - INFO - | epoch  58 | 3600/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 11.50 | mse 11.50 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  58 | time: 339.47s | valid loss/mse 12.5625 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.559127085068997, min_delta 0.0001, val_loss 12.56246281920301
Loss error: -0.003335734134012114
scGPT - INFO - INFO: Early stopping counter 3 of 10
epoch:  58
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.844 3.57  3.285 ... 3.457 3.646 3.883]
 [7.004 3.633 3.342 ... 3.467 3.688 3.914]
 [4.68  3.693 3.822 ... 4.094 3.71  3.549]
 ...
 [4.81  3.873 3.898 ... 4.066 3.783 3.578]
 [4.92  3.916 3.977 ... 4.125 3.863 3.615]
 [6.914 3.668 3.26  ... 3.477 3.611 3.89 ]]
(801, 11) (801, 11)
By feature:  MSE:  0.5711¬±0.0 MAE:  0.4933¬±0.0 R2:  0.2417¬±0.0 PCC:  0.8036¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5711¬±0.0 MAE:  0.4933¬±0.0 R2:  0.2862¬±0.0 PCC:  0.5222¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5710999965667725¬±0.0 MAE: 0.4932999908924103¬±0.0 R2: 0.2417¬±0.0 PCC: 0.8036¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5710999965667725¬±0.0 MAE: 0.4932999908924103¬±0.0 R2: 0.2862¬±0.0 PCC: 0.5222¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  59
Training model
scGPT - INFO - | epoch  59 | 100/3602 batches | lr 0.0000 | ms/batch 92.72 | loss 10.52 | mse 10.52 | mre  0.00 |
scGPT - INFO - | epoch  59 | 200/3602 batches | lr 0.0000 | ms/batch 90.91 | loss 11.80 | mse 11.80 | mre  0.00 |
scGPT - INFO - | epoch  59 | 300/3602 batches | lr 0.0000 | ms/batch 90.82 | loss  9.59 | mse  9.59 | mre  0.00 |
scGPT - INFO - | epoch  59 | 400/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.67 | mse 10.67 | mre  0.00 |
scGPT - INFO - | epoch  59 | 500/3602 batches | lr 0.0000 | ms/batch 90.76 | loss  9.44 | mse  9.44 | mre  0.00 |
scGPT - INFO - | epoch  59 | 600/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 10.90 | mse 10.90 | mre  0.00 |
scGPT - INFO - | epoch  59 | 700/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 11.01 | mse 11.01 | mre  0.00 |
scGPT - INFO - | epoch  59 | 800/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 14.14 | mse 14.14 | mre  0.00 |
scGPT - INFO - | epoch  59 | 900/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 11.10 | mse 11.10 | mre  0.00 |
scGPT - INFO - | epoch  59 | 1000/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 12.36 | mse 12.36 | mre  0.00 |
scGPT - INFO - | epoch  59 | 1100/3602 batches | lr 0.0000 | ms/batch 91.39 | loss 10.55 | mse 10.55 | mre  0.00 |
scGPT - INFO - | epoch  59 | 1200/3602 batches | lr 0.0000 | ms/batch 90.93 | loss 11.17 | mse 11.17 | mre  0.00 |
scGPT - INFO - | epoch  59 | 1300/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 10.55 | mse 10.55 | mre  0.00 |
scGPT - INFO - | epoch  59 | 1400/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 13.20 | mse 13.20 | mre  0.00 |
scGPT - INFO - | epoch  59 | 1500/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 13.74 | mse 13.74 | mre  0.00 |
scGPT - INFO - | epoch  59 | 1600/3602 batches | lr 0.0000 | ms/batch 90.81 | loss  9.86 | mse  9.86 | mre  0.00 |
scGPT - INFO - | epoch  59 | 1700/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 10.79 | mse 10.79 | mre  0.00 |
scGPT - INFO - | epoch  59 | 1800/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 10.65 | mse 10.65 | mre  0.00 |
scGPT - INFO - | epoch  59 | 1900/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 13.06 | mse 13.06 | mre  0.00 |
scGPT - INFO - | epoch  59 | 2000/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 11.05 | mse 11.05 | mre  0.00 |
scGPT - INFO - | epoch  59 | 2100/3602 batches | lr 0.0000 | ms/batch 91.43 | loss 12.68 | mse 12.68 | mre  0.00 |
scGPT - INFO - | epoch  59 | 2200/3602 batches | lr 0.0000 | ms/batch 90.95 | loss 11.19 | mse 11.19 | mre  0.00 |
scGPT - INFO - | epoch  59 | 2300/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 11.03 | mse 11.03 | mre  0.00 |
scGPT - INFO - | epoch  59 | 2400/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 12.00 | mse 12.00 | mre  0.00 |
scGPT - INFO - | epoch  59 | 2500/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 10.28 | mse 10.28 | mre  0.00 |
scGPT - INFO - | epoch  59 | 2600/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 14.10 | mse 14.10 | mre  0.00 |
scGPT - INFO - | epoch  59 | 2700/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 10.20 | mse 10.20 | mre  0.00 |
scGPT - INFO - | epoch  59 | 2800/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 11.81 | mse 11.81 | mre  0.00 |
scGPT - INFO - | epoch  59 | 2900/3602 batches | lr 0.0000 | ms/batch 90.93 | loss 11.45 | mse 11.45 | mre  0.00 |
scGPT - INFO - | epoch  59 | 3000/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 11.84 | mse 11.84 | mre  0.00 |
scGPT - INFO - | epoch  59 | 3100/3602 batches | lr 0.0000 | ms/batch 91.37 | loss 10.68 | mse 10.68 | mre  0.00 |
scGPT - INFO - | epoch  59 | 3200/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 11.13 | mse 11.13 | mre  0.00 |
scGPT - INFO - | epoch  59 | 3300/3602 batches | lr 0.0000 | ms/batch 90.86 | loss  9.28 | mse  9.28 | mre  0.00 |
scGPT - INFO - | epoch  59 | 3400/3602 batches | lr 0.0000 | ms/batch 92.74 | loss 10.45 | mse 10.45 | mre  0.00 |
scGPT - INFO - | epoch  59 | 3500/3602 batches | lr 0.0000 | ms/batch 92.65 | loss 10.80 | mse 10.80 | mre  0.00 |
scGPT - INFO - | epoch  59 | 3600/3602 batches | lr 0.0000 | ms/batch 92.74 | loss 11.57 | mse 11.57 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  59 | time: 340.44s | valid loss/mse 12.5644 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.559127085068997, min_delta 0.0001, val_loss 12.56439936741461
Loss error: -0.005272282345613277
scGPT - INFO - INFO: Early stopping counter 4 of 10
epoch:  59
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.844 3.564 3.281 ... 3.457 3.645 3.88 ]
 [7.004 3.627 3.336 ... 3.463 3.684 3.912]
 [4.684 3.693 3.818 ... 4.094 3.71  3.549]
 ...
 [4.812 3.87  3.895 ... 4.07  3.783 3.576]
 [4.92  3.91  3.973 ... 4.125 3.861 3.613]
 [6.91  3.666 3.254 ... 3.477 3.607 3.889]]
(801, 11) (801, 11)
By feature:  MSE:  0.5712¬±0.0 MAE:  0.4931¬±0.0 R2:  0.2419¬±0.0 PCC:  0.8038¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5712¬±0.0 MAE:  0.4931¬±0.0 R2:  0.2862¬±0.0 PCC:  0.5223¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5712000131607056¬±0.0 MAE: 0.49309998750686646¬±0.0 R2: 0.2419¬±0.0 PCC: 0.8038¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5712000131607056¬±0.0 MAE: 0.49309998750686646¬±0.0 R2: 0.2862¬±0.0 PCC: 0.5223¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  60
Training model
scGPT - INFO - | epoch  60 | 100/3602 batches | lr 0.0000 | ms/batch 94.01 | loss 10.81 | mse 10.81 | mre  0.00 |
scGPT - INFO - | epoch  60 | 200/3602 batches | lr 0.0000 | ms/batch 92.74 | loss 12.31 | mse 12.31 | mre  0.00 |
scGPT - INFO - | epoch  60 | 300/3602 batches | lr 0.0000 | ms/batch 92.71 | loss  9.80 | mse  9.80 | mre  0.00 |
scGPT - INFO - | epoch  60 | 400/3602 batches | lr 0.0000 | ms/batch 92.74 | loss 10.85 | mse 10.85 | mre  0.00 |
scGPT - INFO - | epoch  60 | 500/3602 batches | lr 0.0000 | ms/batch 93.67 | loss  9.48 | mse  9.48 | mre  0.00 |
scGPT - INFO - | epoch  60 | 600/3602 batches | lr 0.0000 | ms/batch 92.74 | loss 10.80 | mse 10.80 | mre  0.00 |
scGPT - INFO - | epoch  60 | 700/3602 batches | lr 0.0000 | ms/batch 92.64 | loss 11.13 | mse 11.13 | mre  0.00 |
scGPT - INFO - | epoch  60 | 800/3602 batches | lr 0.0000 | ms/batch 92.70 | loss 14.08 | mse 14.08 | mre  0.00 |
scGPT - INFO - | epoch  60 | 900/3602 batches | lr 0.0000 | ms/batch 92.63 | loss 11.05 | mse 11.05 | mre  0.00 |
scGPT - INFO - | epoch  60 | 1000/3602 batches | lr 0.0000 | ms/batch 92.67 | loss 12.39 | mse 12.39 | mre  0.00 |
scGPT - INFO - | epoch  60 | 1100/3602 batches | lr 0.0000 | ms/batch 92.70 | loss 11.00 | mse 11.00 | mre  0.00 |
scGPT - INFO - | epoch  60 | 1200/3602 batches | lr 0.0000 | ms/batch 92.60 | loss 11.03 | mse 11.03 | mre  0.00 |
scGPT - INFO - | epoch  60 | 1300/3602 batches | lr 0.0000 | ms/batch 92.70 | loss 10.71 | mse 10.71 | mre  0.00 |
scGPT - INFO - | epoch  60 | 1400/3602 batches | lr 0.0000 | ms/batch 92.75 | loss 13.41 | mse 13.41 | mre  0.00 |
scGPT - INFO - | epoch  60 | 1500/3602 batches | lr 0.0000 | ms/batch 93.64 | loss 13.50 | mse 13.50 | mre  0.00 |
scGPT - INFO - | epoch  60 | 1600/3602 batches | lr 0.0000 | ms/batch 92.56 | loss  9.68 | mse  9.68 | mre  0.00 |
scGPT - INFO - | epoch  60 | 1700/3602 batches | lr 0.0000 | ms/batch 92.57 | loss 10.52 | mse 10.52 | mre  0.00 |
scGPT - INFO - | epoch  60 | 1800/3602 batches | lr 0.0000 | ms/batch 92.52 | loss 10.50 | mse 10.50 | mre  0.00 |
scGPT - INFO - | epoch  60 | 1900/3602 batches | lr 0.0000 | ms/batch 92.52 | loss 13.14 | mse 13.14 | mre  0.00 |
scGPT - INFO - | epoch  60 | 2000/3602 batches | lr 0.0000 | ms/batch 92.49 | loss 10.99 | mse 10.99 | mre  0.00 |
scGPT - INFO - | epoch  60 | 2100/3602 batches | lr 0.0000 | ms/batch 92.68 | loss 12.58 | mse 12.58 | mre  0.00 |
scGPT - INFO - | epoch  60 | 2200/3602 batches | lr 0.0000 | ms/batch 92.70 | loss 11.03 | mse 11.03 | mre  0.00 |
scGPT - INFO - | epoch  60 | 2300/3602 batches | lr 0.0000 | ms/batch 92.64 | loss 10.86 | mse 10.86 | mre  0.00 |
scGPT - INFO - | epoch  60 | 2400/3602 batches | lr 0.0000 | ms/batch 92.59 | loss 11.57 | mse 11.57 | mre  0.00 |
scGPT - INFO - | epoch  60 | 2500/3602 batches | lr 0.0000 | ms/batch 93.78 | loss 10.25 | mse 10.25 | mre  0.00 |
scGPT - INFO - | epoch  60 | 2600/3602 batches | lr 0.0000 | ms/batch 92.77 | loss 14.57 | mse 14.57 | mre  0.00 |
scGPT - INFO - | epoch  60 | 2700/3602 batches | lr 0.0000 | ms/batch 92.75 | loss 10.79 | mse 10.79 | mre  0.00 |
scGPT - INFO - | epoch  60 | 2800/3602 batches | lr 0.0000 | ms/batch 92.68 | loss 11.62 | mse 11.62 | mre  0.00 |
scGPT - INFO - | epoch  60 | 2900/3602 batches | lr 0.0000 | ms/batch 92.76 | loss 11.67 | mse 11.67 | mre  0.00 |
scGPT - INFO - | epoch  60 | 3000/3602 batches | lr 0.0000 | ms/batch 92.75 | loss 11.35 | mse 11.35 | mre  0.00 |
scGPT - INFO - | epoch  60 | 3100/3602 batches | lr 0.0000 | ms/batch 92.80 | loss 10.39 | mse 10.39 | mre  0.00 |
scGPT - INFO - | epoch  60 | 3200/3602 batches | lr 0.0000 | ms/batch 92.83 | loss 11.41 | mse 11.41 | mre  0.00 |
scGPT - INFO - | epoch  60 | 3300/3602 batches | lr 0.0000 | ms/batch 92.76 | loss  9.33 | mse  9.33 | mre  0.00 |
scGPT - INFO - | epoch  60 | 3400/3602 batches | lr 0.0000 | ms/batch 92.86 | loss 10.46 | mse 10.46 | mre  0.00 |
scGPT - INFO - | epoch  60 | 3500/3602 batches | lr 0.0000 | ms/batch 93.70 | loss 10.22 | mse 10.22 | mre  0.00 |
scGPT - INFO - | epoch  60 | 3600/3602 batches | lr 0.0000 | ms/batch 92.88 | loss 11.65 | mse 11.65 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  60 | time: 346.70s | valid loss/mse 12.5661 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.559127085068997, min_delta 0.0001, val_loss 12.566127740190865
Loss error: -0.007000655121867538
scGPT - INFO - INFO: Early stopping counter 5 of 10
epoch:  60
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.844 3.566 3.281 ... 3.457 3.646 3.883]
 [7.004 3.63  3.338 ... 3.465 3.686 3.916]
 [4.688 3.695 3.822 ... 4.094 3.713 3.55 ]
 ...
 [4.816 3.875 3.898 ... 4.07  3.787 3.58 ]
 [4.926 3.916 3.975 ... 4.125 3.865 3.617]
 [6.914 3.666 3.254 ... 3.475 3.61  3.89 ]]
(801, 11) (801, 11)
By feature:  MSE:  0.5712¬±0.0 MAE:  0.493¬±0.0 R2:  0.2419¬±0.0 PCC:  0.8039¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5712¬±0.0 MAE:  0.493¬±0.0 R2:  0.2862¬±0.0 PCC:  0.5224¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5712000131607056¬±0.0 MAE: 0.49300000071525574¬±0.0 R2: 0.2419¬±0.0 PCC: 0.8039¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5712000131607056¬±0.0 MAE: 0.49300000071525574¬±0.0 R2: 0.2862¬±0.0 PCC: 0.5224¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  61
Training model
scGPT - INFO - | epoch  61 | 100/3602 batches | lr 0.0000 | ms/batch 93.98 | loss 10.48 | mse 10.48 | mre  0.00 |
scGPT - INFO - | epoch  61 | 200/3602 batches | lr 0.0000 | ms/batch 92.80 | loss 12.25 | mse 12.25 | mre  0.00 |
scGPT - INFO - | epoch  61 | 300/3602 batches | lr 0.0000 | ms/batch 92.92 | loss 10.01 | mse 10.01 | mre  0.00 |
scGPT - INFO - | epoch  61 | 400/3602 batches | lr 0.0000 | ms/batch 92.89 | loss 10.35 | mse 10.35 | mre  0.00 |
scGPT - INFO - | epoch  61 | 500/3602 batches | lr 0.0000 | ms/batch 92.87 | loss  9.49 | mse  9.49 | mre  0.00 |
scGPT - INFO - | epoch  61 | 600/3602 batches | lr 0.0000 | ms/batch 92.94 | loss 10.86 | mse 10.86 | mre  0.00 |
scGPT - INFO - | epoch  61 | 700/3602 batches | lr 0.0000 | ms/batch 92.84 | loss 11.23 | mse 11.23 | mre  0.00 |
scGPT - INFO - | epoch  61 | 800/3602 batches | lr 0.0000 | ms/batch 92.81 | loss 14.59 | mse 14.59 | mre  0.00 |
scGPT - INFO - | epoch  61 | 900/3602 batches | lr 0.0000 | ms/batch 93.80 | loss 11.23 | mse 11.23 | mre  0.00 |
scGPT - INFO - | epoch  61 | 1000/3602 batches | lr 0.0000 | ms/batch 92.80 | loss 12.48 | mse 12.48 | mre  0.00 |
scGPT - INFO - | epoch  61 | 1100/3602 batches | lr 0.0000 | ms/batch 92.95 | loss 10.49 | mse 10.49 | mre  0.00 |
scGPT - INFO - | epoch  61 | 1200/3602 batches | lr 0.0000 | ms/batch 92.94 | loss 10.69 | mse 10.69 | mre  0.00 |
scGPT - INFO - | epoch  61 | 1300/3602 batches | lr 0.0000 | ms/batch 92.91 | loss 10.81 | mse 10.81 | mre  0.00 |
scGPT - INFO - | epoch  61 | 1400/3602 batches | lr 0.0000 | ms/batch 92.92 | loss 13.09 | mse 13.09 | mre  0.00 |
scGPT - INFO - | epoch  61 | 1500/3602 batches | lr 0.0000 | ms/batch 92.93 | loss 13.33 | mse 13.33 | mre  0.00 |
scGPT - INFO - | epoch  61 | 1600/3602 batches | lr 0.0000 | ms/batch 92.94 | loss  9.66 | mse  9.66 | mre  0.00 |
scGPT - INFO - | epoch  61 | 1700/3602 batches | lr 0.0000 | ms/batch 92.92 | loss 10.34 | mse 10.34 | mre  0.00 |
scGPT - INFO - | epoch  61 | 1800/3602 batches | lr 0.0000 | ms/batch 93.11 | loss 10.37 | mse 10.37 | mre  0.00 |
scGPT - INFO - | epoch  61 | 1900/3602 batches | lr 0.0000 | ms/batch 94.03 | loss 13.22 | mse 13.22 | mre  0.00 |
scGPT - INFO - | epoch  61 | 2000/3602 batches | lr 0.0000 | ms/batch 92.95 | loss 11.22 | mse 11.22 | mre  0.00 |
scGPT - INFO - | epoch  61 | 2100/3602 batches | lr 0.0000 | ms/batch 92.94 | loss 12.78 | mse 12.78 | mre  0.00 |
scGPT - INFO - | epoch  61 | 2200/3602 batches | lr 0.0000 | ms/batch 92.91 | loss 10.62 | mse 10.62 | mre  0.00 |
scGPT - INFO - | epoch  61 | 2300/3602 batches | lr 0.0000 | ms/batch 92.67 | loss 10.96 | mse 10.96 | mre  0.00 |
scGPT - INFO - | epoch  61 | 2400/3602 batches | lr 0.0000 | ms/batch 92.58 | loss 11.77 | mse 11.77 | mre  0.00 |
scGPT - INFO - | epoch  61 | 2500/3602 batches | lr 0.0000 | ms/batch 92.55 | loss 10.30 | mse 10.30 | mre  0.00 |
scGPT - INFO - | epoch  61 | 2600/3602 batches | lr 0.0000 | ms/batch 92.72 | loss 14.14 | mse 14.14 | mre  0.00 |
scGPT - INFO - | epoch  61 | 2700/3602 batches | lr 0.0000 | ms/batch 92.56 | loss 10.26 | mse 10.26 | mre  0.00 |
scGPT - INFO - | epoch  61 | 2800/3602 batches | lr 0.0000 | ms/batch 92.48 | loss 11.35 | mse 11.35 | mre  0.00 |
scGPT - INFO - | epoch  61 | 2900/3602 batches | lr 0.0000 | ms/batch 93.67 | loss 12.03 | mse 12.03 | mre  0.00 |
scGPT - INFO - | epoch  61 | 3000/3602 batches | lr 0.0000 | ms/batch 92.20 | loss 11.59 | mse 11.59 | mre  0.00 |
scGPT - INFO - | epoch  61 | 3100/3602 batches | lr 0.0000 | ms/batch 91.70 | loss 10.46 | mse 10.46 | mre  0.00 |
scGPT - INFO - | epoch  61 | 3200/3602 batches | lr 0.0000 | ms/batch 91.61 | loss 10.70 | mse 10.70 | mre  0.00 |
scGPT - INFO - | epoch  61 | 3300/3602 batches | lr 0.0000 | ms/batch 91.64 | loss  9.83 | mse  9.83 | mre  0.00 |
scGPT - INFO - | epoch  61 | 3400/3602 batches | lr 0.0000 | ms/batch 91.38 | loss 10.87 | mse 10.87 | mre  0.00 |
scGPT - INFO - | epoch  61 | 3500/3602 batches | lr 0.0000 | ms/batch 91.43 | loss 10.39 | mse 10.39 | mre  0.00 |
scGPT - INFO - | epoch  61 | 3600/3602 batches | lr 0.0000 | ms/batch 91.63 | loss 11.78 | mse 11.78 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  61 | time: 346.24s | valid loss/mse 12.5626 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.559127085068997, min_delta 0.0001, val_loss 12.5626310004426
Loss error: -0.0035039153736029505
scGPT - INFO - INFO: Early stopping counter 6 of 10
epoch:  61
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.844 3.566 3.281 ... 3.457 3.646 3.883]
 [7.004 3.63  3.338 ... 3.465 3.688 3.914]
 [4.688 3.697 3.822 ... 4.098 3.715 3.553]
 ...
 [4.812 3.875 3.9   ... 4.07  3.787 3.58 ]
 [4.926 3.916 3.977 ... 4.125 3.865 3.617]
 [6.914 3.666 3.256 ... 3.475 3.611 3.89 ]]
(801, 11) (801, 11)
By feature:  MSE:  0.5711¬±0.0 MAE:  0.4931¬±0.0 R2:  0.242¬±0.0 PCC:  0.8038¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5711¬±0.0 MAE:  0.4931¬±0.0 R2:  0.2863¬±0.0 PCC:  0.5224¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5710999965667725¬±0.0 MAE: 0.49309998750686646¬±0.0 R2: 0.242¬±0.0 PCC: 0.8038¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5710999965667725¬±0.0 MAE: 0.49309998750686646¬±0.0 R2: 0.2863¬±0.0 PCC: 0.5224¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  62
Training model
scGPT - INFO - | epoch  62 | 100/3602 batches | lr 0.0000 | ms/batch 92.86 | loss 10.83 | mse 10.83 | mre  0.00 |
scGPT - INFO - | epoch  62 | 200/3602 batches | lr 0.0000 | ms/batch 91.66 | loss 12.30 | mse 12.30 | mre  0.00 |
scGPT - INFO - | epoch  62 | 300/3602 batches | lr 0.0000 | ms/batch 92.39 | loss  9.88 | mse  9.88 | mre  0.00 |
scGPT - INFO - | epoch  62 | 400/3602 batches | lr 0.0000 | ms/batch 91.43 | loss 10.92 | mse 10.92 | mre  0.00 |
scGPT - INFO - | epoch  62 | 500/3602 batches | lr 0.0000 | ms/batch 91.70 | loss  9.02 | mse  9.02 | mre  0.00 |
scGPT - INFO - | epoch  62 | 600/3602 batches | lr 0.0000 | ms/batch 91.69 | loss 10.82 | mse 10.82 | mre  0.00 |
scGPT - INFO - | epoch  62 | 700/3602 batches | lr 0.0000 | ms/batch 91.58 | loss 11.08 | mse 11.08 | mre  0.00 |
scGPT - INFO - | epoch  62 | 800/3602 batches | lr 0.0000 | ms/batch 91.62 | loss 13.99 | mse 13.99 | mre  0.00 |
scGPT - INFO - | epoch  62 | 900/3602 batches | lr 0.0000 | ms/batch 91.46 | loss 11.14 | mse 11.14 | mre  0.00 |
scGPT - INFO - | epoch  62 | 1000/3602 batches | lr 0.0000 | ms/batch 91.37 | loss 12.49 | mse 12.49 | mre  0.00 |
scGPT - INFO - | epoch  62 | 1100/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 10.48 | mse 10.48 | mre  0.00 |
scGPT - INFO - | epoch  62 | 1200/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 10.94 | mse 10.94 | mre  0.00 |
scGPT - INFO - | epoch  62 | 1300/3602 batches | lr 0.0000 | ms/batch 91.50 | loss 10.47 | mse 10.47 | mre  0.00 |
scGPT - INFO - | epoch  62 | 1400/3602 batches | lr 0.0000 | ms/batch 91.02 | loss 13.58 | mse 13.58 | mre  0.00 |
scGPT - INFO - | epoch  62 | 1500/3602 batches | lr 0.0000 | ms/batch 91.03 | loss 13.54 | mse 13.54 | mre  0.00 |
scGPT - INFO - | epoch  62 | 1600/3602 batches | lr 0.0000 | ms/batch 90.94 | loss  9.48 | mse  9.48 | mre  0.00 |
scGPT - INFO - | epoch  62 | 1700/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 10.49 | mse 10.49 | mre  0.00 |
scGPT - INFO - | epoch  62 | 1800/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 10.77 | mse 10.77 | mre  0.00 |
scGPT - INFO - | epoch  62 | 1900/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 13.08 | mse 13.08 | mre  0.00 |
scGPT - INFO - | epoch  62 | 2000/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 11.23 | mse 11.23 | mre  0.00 |
scGPT - INFO - | epoch  62 | 2100/3602 batches | lr 0.0000 | ms/batch 90.91 | loss 12.51 | mse 12.51 | mre  0.00 |
scGPT - INFO - | epoch  62 | 2200/3602 batches | lr 0.0000 | ms/batch 90.97 | loss 11.05 | mse 11.05 | mre  0.00 |
scGPT - INFO - | epoch  62 | 2300/3602 batches | lr 0.0000 | ms/batch 91.47 | loss 10.81 | mse 10.81 | mre  0.00 |
scGPT - INFO - | epoch  62 | 2400/3602 batches | lr 0.0000 | ms/batch 91.08 | loss 11.86 | mse 11.86 | mre  0.00 |
scGPT - INFO - | epoch  62 | 2500/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 10.11 | mse 10.11 | mre  0.00 |
scGPT - INFO - | epoch  62 | 2600/3602 batches | lr 0.0000 | ms/batch 90.99 | loss 14.23 | mse 14.23 | mre  0.00 |
scGPT - INFO - | epoch  62 | 2700/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 10.69 | mse 10.69 | mre  0.00 |
scGPT - INFO - | epoch  62 | 2800/3602 batches | lr 0.0000 | ms/batch 90.99 | loss 11.91 | mse 11.91 | mre  0.00 |
scGPT - INFO - | epoch  62 | 2900/3602 batches | lr 0.0000 | ms/batch 91.05 | loss 11.34 | mse 11.34 | mre  0.00 |
scGPT - INFO - | epoch  62 | 3000/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 11.14 | mse 11.14 | mre  0.00 |
scGPT - INFO - | epoch  62 | 3100/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 10.36 | mse 10.36 | mre  0.00 |
scGPT - INFO - | epoch  62 | 3200/3602 batches | lr 0.0000 | ms/batch 90.95 | loss 10.86 | mse 10.86 | mre  0.00 |
scGPT - INFO - | epoch  62 | 3300/3602 batches | lr 0.0000 | ms/batch 91.48 | loss  9.70 | mse  9.70 | mre  0.00 |
scGPT - INFO - | epoch  62 | 3400/3602 batches | lr 0.0000 | ms/batch 91.64 | loss 10.90 | mse 10.90 | mre  0.00 |
scGPT - INFO - | epoch  62 | 3500/3602 batches | lr 0.0000 | ms/batch 91.53 | loss 10.25 | mse 10.25 | mre  0.00 |
scGPT - INFO - | epoch  62 | 3600/3602 batches | lr 0.0000 | ms/batch 91.58 | loss 12.00 | mse 12.00 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  62 | time: 341.03s | valid loss/mse 12.5628 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.559127085068997, min_delta 0.0001, val_loss 12.562768693645944
Loss error: -0.003641608576947064
scGPT - INFO - INFO: Early stopping counter 7 of 10
epoch:  62
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.84  3.566 3.28  ... 3.453 3.645 3.879]
 [7.004 3.63  3.338 ... 3.463 3.688 3.914]
 [4.688 3.695 3.822 ... 4.094 3.713 3.55 ]
 ...
 [4.812 3.875 3.898 ... 4.07  3.787 3.578]
 [4.92  3.914 3.975 ... 4.125 3.863 3.615]
 [6.914 3.666 3.254 ... 3.475 3.61  3.89 ]]
(801, 11) (801, 11)
By feature:  MSE:  0.5711¬±0.0 MAE:  0.4931¬±0.0 R2:  0.2416¬±0.0 PCC:  0.8038¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5711¬±0.0 MAE:  0.4931¬±0.0 R2:  0.2863¬±0.0 PCC:  0.5225¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5710999965667725¬±0.0 MAE: 0.49309998750686646¬±0.0 R2: 0.2416¬±0.0 PCC: 0.8038¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5710999965667725¬±0.0 MAE: 0.49309998750686646¬±0.0 R2: 0.2863¬±0.0 PCC: 0.5225¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  63
Training model
scGPT - INFO - | epoch  63 | 100/3602 batches | lr 0.0000 | ms/batch 92.87 | loss 10.87 | mse 10.87 | mre  0.00 |
scGPT - INFO - | epoch  63 | 200/3602 batches | lr 0.0000 | ms/batch 91.53 | loss 12.14 | mse 12.14 | mre  0.00 |
scGPT - INFO - | epoch  63 | 300/3602 batches | lr 0.0000 | ms/batch 91.58 | loss 10.00 | mse 10.00 | mre  0.00 |
scGPT - INFO - | epoch  63 | 400/3602 batches | lr 0.0000 | ms/batch 91.41 | loss 10.97 | mse 10.97 | mre  0.00 |
scGPT - INFO - | epoch  63 | 500/3602 batches | lr 0.0000 | ms/batch 91.41 | loss  9.12 | mse  9.12 | mre  0.00 |
scGPT - INFO - | epoch  63 | 600/3602 batches | lr 0.0000 | ms/batch 91.34 | loss 10.62 | mse 10.62 | mre  0.00 |
scGPT - INFO - | epoch  63 | 700/3602 batches | lr 0.0000 | ms/batch 92.41 | loss 11.25 | mse 11.25 | mre  0.00 |
scGPT - INFO - | epoch  63 | 800/3602 batches | lr 0.0000 | ms/batch 91.39 | loss 14.03 | mse 14.03 | mre  0.00 |
scGPT - INFO - | epoch  63 | 900/3602 batches | lr 0.0000 | ms/batch 91.84 | loss 11.05 | mse 11.05 | mre  0.00 |
scGPT - INFO - | epoch  63 | 1000/3602 batches | lr 0.0000 | ms/batch 91.50 | loss 12.35 | mse 12.35 | mre  0.00 |
scGPT - INFO - | epoch  63 | 1100/3602 batches | lr 0.0000 | ms/batch 91.61 | loss 10.78 | mse 10.78 | mre  0.00 |
scGPT - INFO - | epoch  63 | 1200/3602 batches | lr 0.0000 | ms/batch 91.68 | loss 11.41 | mse 11.41 | mre  0.00 |
scGPT - INFO - | epoch  63 | 1300/3602 batches | lr 0.0000 | ms/batch 91.54 | loss 10.69 | mse 10.69 | mre  0.00 |
scGPT - INFO - | epoch  63 | 1400/3602 batches | lr 0.0000 | ms/batch 91.27 | loss 13.21 | mse 13.21 | mre  0.00 |
scGPT - INFO - | epoch  63 | 1500/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 13.62 | mse 13.62 | mre  0.00 |
scGPT - INFO - | epoch  63 | 1600/3602 batches | lr 0.0000 | ms/batch 90.85 | loss  9.64 | mse  9.64 | mre  0.00 |
scGPT - INFO - | epoch  63 | 1700/3602 batches | lr 0.0000 | ms/batch 91.28 | loss 10.69 | mse 10.69 | mre  0.00 |
scGPT - INFO - | epoch  63 | 1800/3602 batches | lr 0.0000 | ms/batch 90.62 | loss 10.77 | mse 10.77 | mre  0.00 |
scGPT - INFO - | epoch  63 | 1900/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 12.86 | mse 12.86 | mre  0.00 |
scGPT - INFO - | epoch  63 | 2000/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 10.90 | mse 10.90 | mre  0.00 |
scGPT - INFO - | epoch  63 | 2100/3602 batches | lr 0.0000 | ms/batch 91.59 | loss 12.99 | mse 12.99 | mre  0.00 |
scGPT - INFO - | epoch  63 | 2200/3602 batches | lr 0.0000 | ms/batch 91.50 | loss 10.99 | mse 10.99 | mre  0.00 |
scGPT - INFO - | epoch  63 | 2300/3602 batches | lr 0.0000 | ms/batch 91.45 | loss 11.20 | mse 11.20 | mre  0.00 |
scGPT - INFO - | epoch  63 | 2400/3602 batches | lr 0.0000 | ms/batch 91.48 | loss 11.61 | mse 11.61 | mre  0.00 |
scGPT - INFO - | epoch  63 | 2500/3602 batches | lr 0.0000 | ms/batch 91.43 | loss 10.20 | mse 10.20 | mre  0.00 |
scGPT - INFO - | epoch  63 | 2600/3602 batches | lr 0.0000 | ms/batch 91.44 | loss 14.17 | mse 14.17 | mre  0.00 |
scGPT - INFO - | epoch  63 | 2700/3602 batches | lr 0.0000 | ms/batch 92.17 | loss 10.50 | mse 10.50 | mre  0.00 |
scGPT - INFO - | epoch  63 | 2800/3602 batches | lr 0.0000 | ms/batch 91.34 | loss 11.64 | mse 11.64 | mre  0.00 |
scGPT - INFO - | epoch  63 | 2900/3602 batches | lr 0.0000 | ms/batch 91.02 | loss 11.58 | mse 11.58 | mre  0.00 |
scGPT - INFO - | epoch  63 | 3000/3602 batches | lr 0.0000 | ms/batch 90.61 | loss 11.53 | mse 11.53 | mre  0.00 |
scGPT - INFO - | epoch  63 | 3100/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 10.49 | mse 10.49 | mre  0.00 |
scGPT - INFO - | epoch  63 | 3200/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 11.31 | mse 11.31 | mre  0.00 |
scGPT - INFO - | epoch  63 | 3300/3602 batches | lr 0.0000 | ms/batch 90.95 | loss  9.43 | mse  9.43 | mre  0.00 |
scGPT - INFO - | epoch  63 | 3400/3602 batches | lr 0.0000 | ms/batch 91.62 | loss 11.01 | mse 11.01 | mre  0.00 |
scGPT - INFO - | epoch  63 | 3500/3602 batches | lr 0.0000 | ms/batch 91.65 | loss 10.30 | mse 10.30 | mre  0.00 |
scGPT - INFO - | epoch  63 | 3600/3602 batches | lr 0.0000 | ms/batch 91.48 | loss 11.88 | mse 11.88 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  63 | time: 341.45s | valid loss/mse 12.5601 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.559127085068997, min_delta 0.0001, val_loss 12.560078985235664
Loss error: -0.0009519001666671301
scGPT - INFO - INFO: Early stopping counter 8 of 10
epoch:  63
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.84  3.566 3.281 ... 3.455 3.646 3.88 ]
 [7.004 3.63  3.338 ... 3.463 3.688 3.914]
 [4.688 3.695 3.822 ... 4.094 3.715 3.55 ]
 ...
 [4.812 3.873 3.9   ... 4.07  3.787 3.578]
 [4.92  3.916 3.977 ... 4.125 3.865 3.617]
 [6.91  3.668 3.256 ... 3.475 3.611 3.89 ]]
(801, 11) (801, 11)
By feature:  MSE:  0.571¬±0.0 MAE:  0.493¬±0.0 R2:  0.2418¬±0.0 PCC:  0.8038¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.571¬±0.0 MAE:  0.493¬±0.0 R2:  0.2864¬±0.0 PCC:  0.5225¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5709999799728394¬±0.0 MAE: 0.49300000071525574¬±0.0 R2: 0.2418¬±0.0 PCC: 0.8038¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5709999799728394¬±0.0 MAE: 0.49300000071525574¬±0.0 R2: 0.2864¬±0.0 PCC: 0.5225¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  64
Training model
scGPT - INFO - | epoch  64 | 100/3602 batches | lr 0.0000 | ms/batch 93.49 | loss 10.65 | mse 10.65 | mre  0.00 |
scGPT - INFO - | epoch  64 | 200/3602 batches | lr 0.0000 | ms/batch 91.46 | loss 12.23 | mse 12.23 | mre  0.00 |
scGPT - INFO - | epoch  64 | 300/3602 batches | lr 0.0000 | ms/batch 91.40 | loss  9.95 | mse  9.95 | mre  0.00 |
scGPT - INFO - | epoch  64 | 400/3602 batches | lr 0.0000 | ms/batch 91.48 | loss 10.53 | mse 10.53 | mre  0.00 |
scGPT - INFO - | epoch  64 | 500/3602 batches | lr 0.0000 | ms/batch 91.46 | loss  9.36 | mse  9.36 | mre  0.00 |
scGPT - INFO - | epoch  64 | 600/3602 batches | lr 0.0000 | ms/batch 91.87 | loss 10.59 | mse 10.59 | mre  0.00 |
scGPT - INFO - | epoch  64 | 700/3602 batches | lr 0.0000 | ms/batch 92.01 | loss 11.10 | mse 11.10 | mre  0.00 |
scGPT - INFO - | epoch  64 | 800/3602 batches | lr 0.0000 | ms/batch 92.55 | loss 14.22 | mse 14.22 | mre  0.00 |
scGPT - INFO - | epoch  64 | 900/3602 batches | lr 0.0000 | ms/batch 91.42 | loss 10.95 | mse 10.95 | mre  0.00 |
scGPT - INFO - | epoch  64 | 1000/3602 batches | lr 0.0000 | ms/batch 91.43 | loss 12.37 | mse 12.37 | mre  0.00 |
scGPT - INFO - | epoch  64 | 1100/3602 batches | lr 0.0000 | ms/batch 92.22 | loss 10.76 | mse 10.76 | mre  0.00 |
scGPT - INFO - | epoch  64 | 1200/3602 batches | lr 0.0000 | ms/batch 91.37 | loss 11.14 | mse 11.14 | mre  0.00 |
scGPT - INFO - | epoch  64 | 1300/3602 batches | lr 0.0000 | ms/batch 91.41 | loss 10.68 | mse 10.68 | mre  0.00 |
scGPT - INFO - | epoch  64 | 1400/3602 batches | lr 0.0000 | ms/batch 91.39 | loss 13.09 | mse 13.09 | mre  0.00 |
scGPT - INFO - | epoch  64 | 1500/3602 batches | lr 0.0000 | ms/batch 91.42 | loss 13.39 | mse 13.39 | mre  0.00 |
scGPT - INFO - | epoch  64 | 1600/3602 batches | lr 0.0000 | ms/batch 91.41 | loss  9.69 | mse  9.69 | mre  0.00 |
scGPT - INFO - | epoch  64 | 1700/3602 batches | lr 0.0000 | ms/batch 91.48 | loss 10.71 | mse 10.71 | mre  0.00 |
scGPT - INFO - | epoch  64 | 1800/3602 batches | lr 0.0000 | ms/batch 91.39 | loss 10.32 | mse 10.32 | mre  0.00 |
scGPT - INFO - | epoch  64 | 1900/3602 batches | lr 0.0000 | ms/batch 91.42 | loss 13.10 | mse 13.10 | mre  0.00 |
scGPT - INFO - | epoch  64 | 2000/3602 batches | lr 0.0000 | ms/batch 91.41 | loss 11.30 | mse 11.30 | mre  0.00 |
scGPT - INFO - | epoch  64 | 2100/3602 batches | lr 0.0000 | ms/batch 92.23 | loss 12.78 | mse 12.78 | mre  0.00 |
scGPT - INFO - | epoch  64 | 2200/3602 batches | lr 0.0000 | ms/batch 91.46 | loss 10.93 | mse 10.93 | mre  0.00 |
scGPT - INFO - | epoch  64 | 2300/3602 batches | lr 0.0000 | ms/batch 91.42 | loss 11.10 | mse 11.10 | mre  0.00 |
scGPT - INFO - | epoch  64 | 2400/3602 batches | lr 0.0000 | ms/batch 91.46 | loss 11.77 | mse 11.77 | mre  0.00 |
scGPT - INFO - | epoch  64 | 2500/3602 batches | lr 0.0000 | ms/batch 91.39 | loss 10.14 | mse 10.14 | mre  0.00 |
scGPT - INFO - | epoch  64 | 2600/3602 batches | lr 0.0000 | ms/batch 91.69 | loss 14.04 | mse 14.04 | mre  0.00 |
scGPT - INFO - | epoch  64 | 2700/3602 batches | lr 0.0000 | ms/batch 91.42 | loss 10.46 | mse 10.46 | mre  0.00 |
scGPT - INFO - | epoch  64 | 2800/3602 batches | lr 0.0000 | ms/batch 91.50 | loss 11.60 | mse 11.60 | mre  0.00 |
scGPT - INFO - | epoch  64 | 2900/3602 batches | lr 0.0000 | ms/batch 91.42 | loss 11.61 | mse 11.61 | mre  0.00 |
scGPT - INFO - | epoch  64 | 3000/3602 batches | lr 0.0000 | ms/batch 91.44 | loss 11.36 | mse 11.36 | mre  0.00 |
scGPT - INFO - | epoch  64 | 3100/3602 batches | lr 0.0000 | ms/batch 92.30 | loss 10.60 | mse 10.60 | mre  0.00 |
scGPT - INFO - | epoch  64 | 3200/3602 batches | lr 0.0000 | ms/batch 91.39 | loss 11.34 | mse 11.34 | mre  0.00 |
scGPT - INFO - | epoch  64 | 3300/3602 batches | lr 0.0000 | ms/batch 91.41 | loss  9.82 | mse  9.82 | mre  0.00 |
scGPT - INFO - | epoch  64 | 3400/3602 batches | lr 0.0000 | ms/batch 91.35 | loss 10.54 | mse 10.54 | mre  0.00 |
scGPT - INFO - | epoch  64 | 3500/3602 batches | lr 0.0000 | ms/batch 91.50 | loss 10.33 | mse 10.33 | mre  0.00 |
scGPT - INFO - | epoch  64 | 3600/3602 batches | lr 0.0000 | ms/batch 91.45 | loss 11.76 | mse 11.76 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  64 | time: 342.31s | valid loss/mse 12.5603 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.559127085068997, min_delta 0.0001, val_loss 12.56028308791019
Loss error: -0.0011560028411921053
scGPT - INFO - INFO: Early stopping counter 9 of 10
epoch:  64
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.844 3.566 3.281 ... 3.453 3.645 3.88 ]
 [7.004 3.629 3.338 ... 3.46  3.686 3.914]
 [4.688 3.695 3.822 ... 4.094 3.713 3.55 ]
 ...
 [4.812 3.873 3.898 ... 4.066 3.785 3.578]
 [4.926 3.914 3.977 ... 4.125 3.863 3.615]
 [6.914 3.668 3.256 ... 3.475 3.611 3.89 ]]
(801, 11) (801, 11)
By feature:  MSE:  0.571¬±0.0 MAE:  0.493¬±0.0 R2:  0.242¬±0.0 PCC:  0.8037¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.571¬±0.0 MAE:  0.493¬±0.0 R2:  0.2865¬±0.0 PCC:  0.5226¬±0.0 Cosine Similarity:  0.9854¬±0.0
scGPT - INFO - By feature: MSE: 0.5709999799728394¬±0.0 MAE: 0.49300000071525574¬±0.0 R2: 0.242¬±0.0 PCC: 0.8037¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5709999799728394¬±0.0 MAE: 0.49300000071525574¬±0.0 R2: 0.2865¬±0.0 PCC: 0.5226¬±0.0 Cosine Similarity: 0.9854¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  65
Training model
scGPT - INFO - | epoch  65 | 100/3602 batches | lr 0.0000 | ms/batch 92.85 | loss 10.59 | mse 10.59 | mre  0.00 |
scGPT - INFO - | epoch  65 | 200/3602 batches | lr 0.0000 | ms/batch 91.69 | loss 12.17 | mse 12.17 | mre  0.00 |
scGPT - INFO - | epoch  65 | 300/3602 batches | lr 0.0000 | ms/batch 91.69 | loss 10.10 | mse 10.10 | mre  0.00 |
scGPT - INFO - | epoch  65 | 400/3602 batches | lr 0.0000 | ms/batch 91.47 | loss 10.69 | mse 10.69 | mre  0.00 |
scGPT - INFO - | epoch  65 | 500/3602 batches | lr 0.0000 | ms/batch 92.37 | loss  9.36 | mse  9.36 | mre  0.00 |
scGPT - INFO - | epoch  65 | 600/3602 batches | lr 0.0000 | ms/batch 91.39 | loss 10.83 | mse 10.83 | mre  0.00 |
scGPT - INFO - | epoch  65 | 700/3602 batches | lr 0.0000 | ms/batch 91.40 | loss 11.40 | mse 11.40 | mre  0.00 |
scGPT - INFO - | epoch  65 | 800/3602 batches | lr 0.0000 | ms/batch 91.42 | loss 14.03 | mse 14.03 | mre  0.00 |
scGPT - INFO - | epoch  65 | 900/3602 batches | lr 0.0000 | ms/batch 91.41 | loss 10.82 | mse 10.82 | mre  0.00 |
scGPT - INFO - | epoch  65 | 1000/3602 batches | lr 0.0000 | ms/batch 91.47 | loss 12.12 | mse 12.12 | mre  0.00 |
scGPT - INFO - | epoch  65 | 1100/3602 batches | lr 0.0000 | ms/batch 91.88 | loss 10.39 | mse 10.39 | mre  0.00 |
scGPT - INFO - | epoch  65 | 1200/3602 batches | lr 0.0000 | ms/batch 91.86 | loss 11.10 | mse 11.10 | mre  0.00 |
scGPT - INFO - | epoch  65 | 1300/3602 batches | lr 0.0000 | ms/batch 91.87 | loss 10.38 | mse 10.38 | mre  0.00 |
scGPT - INFO - | epoch  65 | 1400/3602 batches | lr 0.0000 | ms/batch 91.89 | loss 13.65 | mse 13.65 | mre  0.00 |
scGPT - INFO - | epoch  65 | 1500/3602 batches | lr 0.0000 | ms/batch 92.55 | loss 13.23 | mse 13.23 | mre  0.00 |
scGPT - INFO - | epoch  65 | 1600/3602 batches | lr 0.0000 | ms/batch 91.89 | loss  9.66 | mse  9.66 | mre  0.00 |
scGPT - INFO - | epoch  65 | 1700/3602 batches | lr 0.0000 | ms/batch 91.87 | loss 10.48 | mse 10.48 | mre  0.00 |
scGPT - INFO - | epoch  65 | 1800/3602 batches | lr 0.0000 | ms/batch 91.81 | loss 10.64 | mse 10.64 | mre  0.00 |
scGPT - INFO - | epoch  65 | 1900/3602 batches | lr 0.0000 | ms/batch 91.98 | loss 12.88 | mse 12.88 | mre  0.00 |
scGPT - INFO - | epoch  65 | 2000/3602 batches | lr 0.0000 | ms/batch 91.62 | loss 11.04 | mse 11.04 | mre  0.00 |
scGPT - INFO - | epoch  65 | 2100/3602 batches | lr 0.0000 | ms/batch 91.60 | loss 12.88 | mse 12.88 | mre  0.00 |
scGPT - INFO - | epoch  65 | 2200/3602 batches | lr 0.0000 | ms/batch 91.57 | loss 11.27 | mse 11.27 | mre  0.00 |
scGPT - INFO - | epoch  65 | 2300/3602 batches | lr 0.0000 | ms/batch 91.03 | loss 11.03 | mse 11.03 | mre  0.00 |
scGPT - INFO - | epoch  65 | 2400/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 11.49 | mse 11.49 | mre  0.00 |
scGPT - INFO - | epoch  65 | 2500/3602 batches | lr 0.0000 | ms/batch 91.17 | loss 10.17 | mse 10.17 | mre  0.00 |
scGPT - INFO - | epoch  65 | 2600/3602 batches | lr 0.0000 | ms/batch 90.60 | loss 14.56 | mse 14.56 | mre  0.00 |
scGPT - INFO - | epoch  65 | 2700/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 10.83 | mse 10.83 | mre  0.00 |
scGPT - INFO - | epoch  65 | 2800/3602 batches | lr 0.0000 | ms/batch 90.62 | loss 11.89 | mse 11.89 | mre  0.00 |
scGPT - INFO - | epoch  65 | 2900/3602 batches | lr 0.0000 | ms/batch 90.60 | loss 11.78 | mse 11.78 | mre  0.00 |
scGPT - INFO - | epoch  65 | 3000/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 11.86 | mse 11.86 | mre  0.00 |
scGPT - INFO - | epoch  65 | 3100/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 10.18 | mse 10.18 | mre  0.00 |
scGPT - INFO - | epoch  65 | 3200/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 10.86 | mse 10.86 | mre  0.00 |
scGPT - INFO - | epoch  65 | 3300/3602 batches | lr 0.0000 | ms/batch 90.65 | loss  9.60 | mse  9.60 | mre  0.00 |
scGPT - INFO - | epoch  65 | 3400/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 10.50 | mse 10.50 | mre  0.00 |
scGPT - INFO - | epoch  65 | 3500/3602 batches | lr 0.0000 | ms/batch 91.16 | loss  9.89 | mse  9.89 | mre  0.00 |
scGPT - INFO - | epoch  65 | 3600/3602 batches | lr 0.0000 | ms/batch 90.62 | loss 11.74 | mse 11.74 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  65 | time: 341.46s | valid loss/mse 12.5564 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.5564
best_loss: 12.559127085068997, min_delta 0.0001, val_loss 12.55644154042638
Loss error: 0.002685544642616833
epoch:  65
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.84  3.566 3.281 ... 3.453 3.645 3.88 ]
 [7.    3.629 3.338 ... 3.46  3.686 3.914]
 [4.684 3.695 3.82  ... 4.094 3.713 3.55 ]
 ...
 [4.81  3.873 3.898 ... 4.066 3.785 3.578]
 [4.92  3.914 3.975 ... 4.12  3.863 3.615]
 [6.906 3.668 3.256 ... 3.475 3.61  3.889]]
(801, 11) (801, 11)
By feature:  MSE:  0.5708¬±0.0 MAE:  0.493¬±0.0 R2:  0.2423¬±0.0 PCC:  0.8037¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5708¬±0.0 MAE:  0.493¬±0.0 R2:  0.2866¬±0.0 PCC:  0.5226¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5708000063896179¬±0.0 MAE: 0.49300000071525574¬±0.0 R2: 0.2423¬±0.0 PCC: 0.8037¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5708000063896179¬±0.0 MAE: 0.49300000071525574¬±0.0 R2: 0.2866¬±0.0 PCC: 0.5226¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  66
Training model
scGPT - INFO - | epoch  66 | 100/3602 batches | lr 0.0000 | ms/batch 91.99 | loss 10.48 | mse 10.48 | mre  0.00 |
scGPT - INFO - | epoch  66 | 200/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 12.00 | mse 12.00 | mre  0.00 |
scGPT - INFO - | epoch  66 | 300/3602 batches | lr 0.0000 | ms/batch 90.61 | loss  9.88 | mse  9.88 | mre  0.00 |
scGPT - INFO - | epoch  66 | 400/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 10.86 | mse 10.86 | mre  0.00 |
scGPT - INFO - | epoch  66 | 500/3602 batches | lr 0.0000 | ms/batch 91.01 | loss  9.37 | mse  9.37 | mre  0.00 |
scGPT - INFO - | epoch  66 | 600/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 10.57 | mse 10.57 | mre  0.00 |
scGPT - INFO - | epoch  66 | 700/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 11.29 | mse 11.29 | mre  0.00 |
scGPT - INFO - | epoch  66 | 800/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 14.10 | mse 14.10 | mre  0.00 |
scGPT - INFO - | epoch  66 | 900/3602 batches | lr 0.0000 | ms/batch 91.31 | loss 11.47 | mse 11.47 | mre  0.00 |
scGPT - INFO - | epoch  66 | 1000/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 12.00 | mse 12.00 | mre  0.00 |
scGPT - INFO - | epoch  66 | 1100/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 10.32 | mse 10.32 | mre  0.00 |
scGPT - INFO - | epoch  66 | 1200/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 10.89 | mse 10.89 | mre  0.00 |
scGPT - INFO - | epoch  66 | 1300/3602 batches | lr 0.0000 | ms/batch 90.59 | loss 10.42 | mse 10.42 | mre  0.00 |
scGPT - INFO - | epoch  66 | 1400/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 13.46 | mse 13.46 | mre  0.00 |
scGPT - INFO - | epoch  66 | 1500/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 13.68 | mse 13.68 | mre  0.00 |
scGPT - INFO - | epoch  66 | 1600/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 10.17 | mse 10.17 | mre  0.00 |
scGPT - INFO - | epoch  66 | 1700/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 10.37 | mse 10.37 | mre  0.00 |
scGPT - INFO - | epoch  66 | 1800/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 10.33 | mse 10.33 | mre  0.00 |
scGPT - INFO - | epoch  66 | 1900/3602 batches | lr 0.0000 | ms/batch 91.22 | loss 13.18 | mse 13.18 | mre  0.00 |
scGPT - INFO - | epoch  66 | 2000/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 11.22 | mse 11.22 | mre  0.00 |
scGPT - INFO - | epoch  66 | 2100/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 12.38 | mse 12.38 | mre  0.00 |
scGPT - INFO - | epoch  66 | 2200/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 10.85 | mse 10.85 | mre  0.00 |
scGPT - INFO - | epoch  66 | 2300/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 11.03 | mse 11.03 | mre  0.00 |
scGPT - INFO - | epoch  66 | 2400/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 11.83 | mse 11.83 | mre  0.00 |
scGPT - INFO - | epoch  66 | 2500/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 10.50 | mse 10.50 | mre  0.00 |
scGPT - INFO - | epoch  66 | 2600/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 14.14 | mse 14.14 | mre  0.00 |
scGPT - INFO - | epoch  66 | 2700/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.46 | mse 10.46 | mre  0.00 |
scGPT - INFO - | epoch  66 | 2800/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 11.75 | mse 11.75 | mre  0.00 |
scGPT - INFO - | epoch  66 | 2900/3602 batches | lr 0.0000 | ms/batch 91.31 | loss 11.46 | mse 11.46 | mre  0.00 |
scGPT - INFO - | epoch  66 | 3000/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 11.66 | mse 11.66 | mre  0.00 |
scGPT - INFO - | epoch  66 | 3100/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 10.20 | mse 10.20 | mre  0.00 |
scGPT - INFO - | epoch  66 | 3200/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 10.91 | mse 10.91 | mre  0.00 |
scGPT - INFO - | epoch  66 | 3300/3602 batches | lr 0.0000 | ms/batch 90.78 | loss  9.81 | mse  9.81 | mre  0.00 |
scGPT - INFO - | epoch  66 | 3400/3602 batches | lr 0.0000 | ms/batch 91.81 | loss 10.45 | mse 10.45 | mre  0.00 |
scGPT - INFO - | epoch  66 | 3500/3602 batches | lr 0.0000 | ms/batch 91.86 | loss 10.74 | mse 10.74 | mre  0.00 |
scGPT - INFO - | epoch  66 | 3600/3602 batches | lr 0.0000 | ms/batch 91.76 | loss 12.20 | mse 12.20 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  66 | time: 339.70s | valid loss/mse 12.5572 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.55644154042638, min_delta 0.0001, val_loss 12.557186503758592
Loss error: -0.0007449633322114124
scGPT - INFO - INFO: Early stopping counter 1 of 10
epoch:  66
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.84  3.566 3.283 ... 3.453 3.646 3.88 ]
 [7.    3.63  3.338 ... 3.463 3.688 3.914]
 [4.688 3.697 3.824 ... 4.094 3.715 3.553]
 ...
 [4.812 3.875 3.9   ... 4.07  3.787 3.58 ]
 [4.926 3.916 3.977 ... 4.125 3.865 3.617]
 [6.91  3.666 3.256 ... 3.475 3.611 3.89 ]]
(801, 11) (801, 11)
By feature:  MSE:  0.5708¬±0.0 MAE:  0.493¬±0.0 R2:  0.2422¬±0.0 PCC:  0.8039¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5708¬±0.0 MAE:  0.493¬±0.0 R2:  0.2866¬±0.0 PCC:  0.5226¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5708000063896179¬±0.0 MAE: 0.49300000071525574¬±0.0 R2: 0.2422¬±0.0 PCC: 0.8039¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5708000063896179¬±0.0 MAE: 0.49300000071525574¬±0.0 R2: 0.2866¬±0.0 PCC: 0.5226¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  67
Training model
scGPT - INFO - | epoch  67 | 100/3602 batches | lr 0.0000 | ms/batch 91.92 | loss 11.13 | mse 11.13 | mre  0.00 |
scGPT - INFO - | epoch  67 | 200/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 12.13 | mse 12.13 | mre  0.00 |
scGPT - INFO - | epoch  67 | 300/3602 batches | lr 0.0000 | ms/batch 91.26 | loss  9.97 | mse  9.97 | mre  0.00 |
scGPT - INFO - | epoch  67 | 400/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 10.62 | mse 10.62 | mre  0.00 |
scGPT - INFO - | epoch  67 | 500/3602 batches | lr 0.0000 | ms/batch 90.64 | loss  9.12 | mse  9.12 | mre  0.00 |
scGPT - INFO - | epoch  67 | 600/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 11.02 | mse 11.02 | mre  0.00 |
scGPT - INFO - | epoch  67 | 700/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 11.14 | mse 11.14 | mre  0.00 |
scGPT - INFO - | epoch  67 | 800/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 14.24 | mse 14.24 | mre  0.00 |
scGPT - INFO - | epoch  67 | 900/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 10.82 | mse 10.82 | mre  0.00 |
scGPT - INFO - | epoch  67 | 1000/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 12.64 | mse 12.64 | mre  0.00 |
scGPT - INFO - | epoch  67 | 1100/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 10.35 | mse 10.35 | mre  0.00 |
scGPT - INFO - | epoch  67 | 1200/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 11.04 | mse 11.04 | mre  0.00 |
scGPT - INFO - | epoch  67 | 1300/3602 batches | lr 0.0000 | ms/batch 91.21 | loss 10.70 | mse 10.70 | mre  0.00 |
scGPT - INFO - | epoch  67 | 1400/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 13.24 | mse 13.24 | mre  0.00 |
scGPT - INFO - | epoch  67 | 1500/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 13.70 | mse 13.70 | mre  0.00 |
scGPT - INFO - | epoch  67 | 1600/3602 batches | lr 0.0000 | ms/batch 90.70 | loss  9.61 | mse  9.61 | mre  0.00 |
scGPT - INFO - | epoch  67 | 1700/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 10.38 | mse 10.38 | mre  0.00 |
scGPT - INFO - | epoch  67 | 1800/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 10.45 | mse 10.45 | mre  0.00 |
scGPT - INFO - | epoch  67 | 1900/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 13.04 | mse 13.04 | mre  0.00 |
scGPT - INFO - | epoch  67 | 2000/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 11.44 | mse 11.44 | mre  0.00 |
scGPT - INFO - | epoch  67 | 2100/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 12.82 | mse 12.82 | mre  0.00 |
scGPT - INFO - | epoch  67 | 2200/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.91 | mse 10.91 | mre  0.00 |
scGPT - INFO - | epoch  67 | 2300/3602 batches | lr 0.0000 | ms/batch 91.32 | loss 10.92 | mse 10.92 | mre  0.00 |
scGPT - INFO - | epoch  67 | 2400/3602 batches | lr 0.0000 | ms/batch 90.93 | loss 11.35 | mse 11.35 | mre  0.00 |
scGPT - INFO - | epoch  67 | 2500/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 10.11 | mse 10.11 | mre  0.00 |
scGPT - INFO - | epoch  67 | 2600/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 14.10 | mse 14.10 | mre  0.00 |
scGPT - INFO - | epoch  67 | 2700/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 10.52 | mse 10.52 | mre  0.00 |
scGPT - INFO - | epoch  67 | 2800/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 11.66 | mse 11.66 | mre  0.00 |
scGPT - INFO - | epoch  67 | 2900/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 11.67 | mse 11.67 | mre  0.00 |
scGPT - INFO - | epoch  67 | 3000/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 11.72 | mse 11.72 | mre  0.00 |
scGPT - INFO - | epoch  67 | 3100/3602 batches | lr 0.0000 | ms/batch 90.97 | loss 10.44 | mse 10.44 | mre  0.00 |
scGPT - INFO - | epoch  67 | 3200/3602 batches | lr 0.0000 | ms/batch 91.05 | loss 10.89 | mse 10.89 | mre  0.00 |
scGPT - INFO - | epoch  67 | 3300/3602 batches | lr 0.0000 | ms/batch 91.57 | loss  9.52 | mse  9.52 | mre  0.00 |
scGPT - INFO - | epoch  67 | 3400/3602 batches | lr 0.0000 | ms/batch 90.97 | loss 10.83 | mse 10.83 | mre  0.00 |
scGPT - INFO - | epoch  67 | 3500/3602 batches | lr 0.0000 | ms/batch 90.97 | loss 10.54 | mse 10.54 | mre  0.00 |
scGPT - INFO - | epoch  67 | 3600/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 11.58 | mse 11.58 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  67 | time: 339.68s | valid loss/mse 12.5577 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.55644154042638, min_delta 0.0001, val_loss 12.557666382316347
Loss error: -0.0012248418899662994
scGPT - INFO - INFO: Early stopping counter 2 of 10
epoch:  67
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.84  3.566 3.283 ... 3.453 3.646 3.88 ]
 [7.    3.63  3.338 ... 3.46  3.688 3.914]
 [4.684 3.695 3.82  ... 4.094 3.715 3.55 ]
 ...
 [4.812 3.875 3.898 ... 4.07  3.787 3.58 ]
 [4.92  3.918 3.977 ... 4.125 3.865 3.617]
 [6.91  3.664 3.254 ... 3.473 3.61  3.889]]
(801, 11) (801, 11)
By feature:  MSE:  0.5708¬±0.0 MAE:  0.4929¬±0.0 R2:  0.2422¬±0.0 PCC:  0.804¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5708¬±0.0 MAE:  0.4929¬±0.0 R2:  0.2865¬±0.0 PCC:  0.5226¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5708000063896179¬±0.0 MAE: 0.492900013923645¬±0.0 R2: 0.2422¬±0.0 PCC: 0.804¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5708000063896179¬±0.0 MAE: 0.492900013923645¬±0.0 R2: 0.2865¬±0.0 PCC: 0.5226¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  68
Training model
scGPT - INFO - | epoch  68 | 100/3602 batches | lr 0.0000 | ms/batch 92.14 | loss 10.69 | mse 10.69 | mre  0.00 |
scGPT - INFO - | epoch  68 | 200/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 12.10 | mse 12.10 | mre  0.00 |
scGPT - INFO - | epoch  68 | 300/3602 batches | lr 0.0000 | ms/batch 90.69 | loss  9.93 | mse  9.93 | mre  0.00 |
scGPT - INFO - | epoch  68 | 400/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 10.55 | mse 10.55 | mre  0.00 |
scGPT - INFO - | epoch  68 | 500/3602 batches | lr 0.0000 | ms/batch 90.68 | loss  9.76 | mse  9.76 | mre  0.00 |
scGPT - INFO - | epoch  68 | 600/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 11.11 | mse 11.11 | mre  0.00 |
scGPT - INFO - | epoch  68 | 700/3602 batches | lr 0.0000 | ms/batch 91.21 | loss 11.06 | mse 11.06 | mre  0.00 |
scGPT - INFO - | epoch  68 | 800/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 14.02 | mse 14.02 | mre  0.00 |
scGPT - INFO - | epoch  68 | 900/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 11.21 | mse 11.21 | mre  0.00 |
scGPT - INFO - | epoch  68 | 1000/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 12.41 | mse 12.41 | mre  0.00 |
scGPT - INFO - | epoch  68 | 1100/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 10.44 | mse 10.44 | mre  0.00 |
scGPT - INFO - | epoch  68 | 1200/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 11.12 | mse 11.12 | mre  0.00 |
scGPT - INFO - | epoch  68 | 1300/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 10.22 | mse 10.22 | mre  0.00 |
scGPT - INFO - | epoch  68 | 1400/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 13.30 | mse 13.30 | mre  0.00 |
scGPT - INFO - | epoch  68 | 1500/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 13.50 | mse 13.50 | mre  0.00 |
scGPT - INFO - | epoch  68 | 1600/3602 batches | lr 0.0000 | ms/batch 90.91 | loss  9.70 | mse  9.70 | mre  0.00 |
scGPT - INFO - | epoch  68 | 1700/3602 batches | lr 0.0000 | ms/batch 91.34 | loss 10.35 | mse 10.35 | mre  0.00 |
scGPT - INFO - | epoch  68 | 1800/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 10.54 | mse 10.54 | mre  0.00 |
scGPT - INFO - | epoch  68 | 1900/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 13.18 | mse 13.18 | mre  0.00 |
scGPT - INFO - | epoch  68 | 2000/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 11.77 | mse 11.77 | mre  0.00 |
scGPT - INFO - | epoch  68 | 2100/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 12.37 | mse 12.37 | mre  0.00 |
scGPT - INFO - | epoch  68 | 2200/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 10.98 | mse 10.98 | mre  0.00 |
scGPT - INFO - | epoch  68 | 2300/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.73 | mse 10.73 | mre  0.00 |
scGPT - INFO - | epoch  68 | 2400/3602 batches | lr 0.0000 | ms/batch 90.95 | loss 11.72 | mse 11.72 | mre  0.00 |
scGPT - INFO - | epoch  68 | 2500/3602 batches | lr 0.0000 | ms/batch 90.76 | loss  9.91 | mse  9.91 | mre  0.00 |
scGPT - INFO - | epoch  68 | 2600/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 14.39 | mse 14.39 | mre  0.00 |
scGPT - INFO - | epoch  68 | 2700/3602 batches | lr 0.0000 | ms/batch 91.25 | loss 10.52 | mse 10.52 | mre  0.00 |
scGPT - INFO - | epoch  68 | 2800/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 11.94 | mse 11.94 | mre  0.00 |
scGPT - INFO - | epoch  68 | 2900/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 11.39 | mse 11.39 | mre  0.00 |
scGPT - INFO - | epoch  68 | 3000/3602 batches | lr 0.0000 | ms/batch 91.09 | loss 11.71 | mse 11.71 | mre  0.00 |
scGPT - INFO - | epoch  68 | 3100/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 10.52 | mse 10.52 | mre  0.00 |
scGPT - INFO - | epoch  68 | 3200/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 10.81 | mse 10.81 | mre  0.00 |
scGPT - INFO - | epoch  68 | 3300/3602 batches | lr 0.0000 | ms/batch 90.79 | loss  9.90 | mse  9.90 | mre  0.00 |
scGPT - INFO - | epoch  68 | 3400/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 10.66 | mse 10.66 | mre  0.00 |
scGPT - INFO - | epoch  68 | 3500/3602 batches | lr 0.0000 | ms/batch 90.61 | loss 10.07 | mse 10.07 | mre  0.00 |
scGPT - INFO - | epoch  68 | 3600/3602 batches | lr 0.0000 | ms/batch 90.62 | loss 12.09 | mse 12.09 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  68 | time: 339.51s | valid loss/mse 12.5553 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.5553
best_loss: 12.55644154042638, min_delta 0.0001, val_loss 12.555299157283725
Loss error: 0.0011423831426551345
epoch:  68
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.836 3.566 3.283 ... 3.453 3.646 3.879]
 [6.996 3.63  3.338 ... 3.46  3.686 3.914]
 [4.684 3.695 3.82  ... 4.094 3.713 3.55 ]
 ...
 [4.81  3.875 3.898 ... 4.066 3.787 3.578]
 [4.92  3.918 3.977 ... 4.12  3.865 3.615]
 [6.906 3.668 3.256 ... 3.473 3.61  3.889]]
(801, 11) (801, 11)
By feature:  MSE:  0.5707¬±0.0 MAE:  0.493¬±0.0 R2:  0.2424¬±0.0 PCC:  0.8038¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5707¬±0.0 MAE:  0.493¬±0.0 R2:  0.2866¬±0.0 PCC:  0.5226¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5706999897956848¬±0.0 MAE: 0.49300000071525574¬±0.0 R2: 0.2424¬±0.0 PCC: 0.8038¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5706999897956848¬±0.0 MAE: 0.49300000071525574¬±0.0 R2: 0.2866¬±0.0 PCC: 0.5226¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  69
Training model
scGPT - INFO - | epoch  69 | 100/3602 batches | lr 0.0000 | ms/batch 92.38 | loss 10.47 | mse 10.47 | mre  0.00 |
scGPT - INFO - | epoch  69 | 200/3602 batches | lr 0.0000 | ms/batch 90.61 | loss 12.40 | mse 12.40 | mre  0.00 |
scGPT - INFO - | epoch  69 | 300/3602 batches | lr 0.0000 | ms/batch 90.69 | loss  9.55 | mse  9.55 | mre  0.00 |
scGPT - INFO - | epoch  69 | 400/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 10.64 | mse 10.64 | mre  0.00 |
scGPT - INFO - | epoch  69 | 500/3602 batches | lr 0.0000 | ms/batch 90.71 | loss  9.28 | mse  9.28 | mre  0.00 |
scGPT - INFO - | epoch  69 | 600/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 10.86 | mse 10.86 | mre  0.00 |
scGPT - INFO - | epoch  69 | 700/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 11.18 | mse 11.18 | mre  0.00 |
scGPT - INFO - | epoch  69 | 800/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 14.45 | mse 14.45 | mre  0.00 |
scGPT - INFO - | epoch  69 | 900/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 11.31 | mse 11.31 | mre  0.00 |
scGPT - INFO - | epoch  69 | 1000/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 12.39 | mse 12.39 | mre  0.00 |
scGPT - INFO - | epoch  69 | 1100/3602 batches | lr 0.0000 | ms/batch 91.17 | loss 10.41 | mse 10.41 | mre  0.00 |
scGPT - INFO - | epoch  69 | 1200/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 10.49 | mse 10.49 | mre  0.00 |
scGPT - INFO - | epoch  69 | 1300/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 10.62 | mse 10.62 | mre  0.00 |
scGPT - INFO - | epoch  69 | 1400/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 13.36 | mse 13.36 | mre  0.00 |
scGPT - INFO - | epoch  69 | 1500/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 13.51 | mse 13.51 | mre  0.00 |
scGPT - INFO - | epoch  69 | 1600/3602 batches | lr 0.0000 | ms/batch 90.59 | loss  9.41 | mse  9.41 | mre  0.00 |
scGPT - INFO - | epoch  69 | 1700/3602 batches | lr 0.0000 | ms/batch 91.57 | loss 10.69 | mse 10.69 | mre  0.00 |
scGPT - INFO - | epoch  69 | 1800/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.64 | mse 10.64 | mre  0.00 |
scGPT - INFO - | epoch  69 | 1900/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 13.06 | mse 13.06 | mre  0.00 |
scGPT - INFO - | epoch  69 | 2000/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 11.06 | mse 11.06 | mre  0.00 |
scGPT - INFO - | epoch  69 | 2100/3602 batches | lr 0.0000 | ms/batch 91.16 | loss 12.56 | mse 12.56 | mre  0.00 |
scGPT - INFO - | epoch  69 | 2200/3602 batches | lr 0.0000 | ms/batch 90.63 | loss 11.01 | mse 11.01 | mre  0.00 |
scGPT - INFO - | epoch  69 | 2300/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 10.96 | mse 10.96 | mre  0.00 |
scGPT - INFO - | epoch  69 | 2400/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 12.03 | mse 12.03 | mre  0.00 |
scGPT - INFO - | epoch  69 | 2500/3602 batches | lr 0.0000 | ms/batch 90.73 | loss  9.78 | mse  9.78 | mre  0.00 |
scGPT - INFO - | epoch  69 | 2600/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 14.17 | mse 14.17 | mre  0.00 |
scGPT - INFO - | epoch  69 | 2700/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 10.49 | mse 10.49 | mre  0.00 |
scGPT - INFO - | epoch  69 | 2800/3602 batches | lr 0.0000 | ms/batch 90.81 | loss 11.96 | mse 11.96 | mre  0.00 |
scGPT - INFO - | epoch  69 | 2900/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 11.41 | mse 11.41 | mre  0.00 |
scGPT - INFO - | epoch  69 | 3000/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 11.37 | mse 11.37 | mre  0.00 |
scGPT - INFO - | epoch  69 | 3100/3602 batches | lr 0.0000 | ms/batch 91.26 | loss 10.28 | mse 10.28 | mre  0.00 |
scGPT - INFO - | epoch  69 | 3200/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 11.01 | mse 11.01 | mre  0.00 |
scGPT - INFO - | epoch  69 | 3300/3602 batches | lr 0.0000 | ms/batch 90.74 | loss  9.73 | mse  9.73 | mre  0.00 |
scGPT - INFO - | epoch  69 | 3400/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 10.94 | mse 10.94 | mre  0.00 |
scGPT - INFO - | epoch  69 | 3500/3602 batches | lr 0.0000 | ms/batch 90.74 | loss  9.97 | mse  9.97 | mre  0.00 |
scGPT - INFO - | epoch  69 | 3600/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 11.53 | mse 11.53 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  69 | time: 339.37s | valid loss/mse 12.5560 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.555299157283725, min_delta 0.0001, val_loss 12.55598768000299
Loss error: -0.0006885227192654497
scGPT - INFO - INFO: Early stopping counter 1 of 10
epoch:  69
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.836 3.566 3.281 ... 3.453 3.645 3.879]
 [6.996 3.629 3.338 ... 3.46  3.686 3.912]
 [4.684 3.695 3.82  ... 4.094 3.713 3.549]
 ...
 [4.812 3.875 3.9   ... 4.066 3.787 3.578]
 [4.92  3.918 3.977 ... 4.12  3.865 3.615]
 [6.91  3.664 3.254 ... 3.47  3.61  3.887]]
(801, 11) (801, 11)
By feature:  MSE:  0.5708¬±0.0 MAE:  0.4928¬±0.0 R2:  0.2425¬±0.0 PCC:  0.8039¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5708¬±0.0 MAE:  0.4928¬±0.0 R2:  0.2866¬±0.0 PCC:  0.5226¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5708000063896179¬±0.0 MAE: 0.4927999973297119¬±0.0 R2: 0.2425¬±0.0 PCC: 0.8039¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5708000063896179¬±0.0 MAE: 0.4927999973297119¬±0.0 R2: 0.2866¬±0.0 PCC: 0.5226¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  70
Training model
scGPT - INFO - | epoch  70 | 100/3602 batches | lr 0.0000 | ms/batch 91.95 | loss 10.47 | mse 10.47 | mre  0.00 |
scGPT - INFO - | epoch  70 | 200/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 12.55 | mse 12.55 | mre  0.00 |
scGPT - INFO - | epoch  70 | 300/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 10.22 | mse 10.22 | mre  0.00 |
scGPT - INFO - | epoch  70 | 400/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 10.69 | mse 10.69 | mre  0.00 |
scGPT - INFO - | epoch  70 | 500/3602 batches | lr 0.0000 | ms/batch 91.23 | loss  9.45 | mse  9.45 | mre  0.00 |
scGPT - INFO - | epoch  70 | 600/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 11.15 | mse 11.15 | mre  0.00 |
scGPT - INFO - | epoch  70 | 700/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 11.25 | mse 11.25 | mre  0.00 |
scGPT - INFO - | epoch  70 | 800/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 13.97 | mse 13.97 | mre  0.00 |
scGPT - INFO - | epoch  70 | 900/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 11.05 | mse 11.05 | mre  0.00 |
scGPT - INFO - | epoch  70 | 1000/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 12.68 | mse 12.68 | mre  0.00 |
scGPT - INFO - | epoch  70 | 1100/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 10.97 | mse 10.97 | mre  0.00 |
scGPT - INFO - | epoch  70 | 1200/3602 batches | lr 0.0000 | ms/batch 90.60 | loss 10.61 | mse 10.61 | mre  0.00 |
scGPT - INFO - | epoch  70 | 1300/3602 batches | lr 0.0000 | ms/batch 90.61 | loss 10.56 | mse 10.56 | mre  0.00 |
scGPT - INFO - | epoch  70 | 1400/3602 batches | lr 0.0000 | ms/batch 90.62 | loss 12.94 | mse 12.94 | mre  0.00 |
scGPT - INFO - | epoch  70 | 1500/3602 batches | lr 0.0000 | ms/batch 91.12 | loss 13.18 | mse 13.18 | mre  0.00 |
scGPT - INFO - | epoch  70 | 1600/3602 batches | lr 0.0000 | ms/batch 90.58 | loss  9.81 | mse  9.81 | mre  0.00 |
scGPT - INFO - | epoch  70 | 1700/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 10.59 | mse 10.59 | mre  0.00 |
scGPT - INFO - | epoch  70 | 1800/3602 batches | lr 0.0000 | ms/batch 90.61 | loss 10.48 | mse 10.48 | mre  0.00 |
scGPT - INFO - | epoch  70 | 1900/3602 batches | lr 0.0000 | ms/batch 90.60 | loss 13.20 | mse 13.20 | mre  0.00 |
scGPT - INFO - | epoch  70 | 2000/3602 batches | lr 0.0000 | ms/batch 90.57 | loss 11.15 | mse 11.15 | mre  0.00 |
scGPT - INFO - | epoch  70 | 2100/3602 batches | lr 0.0000 | ms/batch 90.62 | loss 12.76 | mse 12.76 | mre  0.00 |
scGPT - INFO - | epoch  70 | 2200/3602 batches | lr 0.0000 | ms/batch 90.62 | loss 11.11 | mse 11.11 | mre  0.00 |
scGPT - INFO - | epoch  70 | 2300/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 10.97 | mse 10.97 | mre  0.00 |
scGPT - INFO - | epoch  70 | 2400/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 11.48 | mse 11.48 | mre  0.00 |
scGPT - INFO - | epoch  70 | 2500/3602 batches | lr 0.0000 | ms/batch 91.13 | loss  9.76 | mse  9.76 | mre  0.00 |
scGPT - INFO - | epoch  70 | 2600/3602 batches | lr 0.0000 | ms/batch 90.59 | loss 14.27 | mse 14.27 | mre  0.00 |
scGPT - INFO - | epoch  70 | 2700/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 10.67 | mse 10.67 | mre  0.00 |
scGPT - INFO - | epoch  70 | 2800/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 12.00 | mse 12.00 | mre  0.00 |
scGPT - INFO - | epoch  70 | 2900/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 11.53 | mse 11.53 | mre  0.00 |
scGPT - INFO - | epoch  70 | 3000/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 11.20 | mse 11.20 | mre  0.00 |
scGPT - INFO - | epoch  70 | 3100/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 10.37 | mse 10.37 | mre  0.00 |
scGPT - INFO - | epoch  70 | 3200/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 10.84 | mse 10.84 | mre  0.00 |
scGPT - INFO - | epoch  70 | 3300/3602 batches | lr 0.0000 | ms/batch 90.73 | loss  9.64 | mse  9.64 | mre  0.00 |
scGPT - INFO - | epoch  70 | 3400/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 10.93 | mse 10.93 | mre  0.00 |
scGPT - INFO - | epoch  70 | 3500/3602 batches | lr 0.0000 | ms/batch 91.33 | loss 10.34 | mse 10.34 | mre  0.00 |
scGPT - INFO - | epoch  70 | 3600/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 11.99 | mse 11.99 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  70 | time: 339.23s | valid loss/mse 12.5570 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.555299157283725, min_delta 0.0001, val_loss 12.556992705023095
Loss error: -0.001693547739369805
scGPT - INFO - INFO: Early stopping counter 2 of 10
epoch:  70
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.836 3.566 3.283 ... 3.453 3.646 3.879]
 [6.996 3.629 3.336 ... 3.459 3.686 3.912]
 [4.684 3.695 3.82  ... 4.09  3.713 3.55 ]
 ...
 [4.812 3.875 3.898 ... 4.066 3.787 3.578]
 [4.92  3.918 3.975 ... 4.125 3.865 3.615]
 [6.914 3.664 3.254 ... 3.47  3.61  3.889]]
(801, 11) (801, 11)
By feature:  MSE:  0.5708¬±0.0 MAE:  0.4927¬±0.0 R2:  0.2424¬±0.0 PCC:  0.804¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5708¬±0.0 MAE:  0.4927¬±0.0 R2:  0.2867¬±0.0 PCC:  0.5228¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5708000063896179¬±0.0 MAE: 0.4927000105381012¬±0.0 R2: 0.2424¬±0.0 PCC: 0.804¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5708000063896179¬±0.0 MAE: 0.4927000105381012¬±0.0 R2: 0.2867¬±0.0 PCC: 0.5228¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  71
Training model
scGPT - INFO - | epoch  71 | 100/3602 batches | lr 0.0000 | ms/batch 92.00 | loss 10.63 | mse 10.63 | mre  0.00 |
scGPT - INFO - | epoch  71 | 200/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 12.46 | mse 12.46 | mre  0.00 |
scGPT - INFO - | epoch  71 | 300/3602 batches | lr 0.0000 | ms/batch 90.62 | loss  9.99 | mse  9.99 | mre  0.00 |
scGPT - INFO - | epoch  71 | 400/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 10.72 | mse 10.72 | mre  0.00 |
scGPT - INFO - | epoch  71 | 500/3602 batches | lr 0.0000 | ms/batch 90.66 | loss  9.44 | mse  9.44 | mre  0.00 |
scGPT - INFO - | epoch  71 | 600/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 10.95 | mse 10.95 | mre  0.00 |
scGPT - INFO - | epoch  71 | 700/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 11.06 | mse 11.06 | mre  0.00 |
scGPT - INFO - | epoch  71 | 800/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 14.51 | mse 14.51 | mre  0.00 |
scGPT - INFO - | epoch  71 | 900/3602 batches | lr 0.0000 | ms/batch 91.21 | loss 10.92 | mse 10.92 | mre  0.00 |
scGPT - INFO - | epoch  71 | 1000/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 12.48 | mse 12.48 | mre  0.00 |
scGPT - INFO - | epoch  71 | 1100/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 10.64 | mse 10.64 | mre  0.00 |
scGPT - INFO - | epoch  71 | 1200/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 11.18 | mse 11.18 | mre  0.00 |
scGPT - INFO - | epoch  71 | 1300/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.60 | mse 10.60 | mre  0.00 |
scGPT - INFO - | epoch  71 | 1400/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 13.57 | mse 13.57 | mre  0.00 |
scGPT - INFO - | epoch  71 | 1500/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 13.89 | mse 13.89 | mre  0.00 |
scGPT - INFO - | epoch  71 | 1600/3602 batches | lr 0.0000 | ms/batch 90.82 | loss 10.07 | mse 10.07 | mre  0.00 |
scGPT - INFO - | epoch  71 | 1700/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 10.59 | mse 10.59 | mre  0.00 |
scGPT - INFO - | epoch  71 | 1800/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 10.53 | mse 10.53 | mre  0.00 |
scGPT - INFO - | epoch  71 | 1900/3602 batches | lr 0.0000 | ms/batch 91.32 | loss 13.04 | mse 13.04 | mre  0.00 |
scGPT - INFO - | epoch  71 | 2000/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 11.21 | mse 11.21 | mre  0.00 |
scGPT - INFO - | epoch  71 | 2100/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 12.54 | mse 12.54 | mre  0.00 |
scGPT - INFO - | epoch  71 | 2200/3602 batches | lr 0.0000 | ms/batch 90.61 | loss 11.02 | mse 11.02 | mre  0.00 |
scGPT - INFO - | epoch  71 | 2300/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 11.03 | mse 11.03 | mre  0.00 |
scGPT - INFO - | epoch  71 | 2400/3602 batches | lr 0.0000 | ms/batch 90.61 | loss 11.30 | mse 11.30 | mre  0.00 |
scGPT - INFO - | epoch  71 | 2500/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 10.30 | mse 10.30 | mre  0.00 |
scGPT - INFO - | epoch  71 | 2600/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 13.96 | mse 13.96 | mre  0.00 |
scGPT - INFO - | epoch  71 | 2700/3602 batches | lr 0.0000 | ms/batch 90.75 | loss 10.33 | mse 10.33 | mre  0.00 |
scGPT - INFO - | epoch  71 | 2800/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 11.79 | mse 11.79 | mre  0.00 |
scGPT - INFO - | epoch  71 | 2900/3602 batches | lr 0.0000 | ms/batch 91.28 | loss 11.49 | mse 11.49 | mre  0.00 |
scGPT - INFO - | epoch  71 | 3000/3602 batches | lr 0.0000 | ms/batch 90.70 | loss 11.37 | mse 11.37 | mre  0.00 |
scGPT - INFO - | epoch  71 | 3100/3602 batches | lr 0.0000 | ms/batch 90.66 | loss 10.24 | mse 10.24 | mre  0.00 |
scGPT - INFO - | epoch  71 | 3200/3602 batches | lr 0.0000 | ms/batch 90.67 | loss 10.80 | mse 10.80 | mre  0.00 |
scGPT - INFO - | epoch  71 | 3300/3602 batches | lr 0.0000 | ms/batch 90.71 | loss  9.19 | mse  9.19 | mre  0.00 |
scGPT - INFO - | epoch  71 | 3400/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 10.65 | mse 10.65 | mre  0.00 |
scGPT - INFO - | epoch  71 | 3500/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 10.21 | mse 10.21 | mre  0.00 |
scGPT - INFO - | epoch  71 | 3600/3602 batches | lr 0.0000 | ms/batch 90.62 | loss 11.54 | mse 11.54 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  71 | time: 339.27s | valid loss/mse 12.5551 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.5551
best_loss: 12.555299157283725, min_delta 0.0001, val_loss 12.555061570862854
Loss error: 0.0002375864208712386
epoch:  71
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.836 3.568 3.283 ... 3.453 3.646 3.879]
 [6.996 3.63  3.338 ... 3.46  3.688 3.912]
 [4.684 3.697 3.82  ... 4.09  3.713 3.55 ]
 ...
 [4.812 3.877 3.898 ... 4.066 3.787 3.578]
 [4.92  3.916 3.975 ... 4.12  3.863 3.615]
 [6.914 3.664 3.254 ... 3.469 3.61  3.889]]
(801, 11) (801, 11)
By feature:  MSE:  0.5707¬±0.0 MAE:  0.4926¬±0.0 R2:  0.2425¬±0.0 PCC:  0.8042¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5707¬±0.0 MAE:  0.4926¬±0.0 R2:  0.2869¬±0.0 PCC:  0.5228¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5706999897956848¬±0.0 MAE: 0.4925999939441681¬±0.0 R2: 0.2425¬±0.0 PCC: 0.8042¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5706999897956848¬±0.0 MAE: 0.4925999939441681¬±0.0 R2: 0.2869¬±0.0 PCC: 0.5228¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  72
Training model
scGPT - INFO - | epoch  72 | 100/3602 batches | lr 0.0000 | ms/batch 92.03 | loss 10.83 | mse 10.83 | mre  0.00 |
scGPT - INFO - | epoch  72 | 200/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 12.10 | mse 12.10 | mre  0.00 |
scGPT - INFO - | epoch  72 | 300/3602 batches | lr 0.0000 | ms/batch 91.23 | loss  9.88 | mse  9.88 | mre  0.00 |
scGPT - INFO - | epoch  72 | 400/3602 batches | lr 0.0000 | ms/batch 90.73 | loss 10.75 | mse 10.75 | mre  0.00 |
scGPT - INFO - | epoch  72 | 500/3602 batches | lr 0.0000 | ms/batch 90.64 | loss  9.16 | mse  9.16 | mre  0.00 |
scGPT - INFO - | epoch  72 | 600/3602 batches | lr 0.0000 | ms/batch 90.61 | loss 10.29 | mse 10.29 | mre  0.00 |
scGPT - INFO - | epoch  72 | 700/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 11.26 | mse 11.26 | mre  0.00 |
scGPT - INFO - | epoch  72 | 800/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 14.28 | mse 14.28 | mre  0.00 |
scGPT - INFO - | epoch  72 | 900/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 11.10 | mse 11.10 | mre  0.00 |
scGPT - INFO - | epoch  72 | 1000/3602 batches | lr 0.0000 | ms/batch 90.62 | loss 12.73 | mse 12.73 | mre  0.00 |
scGPT - INFO - | epoch  72 | 1100/3602 batches | lr 0.0000 | ms/batch 90.64 | loss 10.91 | mse 10.91 | mre  0.00 |
scGPT - INFO - | epoch  72 | 1200/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.91 | mse 10.91 | mre  0.00 |
scGPT - INFO - | epoch  72 | 1300/3602 batches | lr 0.0000 | ms/batch 91.15 | loss 10.28 | mse 10.28 | mre  0.00 |
scGPT - INFO - | epoch  72 | 1400/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 13.46 | mse 13.46 | mre  0.00 |
scGPT - INFO - | epoch  72 | 1500/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 13.83 | mse 13.83 | mre  0.00 |
scGPT - INFO - | epoch  72 | 1600/3602 batches | lr 0.0000 | ms/batch 90.57 | loss  9.90 | mse  9.90 | mre  0.00 |
scGPT - INFO - | epoch  72 | 1700/3602 batches | lr 0.0000 | ms/batch 90.98 | loss 10.73 | mse 10.73 | mre  0.00 |
scGPT - INFO - | epoch  72 | 1800/3602 batches | lr 0.0000 | ms/batch 90.54 | loss 10.59 | mse 10.59 | mre  0.00 |
scGPT - INFO - | epoch  72 | 1900/3602 batches | lr 0.0000 | ms/batch 90.61 | loss 12.77 | mse 12.77 | mre  0.00 |
scGPT - INFO - | epoch  72 | 2000/3602 batches | lr 0.0000 | ms/batch 90.55 | loss 11.18 | mse 11.18 | mre  0.00 |
scGPT - INFO - | epoch  72 | 2100/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 12.78 | mse 12.78 | mre  0.00 |
scGPT - INFO - | epoch  72 | 2200/3602 batches | lr 0.0000 | ms/batch 90.65 | loss 10.93 | mse 10.93 | mre  0.00 |
scGPT - INFO - | epoch  72 | 2300/3602 batches | lr 0.0000 | ms/batch 91.23 | loss 10.91 | mse 10.91 | mre  0.00 |
scGPT - INFO - | epoch  72 | 2400/3602 batches | lr 0.0000 | ms/batch 90.93 | loss 12.04 | mse 12.04 | mre  0.00 |
scGPT - INFO - | epoch  72 | 2500/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 10.58 | mse 10.58 | mre  0.00 |
scGPT - INFO - | epoch  72 | 2600/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 14.22 | mse 14.22 | mre  0.00 |
scGPT - INFO - | epoch  72 | 2700/3602 batches | lr 0.0000 | ms/batch 90.91 | loss 10.49 | mse 10.49 | mre  0.00 |
scGPT - INFO - | epoch  72 | 2800/3602 batches | lr 0.0000 | ms/batch 90.91 | loss 11.85 | mse 11.85 | mre  0.00 |
scGPT - INFO - | epoch  72 | 2900/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 11.88 | mse 11.88 | mre  0.00 |
scGPT - INFO - | epoch  72 | 3000/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 11.58 | mse 11.58 | mre  0.00 |
scGPT - INFO - | epoch  72 | 3100/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 10.44 | mse 10.44 | mre  0.00 |
scGPT - INFO - | epoch  72 | 3200/3602 batches | lr 0.0000 | ms/batch 90.71 | loss 10.98 | mse 10.98 | mre  0.00 |
scGPT - INFO - | epoch  72 | 3300/3602 batches | lr 0.0000 | ms/batch 91.27 | loss  9.64 | mse  9.64 | mre  0.00 |
scGPT - INFO - | epoch  72 | 3400/3602 batches | lr 0.0000 | ms/batch 90.98 | loss 10.30 | mse 10.30 | mre  0.00 |
scGPT - INFO - | epoch  72 | 3500/3602 batches | lr 0.0000 | ms/batch 90.83 | loss 10.16 | mse 10.16 | mre  0.00 |
scGPT - INFO - | epoch  72 | 3600/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 11.51 | mse 11.51 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  72 | time: 339.51s | valid loss/mse 12.5552 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.555061570862854, min_delta 0.0001, val_loss 12.555178702026419
Loss error: -0.0001171311635648209
scGPT - INFO - INFO: Early stopping counter 1 of 10
epoch:  72
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.836 3.568 3.285 ... 3.453 3.648 3.879]
 [6.996 3.63  3.338 ... 3.46  3.688 3.912]
 [4.684 3.697 3.82  ... 4.09  3.713 3.549]
 ...
 [4.812 3.877 3.9   ... 4.066 3.787 3.578]
 [4.92  3.916 3.975 ... 4.12  3.865 3.615]
 [6.914 3.664 3.256 ... 3.47  3.611 3.889]]
(801, 11) (801, 11)
By feature:  MSE:  0.5707¬±0.0 MAE:  0.4926¬±0.0 R2:  0.2426¬±0.0 PCC:  0.8041¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5707¬±0.0 MAE:  0.4926¬±0.0 R2:  0.2868¬±0.0 PCC:  0.5228¬±0.0 Cosine Similarity:  0.9854¬±0.0
scGPT - INFO - By feature: MSE: 0.5706999897956848¬±0.0 MAE: 0.4925999939441681¬±0.0 R2: 0.2426¬±0.0 PCC: 0.8041¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5706999897956848¬±0.0 MAE: 0.4925999939441681¬±0.0 R2: 0.2868¬±0.0 PCC: 0.5228¬±0.0 Cosine Similarity: 0.9854¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  73
Training model
scGPT - INFO - | epoch  73 | 100/3602 batches | lr 0.0000 | ms/batch 91.94 | loss 10.61 | mse 10.61 | mre  0.00 |
scGPT - INFO - | epoch  73 | 200/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 12.24 | mse 12.24 | mre  0.00 |
scGPT - INFO - | epoch  73 | 300/3602 batches | lr 0.0000 | ms/batch 90.77 | loss  9.95 | mse  9.95 | mre  0.00 |
scGPT - INFO - | epoch  73 | 400/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 10.65 | mse 10.65 | mre  0.00 |
scGPT - INFO - | epoch  73 | 500/3602 batches | lr 0.0000 | ms/batch 90.71 | loss  9.57 | mse  9.57 | mre  0.00 |
scGPT - INFO - | epoch  73 | 600/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 10.71 | mse 10.71 | mre  0.00 |
scGPT - INFO - | epoch  73 | 700/3602 batches | lr 0.0000 | ms/batch 91.23 | loss 11.31 | mse 11.31 | mre  0.00 |
scGPT - INFO - | epoch  73 | 800/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 14.70 | mse 14.70 | mre  0.00 |
scGPT - INFO - | epoch  73 | 900/3602 batches | lr 0.0000 | ms/batch 90.78 | loss 11.00 | mse 11.00 | mre  0.00 |
scGPT - INFO - | epoch  73 | 1000/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 12.85 | mse 12.85 | mre  0.00 |
scGPT - INFO - | epoch  73 | 1100/3602 batches | lr 0.0000 | ms/batch 90.76 | loss 10.39 | mse 10.39 | mre  0.00 |
scGPT - INFO - | epoch  73 | 1200/3602 batches | lr 0.0000 | ms/batch 90.85 | loss 11.03 | mse 11.03 | mre  0.00 |
scGPT - INFO - | epoch  73 | 1300/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 10.41 | mse 10.41 | mre  0.00 |
scGPT - INFO - | epoch  73 | 1400/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 13.14 | mse 13.14 | mre  0.00 |
scGPT - INFO - | epoch  73 | 1500/3602 batches | lr 0.0000 | ms/batch 90.72 | loss 13.39 | mse 13.39 | mre  0.00 |
scGPT - INFO - | epoch  73 | 1600/3602 batches | lr 0.0000 | ms/batch 90.74 | loss  9.81 | mse  9.81 | mre  0.00 |
scGPT - INFO - | epoch  73 | 1700/3602 batches | lr 0.0000 | ms/batch 91.30 | loss 10.27 | mse 10.27 | mre  0.00 |
scGPT - INFO - | epoch  73 | 1800/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 10.50 | mse 10.50 | mre  0.00 |
scGPT - INFO - | epoch  73 | 1900/3602 batches | lr 0.0000 | ms/batch 90.69 | loss 13.10 | mse 13.10 | mre  0.00 |
scGPT - INFO - | epoch  73 | 2000/3602 batches | lr 0.0000 | ms/batch 90.68 | loss 11.48 | mse 11.48 | mre  0.00 |
scGPT - INFO - | epoch  73 | 2100/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 12.97 | mse 12.97 | mre  0.00 |
scGPT - INFO - | epoch  73 | 2200/3602 batches | lr 0.0000 | ms/batch 90.95 | loss 10.91 | mse 10.91 | mre  0.00 |
scGPT - INFO - | epoch  73 | 2300/3602 batches | lr 0.0000 | ms/batch 91.26 | loss 10.79 | mse 10.79 | mre  0.00 |
scGPT - INFO - | epoch  73 | 2400/3602 batches | lr 0.0000 | ms/batch 91.48 | loss 12.00 | mse 12.00 | mre  0.00 |
scGPT - INFO - | epoch  73 | 2500/3602 batches | lr 0.0000 | ms/batch 91.29 | loss  9.86 | mse  9.86 | mre  0.00 |
scGPT - INFO - | epoch  73 | 2600/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 14.19 | mse 14.19 | mre  0.00 |
scGPT - INFO - | epoch  73 | 2700/3602 batches | lr 0.0000 | ms/batch 91.40 | loss 10.89 | mse 10.89 | mre  0.00 |
scGPT - INFO - | epoch  73 | 2800/3602 batches | lr 0.0000 | ms/batch 91.03 | loss 11.97 | mse 11.97 | mre  0.00 |
scGPT - INFO - | epoch  73 | 2900/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 11.70 | mse 11.70 | mre  0.00 |
scGPT - INFO - | epoch  73 | 3000/3602 batches | lr 0.0000 | ms/batch 91.29 | loss 11.64 | mse 11.64 | mre  0.00 |
scGPT - INFO - | epoch  73 | 3100/3602 batches | lr 0.0000 | ms/batch 90.84 | loss 10.75 | mse 10.75 | mre  0.00 |
scGPT - INFO - | epoch  73 | 3200/3602 batches | lr 0.0000 | ms/batch 90.79 | loss 10.99 | mse 10.99 | mre  0.00 |
scGPT - INFO - | epoch  73 | 3300/3602 batches | lr 0.0000 | ms/batch 90.75 | loss  9.50 | mse  9.50 | mre  0.00 |
scGPT - INFO - | epoch  73 | 3400/3602 batches | lr 0.0000 | ms/batch 90.74 | loss 10.42 | mse 10.42 | mre  0.00 |
scGPT - INFO - | epoch  73 | 3500/3602 batches | lr 0.0000 | ms/batch 94.70 | loss 10.36 | mse 10.36 | mre  0.00 |
scGPT - INFO - | epoch  73 | 3600/3602 batches | lr 0.0000 | ms/batch 91.11 | loss 11.66 | mse 11.66 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  73 | time: 340.49s | valid loss/mse 12.5533 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.5533
best_loss: 12.555061570862854, min_delta 0.0001, val_loss 12.553310108839646
Loss error: 0.0017514620232077505
epoch:  73
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.836 3.568 3.283 ... 3.453 3.646 3.879]
 [6.996 3.633 3.338 ... 3.46  3.688 3.914]
 [4.684 3.697 3.82  ... 4.09  3.713 3.55 ]
 ...
 [4.812 3.877 3.9   ... 4.066 3.787 3.578]
 [4.92  3.918 3.975 ... 4.12  3.865 3.615]
 [6.914 3.666 3.256 ... 3.47  3.611 3.889]]
(801, 11) (801, 11)
By feature:  MSE:  0.5706¬±0.0 MAE:  0.4926¬±0.0 R2:  0.2428¬±0.0 PCC:  0.8041¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5706¬±0.0 MAE:  0.4926¬±0.0 R2:  0.2869¬±0.0 PCC:  0.5228¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5705999732017517¬±0.0 MAE: 0.4925999939441681¬±0.0 R2: 0.2428¬±0.0 PCC: 0.8041¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5705999732017517¬±0.0 MAE: 0.4925999939441681¬±0.0 R2: 0.2869¬±0.0 PCC: 0.5228¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  74
Training model
scGPT - INFO - | epoch  74 | 100/3602 batches | lr 0.0000 | ms/batch 92.86 | loss 10.74 | mse 10.74 | mre  0.00 |
scGPT - INFO - | epoch  74 | 200/3602 batches | lr 0.0000 | ms/batch 91.09 | loss 12.35 | mse 12.35 | mre  0.00 |
scGPT - INFO - | epoch  74 | 300/3602 batches | lr 0.0000 | ms/batch 91.28 | loss  9.55 | mse  9.55 | mre  0.00 |
scGPT - INFO - | epoch  74 | 400/3602 batches | lr 0.0000 | ms/batch 91.20 | loss 10.70 | mse 10.70 | mre  0.00 |
scGPT - INFO - | epoch  74 | 500/3602 batches | lr 0.0000 | ms/batch 91.12 | loss  8.98 | mse  8.98 | mre  0.00 |
scGPT - INFO - | epoch  74 | 600/3602 batches | lr 0.0000 | ms/batch 91.13 | loss 10.82 | mse 10.82 | mre  0.00 |
scGPT - INFO - | epoch  74 | 700/3602 batches | lr 0.0000 | ms/batch 91.05 | loss 11.45 | mse 11.45 | mre  0.00 |
scGPT - INFO - | epoch  74 | 800/3602 batches | lr 0.0000 | ms/batch 91.12 | loss 13.86 | mse 13.86 | mre  0.00 |
scGPT - INFO - | epoch  74 | 900/3602 batches | lr 0.0000 | ms/batch 91.03 | loss 11.09 | mse 11.09 | mre  0.00 |
scGPT - INFO - | epoch  74 | 1000/3602 batches | lr 0.0000 | ms/batch 91.04 | loss 12.25 | mse 12.25 | mre  0.00 |
scGPT - INFO - | epoch  74 | 1100/3602 batches | lr 0.0000 | ms/batch 91.60 | loss 10.80 | mse 10.80 | mre  0.00 |
scGPT - INFO - | epoch  74 | 1200/3602 batches | lr 0.0000 | ms/batch 91.05 | loss 11.56 | mse 11.56 | mre  0.00 |
scGPT - INFO - | epoch  74 | 1300/3602 batches | lr 0.0000 | ms/batch 91.36 | loss 10.48 | mse 10.48 | mre  0.00 |
scGPT - INFO - | epoch  74 | 1400/3602 batches | lr 0.0000 | ms/batch 91.14 | loss 13.08 | mse 13.08 | mre  0.00 |
scGPT - INFO - | epoch  74 | 1500/3602 batches | lr 0.0000 | ms/batch 91.01 | loss 13.50 | mse 13.50 | mre  0.00 |
scGPT - INFO - | epoch  74 | 1600/3602 batches | lr 0.0000 | ms/batch 91.04 | loss  9.81 | mse  9.81 | mre  0.00 |
scGPT - INFO - | epoch  74 | 1700/3602 batches | lr 0.0000 | ms/batch 91.13 | loss 10.64 | mse 10.64 | mre  0.00 |
scGPT - INFO - | epoch  74 | 1800/3602 batches | lr 0.0000 | ms/batch 91.01 | loss 10.52 | mse 10.52 | mre  0.00 |
scGPT - INFO - | epoch  74 | 1900/3602 batches | lr 0.0000 | ms/batch 91.25 | loss 12.97 | mse 12.97 | mre  0.00 |
scGPT - INFO - | epoch  74 | 2000/3602 batches | lr 0.0000 | ms/batch 91.11 | loss 11.23 | mse 11.23 | mre  0.00 |
scGPT - INFO - | epoch  74 | 2100/3602 batches | lr 0.0000 | ms/batch 91.58 | loss 12.72 | mse 12.72 | mre  0.00 |
scGPT - INFO - | epoch  74 | 2200/3602 batches | lr 0.0000 | ms/batch 91.11 | loss 11.29 | mse 11.29 | mre  0.00 |
scGPT - INFO - | epoch  74 | 2300/3602 batches | lr 0.0000 | ms/batch 91.11 | loss 10.97 | mse 10.97 | mre  0.00 |
scGPT - INFO - | epoch  74 | 2400/3602 batches | lr 0.0000 | ms/batch 91.21 | loss 11.71 | mse 11.71 | mre  0.00 |
scGPT - INFO - | epoch  74 | 2500/3602 batches | lr 0.0000 | ms/batch 91.41 | loss 10.24 | mse 10.24 | mre  0.00 |
scGPT - INFO - | epoch  74 | 2600/3602 batches | lr 0.0000 | ms/batch 91.07 | loss 14.17 | mse 14.17 | mre  0.00 |
scGPT - INFO - | epoch  74 | 2700/3602 batches | lr 0.0000 | ms/batch 91.12 | loss 10.35 | mse 10.35 | mre  0.00 |
scGPT - INFO - | epoch  74 | 2800/3602 batches | lr 0.0000 | ms/batch 91.19 | loss 11.84 | mse 11.84 | mre  0.00 |
scGPT - INFO - | epoch  74 | 2900/3602 batches | lr 0.0000 | ms/batch 91.17 | loss 11.40 | mse 11.40 | mre  0.00 |
scGPT - INFO - | epoch  74 | 3000/3602 batches | lr 0.0000 | ms/batch 91.13 | loss 11.28 | mse 11.28 | mre  0.00 |
scGPT - INFO - | epoch  74 | 3100/3602 batches | lr 0.0000 | ms/batch 91.53 | loss 10.47 | mse 10.47 | mre  0.00 |
scGPT - INFO - | epoch  74 | 3200/3602 batches | lr 0.0000 | ms/batch 91.06 | loss 10.97 | mse 10.97 | mre  0.00 |
scGPT - INFO - | epoch  74 | 3300/3602 batches | lr 0.0000 | ms/batch 91.07 | loss  9.49 | mse  9.49 | mre  0.00 |
scGPT - INFO - | epoch  74 | 3400/3602 batches | lr 0.0000 | ms/batch 91.21 | loss 10.50 | mse 10.50 | mre  0.00 |
scGPT - INFO - | epoch  74 | 3500/3602 batches | lr 0.0000 | ms/batch 91.03 | loss 10.26 | mse 10.26 | mre  0.00 |
scGPT - INFO - | epoch  74 | 3600/3602 batches | lr 0.0000 | ms/batch 91.21 | loss 11.76 | mse 11.76 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  74 | time: 340.88s | valid loss/mse 12.5537 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.553310108839646, min_delta 0.0001, val_loss 12.553683839487226
Loss error: -0.0003737306475795066
scGPT - INFO - INFO: Early stopping counter 1 of 10
epoch:  74
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.836 3.568 3.285 ... 3.453 3.648 3.879]
 [6.996 3.63  3.34  ... 3.46  3.688 3.912]
 [4.684 3.697 3.822 ... 4.09  3.713 3.549]
 ...
 [4.812 3.879 3.9   ... 4.066 3.79  3.578]
 [4.92  3.918 3.977 ... 4.12  3.865 3.617]
 [6.914 3.666 3.256 ... 3.47  3.611 3.889]]
(801, 11) (801, 11)
By feature:  MSE:  0.5707¬±0.0 MAE:  0.4926¬±0.0 R2:  0.2428¬±0.0 PCC:  0.8042¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5707¬±0.0 MAE:  0.4926¬±0.0 R2:  0.2869¬±0.0 PCC:  0.5228¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5706999897956848¬±0.0 MAE: 0.4925999939441681¬±0.0 R2: 0.2428¬±0.0 PCC: 0.8042¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5706999897956848¬±0.0 MAE: 0.4925999939441681¬±0.0 R2: 0.2869¬±0.0 PCC: 0.5228¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  75
Training model
scGPT - INFO - | epoch  75 | 100/3602 batches | lr 0.0000 | ms/batch 92.19 | loss 10.94 | mse 10.94 | mre  0.00 |
scGPT - INFO - | epoch  75 | 200/3602 batches | lr 0.0000 | ms/batch 91.00 | loss 12.28 | mse 12.28 | mre  0.00 |
scGPT - INFO - | epoch  75 | 300/3602 batches | lr 0.0000 | ms/batch 90.98 | loss  9.94 | mse  9.94 | mre  0.00 |
scGPT - INFO - | epoch  75 | 400/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 11.00 | mse 11.00 | mre  0.00 |
scGPT - INFO - | epoch  75 | 500/3602 batches | lr 0.0000 | ms/batch 91.40 | loss  9.60 | mse  9.60 | mre  0.00 |
scGPT - INFO - | epoch  75 | 600/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 10.48 | mse 10.48 | mre  0.00 |
scGPT - INFO - | epoch  75 | 700/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 11.37 | mse 11.37 | mre  0.00 |
scGPT - INFO - | epoch  75 | 800/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 13.69 | mse 13.69 | mre  0.00 |
scGPT - INFO - | epoch  75 | 900/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 10.92 | mse 10.92 | mre  0.00 |
scGPT - INFO - | epoch  75 | 1000/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 12.57 | mse 12.57 | mre  0.00 |
scGPT - INFO - | epoch  75 | 1100/3602 batches | lr 0.0000 | ms/batch 90.91 | loss 10.37 | mse 10.37 | mre  0.00 |
scGPT - INFO - | epoch  75 | 1200/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 11.09 | mse 11.09 | mre  0.00 |
scGPT - INFO - | epoch  75 | 1300/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 10.13 | mse 10.13 | mre  0.00 |
scGPT - INFO - | epoch  75 | 1400/3602 batches | lr 0.0000 | ms/batch 90.97 | loss 13.30 | mse 13.30 | mre  0.00 |
scGPT - INFO - | epoch  75 | 1500/3602 batches | lr 0.0000 | ms/batch 91.52 | loss 13.55 | mse 13.55 | mre  0.00 |
scGPT - INFO - | epoch  75 | 1600/3602 batches | lr 0.0000 | ms/batch 90.96 | loss  9.90 | mse  9.90 | mre  0.00 |
scGPT - INFO - | epoch  75 | 1700/3602 batches | lr 0.0000 | ms/batch 91.02 | loss 10.64 | mse 10.64 | mre  0.00 |
scGPT - INFO - | epoch  75 | 1800/3602 batches | lr 0.0000 | ms/batch 91.03 | loss 10.64 | mse 10.64 | mre  0.00 |
scGPT - INFO - | epoch  75 | 1900/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 13.17 | mse 13.17 | mre  0.00 |
scGPT - INFO - | epoch  75 | 2000/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 11.30 | mse 11.30 | mre  0.00 |
scGPT - INFO - | epoch  75 | 2100/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 12.72 | mse 12.72 | mre  0.00 |
scGPT - INFO - | epoch  75 | 2200/3602 batches | lr 0.0000 | ms/batch 90.77 | loss 11.32 | mse 11.32 | mre  0.00 |
scGPT - INFO - | epoch  75 | 2300/3602 batches | lr 0.0000 | ms/batch 90.80 | loss 10.79 | mse 10.79 | mre  0.00 |
scGPT - INFO - | epoch  75 | 2400/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 11.88 | mse 11.88 | mre  0.00 |
scGPT - INFO - | epoch  75 | 2500/3602 batches | lr 0.0000 | ms/batch 91.55 | loss 10.11 | mse 10.11 | mre  0.00 |
scGPT - INFO - | epoch  75 | 2600/3602 batches | lr 0.0000 | ms/batch 90.95 | loss 14.59 | mse 14.59 | mre  0.00 |
scGPT - INFO - | epoch  75 | 2700/3602 batches | lr 0.0000 | ms/batch 90.93 | loss 10.61 | mse 10.61 | mre  0.00 |
scGPT - INFO - | epoch  75 | 2800/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 11.70 | mse 11.70 | mre  0.00 |
scGPT - INFO - | epoch  75 | 2900/3602 batches | lr 0.0000 | ms/batch 90.95 | loss 11.89 | mse 11.89 | mre  0.00 |
scGPT - INFO - | epoch  75 | 3000/3602 batches | lr 0.0000 | ms/batch 90.95 | loss 11.27 | mse 11.27 | mre  0.00 |
scGPT - INFO - | epoch  75 | 3100/3602 batches | lr 0.0000 | ms/batch 91.02 | loss 10.70 | mse 10.70 | mre  0.00 |
scGPT - INFO - | epoch  75 | 3200/3602 batches | lr 0.0000 | ms/batch 90.99 | loss 11.09 | mse 11.09 | mre  0.00 |
scGPT - INFO - | epoch  75 | 3300/3602 batches | lr 0.0000 | ms/batch 91.05 | loss  9.81 | mse  9.81 | mre  0.00 |
scGPT - INFO - | epoch  75 | 3400/3602 batches | lr 0.0000 | ms/batch 91.00 | loss 10.76 | mse 10.76 | mre  0.00 |
scGPT - INFO - | epoch  75 | 3500/3602 batches | lr 0.0000 | ms/batch 91.53 | loss 10.22 | mse 10.22 | mre  0.00 |
scGPT - INFO - | epoch  75 | 3600/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 11.91 | mse 11.91 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  75 | time: 340.21s | valid loss/mse 12.5529 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 12.5529
best_loss: 12.553310108839646, min_delta 0.0001, val_loss 12.552931918187088
Loss error: 0.00037819065255817463
epoch:  75
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.836 3.568 3.285 ... 3.453 3.648 3.879]
 [6.996 3.633 3.34  ... 3.46  3.688 3.912]
 [4.684 3.7   3.822 ... 4.094 3.715 3.55 ]
 ...
 [4.812 3.879 3.9   ... 4.066 3.79  3.578]
 [4.92  3.918 3.977 ... 4.12  3.865 3.615]
 [6.914 3.666 3.256 ... 3.47  3.611 3.887]]
(801, 11) (801, 11)
By feature:  MSE:  0.5706¬±0.0 MAE:  0.4926¬±0.0 R2:  0.2429¬±0.0 PCC:  0.8042¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5706¬±0.0 MAE:  0.4926¬±0.0 R2:  0.2869¬±0.0 PCC:  0.5228¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5705999732017517¬±0.0 MAE: 0.4925999939441681¬±0.0 R2: 0.2429¬±0.0 PCC: 0.8042¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5705999732017517¬±0.0 MAE: 0.4925999939441681¬±0.0 R2: 0.2869¬±0.0 PCC: 0.5228¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  76
Training model
scGPT - INFO - | epoch  76 | 100/3602 batches | lr 0.0000 | ms/batch 92.71 | loss 10.70 | mse 10.70 | mre  0.00 |
scGPT - INFO - | epoch  76 | 200/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 12.42 | mse 12.42 | mre  0.00 |
scGPT - INFO - | epoch  76 | 300/3602 batches | lr 0.0000 | ms/batch 90.92 | loss  9.92 | mse  9.92 | mre  0.00 |
scGPT - INFO - | epoch  76 | 400/3602 batches | lr 0.0000 | ms/batch 91.11 | loss 10.48 | mse 10.48 | mre  0.00 |
scGPT - INFO - | epoch  76 | 500/3602 batches | lr 0.0000 | ms/batch 90.91 | loss  9.25 | mse  9.25 | mre  0.00 |
scGPT - INFO - | epoch  76 | 600/3602 batches | lr 0.0000 | ms/batch 90.87 | loss 10.79 | mse 10.79 | mre  0.00 |
scGPT - INFO - | epoch  76 | 700/3602 batches | lr 0.0000 | ms/batch 91.12 | loss 11.17 | mse 11.17 | mre  0.00 |
scGPT - INFO - | epoch  76 | 800/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 14.18 | mse 14.18 | mre  0.00 |
scGPT - INFO - | epoch  76 | 900/3602 batches | lr 0.0000 | ms/batch 91.65 | loss 11.07 | mse 11.07 | mre  0.00 |
scGPT - INFO - | epoch  76 | 1000/3602 batches | lr 0.0000 | ms/batch 91.42 | loss 12.58 | mse 12.58 | mre  0.00 |
scGPT - INFO - | epoch  76 | 1100/3602 batches | lr 0.0000 | ms/batch 91.36 | loss 10.63 | mse 10.63 | mre  0.00 |
scGPT - INFO - | epoch  76 | 1200/3602 batches | lr 0.0000 | ms/batch 91.35 | loss 11.01 | mse 11.01 | mre  0.00 |
scGPT - INFO - | epoch  76 | 1300/3602 batches | lr 0.0000 | ms/batch 91.23 | loss 10.40 | mse 10.40 | mre  0.00 |
scGPT - INFO - | epoch  76 | 1400/3602 batches | lr 0.0000 | ms/batch 91.20 | loss 13.13 | mse 13.13 | mre  0.00 |
scGPT - INFO - | epoch  76 | 1500/3602 batches | lr 0.0000 | ms/batch 91.33 | loss 13.62 | mse 13.62 | mre  0.00 |
scGPT - INFO - | epoch  76 | 1600/3602 batches | lr 0.0000 | ms/batch 90.92 | loss  9.62 | mse  9.62 | mre  0.00 |
scGPT - INFO - | epoch  76 | 1700/3602 batches | lr 0.0000 | ms/batch 90.95 | loss 10.67 | mse 10.67 | mre  0.00 |
scGPT - INFO - | epoch  76 | 1800/3602 batches | lr 0.0000 | ms/batch 91.40 | loss 10.77 | mse 10.77 | mre  0.00 |
scGPT - INFO - | epoch  76 | 1900/3602 batches | lr 0.0000 | ms/batch 91.98 | loss 13.03 | mse 13.03 | mre  0.00 |
scGPT - INFO - | epoch  76 | 2000/3602 batches | lr 0.0000 | ms/batch 91.69 | loss 11.02 | mse 11.02 | mre  0.00 |
scGPT - INFO - | epoch  76 | 2100/3602 batches | lr 0.0000 | ms/batch 91.84 | loss 12.73 | mse 12.73 | mre  0.00 |
scGPT - INFO - | epoch  76 | 2200/3602 batches | lr 0.0000 | ms/batch 91.69 | loss 10.86 | mse 10.86 | mre  0.00 |
scGPT - INFO - | epoch  76 | 2300/3602 batches | lr 0.0000 | ms/batch 91.60 | loss 10.79 | mse 10.79 | mre  0.00 |
scGPT - INFO - | epoch  76 | 2400/3602 batches | lr 0.0000 | ms/batch 91.76 | loss 11.91 | mse 11.91 | mre  0.00 |
scGPT - INFO - | epoch  76 | 2500/3602 batches | lr 0.0000 | ms/batch 91.70 | loss 10.00 | mse 10.00 | mre  0.00 |
scGPT - INFO - | epoch  76 | 2600/3602 batches | lr 0.0000 | ms/batch 91.64 | loss 13.63 | mse 13.63 | mre  0.00 |
scGPT - INFO - | epoch  76 | 2700/3602 batches | lr 0.0000 | ms/batch 91.77 | loss 10.16 | mse 10.16 | mre  0.00 |
scGPT - INFO - | epoch  76 | 2800/3602 batches | lr 0.0000 | ms/batch 91.66 | loss 11.88 | mse 11.88 | mre  0.00 |
scGPT - INFO - | epoch  76 | 2900/3602 batches | lr 0.0000 | ms/batch 92.43 | loss 12.16 | mse 12.16 | mre  0.00 |
scGPT - INFO - | epoch  76 | 3000/3602 batches | lr 0.0000 | ms/batch 91.43 | loss 11.25 | mse 11.25 | mre  0.00 |
scGPT - INFO - | epoch  76 | 3100/3602 batches | lr 0.0000 | ms/batch 91.63 | loss 10.24 | mse 10.24 | mre  0.00 |
scGPT - INFO - | epoch  76 | 3200/3602 batches | lr 0.0000 | ms/batch 91.72 | loss 11.10 | mse 11.10 | mre  0.00 |
scGPT - INFO - | epoch  76 | 3300/3602 batches | lr 0.0000 | ms/batch 91.74 | loss  9.32 | mse  9.32 | mre  0.00 |
scGPT - INFO - | epoch  76 | 3400/3602 batches | lr 0.0000 | ms/batch 91.61 | loss 10.13 | mse 10.13 | mre  0.00 |
scGPT - INFO - | epoch  76 | 3500/3602 batches | lr 0.0000 | ms/batch 91.73 | loss 10.50 | mse 10.50 | mre  0.00 |
scGPT - INFO - | epoch  76 | 3600/3602 batches | lr 0.0000 | ms/batch 91.57 | loss 11.50 | mse 11.50 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  76 | time: 341.96s | valid loss/mse 12.5534 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.552931918187088, min_delta 0.0001, val_loss 12.553426270627797
Loss error: -0.0004943524407092781
scGPT - INFO - INFO: Early stopping counter 1 of 10
epoch:  76
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.836 3.568 3.285 ... 3.453 3.648 3.879]
 [6.996 3.633 3.34  ... 3.46  3.69  3.912]
 [4.684 3.7   3.822 ... 4.09  3.713 3.55 ]
 ...
 [4.812 3.879 3.9   ... 4.066 3.79  3.578]
 [4.92  3.92  3.977 ... 4.12  3.865 3.617]
 [6.914 3.666 3.256 ... 3.47  3.611 3.889]]
(801, 11) (801, 11)
By feature:  MSE:  0.5707¬±0.0 MAE:  0.4927¬±0.0 R2:  0.2428¬±0.0 PCC:  0.8042¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5707¬±0.0 MAE:  0.4927¬±0.0 R2:  0.2869¬±0.0 PCC:  0.5228¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5706999897956848¬±0.0 MAE: 0.4927000105381012¬±0.0 R2: 0.2428¬±0.0 PCC: 0.8042¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5706999897956848¬±0.0 MAE: 0.4927000105381012¬±0.0 R2: 0.2869¬±0.0 PCC: 0.5228¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  77
Training model
scGPT - INFO - | epoch  77 | 100/3602 batches | lr 0.0000 | ms/batch 92.78 | loss 10.65 | mse 10.65 | mre  0.00 |
scGPT - INFO - | epoch  77 | 200/3602 batches | lr 0.0000 | ms/batch 91.44 | loss 12.21 | mse 12.21 | mre  0.00 |
scGPT - INFO - | epoch  77 | 300/3602 batches | lr 0.0000 | ms/batch 92.04 | loss  9.77 | mse  9.77 | mre  0.00 |
scGPT - INFO - | epoch  77 | 400/3602 batches | lr 0.0000 | ms/batch 91.62 | loss 10.53 | mse 10.53 | mre  0.00 |
scGPT - INFO - | epoch  77 | 500/3602 batches | lr 0.0000 | ms/batch 91.57 | loss  9.46 | mse  9.46 | mre  0.00 |
scGPT - INFO - | epoch  77 | 600/3602 batches | lr 0.0000 | ms/batch 91.46 | loss 10.94 | mse 10.94 | mre  0.00 |
scGPT - INFO - | epoch  77 | 700/3602 batches | lr 0.0000 | ms/batch 91.76 | loss 11.51 | mse 11.51 | mre  0.00 |
scGPT - INFO - | epoch  77 | 800/3602 batches | lr 0.0000 | ms/batch 91.41 | loss 13.76 | mse 13.76 | mre  0.00 |
scGPT - INFO - | epoch  77 | 900/3602 batches | lr 0.0000 | ms/batch 91.43 | loss 11.12 | mse 11.12 | mre  0.00 |
scGPT - INFO - | epoch  77 | 1000/3602 batches | lr 0.0000 | ms/batch 91.42 | loss 12.44 | mse 12.44 | mre  0.00 |
scGPT - INFO - | epoch  77 | 1100/3602 batches | lr 0.0000 | ms/batch 91.45 | loss 10.53 | mse 10.53 | mre  0.00 |
scGPT - INFO - | epoch  77 | 1200/3602 batches | lr 0.0000 | ms/batch 91.56 | loss 10.62 | mse 10.62 | mre  0.00 |
scGPT - INFO - | epoch  77 | 1300/3602 batches | lr 0.0000 | ms/batch 91.95 | loss 10.63 | mse 10.63 | mre  0.00 |
scGPT - INFO - | epoch  77 | 1400/3602 batches | lr 0.0000 | ms/batch 91.47 | loss 13.42 | mse 13.42 | mre  0.00 |
scGPT - INFO - | epoch  77 | 1500/3602 batches | lr 0.0000 | ms/batch 91.41 | loss 13.59 | mse 13.59 | mre  0.00 |
scGPT - INFO - | epoch  77 | 1600/3602 batches | lr 0.0000 | ms/batch 91.46 | loss  9.72 | mse  9.72 | mre  0.00 |
scGPT - INFO - | epoch  77 | 1700/3602 batches | lr 0.0000 | ms/batch 91.41 | loss 10.44 | mse 10.44 | mre  0.00 |
scGPT - INFO - | epoch  77 | 1800/3602 batches | lr 0.0000 | ms/batch 91.49 | loss 10.53 | mse 10.53 | mre  0.00 |
scGPT - INFO - | epoch  77 | 1900/3602 batches | lr 0.0000 | ms/batch 91.37 | loss 12.91 | mse 12.91 | mre  0.00 |
scGPT - INFO - | epoch  77 | 2000/3602 batches | lr 0.0000 | ms/batch 91.54 | loss 11.03 | mse 11.03 | mre  0.00 |
scGPT - INFO - | epoch  77 | 2100/3602 batches | lr 0.0000 | ms/batch 91.40 | loss 12.70 | mse 12.70 | mre  0.00 |
scGPT - INFO - | epoch  77 | 2200/3602 batches | lr 0.0000 | ms/batch 91.56 | loss 10.91 | mse 10.91 | mre  0.00 |
scGPT - INFO - | epoch  77 | 2300/3602 batches | lr 0.0000 | ms/batch 92.08 | loss 10.89 | mse 10.89 | mre  0.00 |
scGPT - INFO - | epoch  77 | 2400/3602 batches | lr 0.0000 | ms/batch 91.59 | loss 11.53 | mse 11.53 | mre  0.00 |
scGPT - INFO - | epoch  77 | 2500/3602 batches | lr 0.0000 | ms/batch 91.58 | loss  9.80 | mse  9.80 | mre  0.00 |
scGPT - INFO - | epoch  77 | 2600/3602 batches | lr 0.0000 | ms/batch 91.57 | loss 14.74 | mse 14.74 | mre  0.00 |
scGPT - INFO - | epoch  77 | 2700/3602 batches | lr 0.0000 | ms/batch 91.57 | loss 10.54 | mse 10.54 | mre  0.00 |
scGPT - INFO - | epoch  77 | 2800/3602 batches | lr 0.0000 | ms/batch 91.63 | loss 11.73 | mse 11.73 | mre  0.00 |
scGPT - INFO - | epoch  77 | 2900/3602 batches | lr 0.0000 | ms/batch 91.66 | loss 11.48 | mse 11.48 | mre  0.00 |
scGPT - INFO - | epoch  77 | 3000/3602 batches | lr 0.0000 | ms/batch 91.51 | loss 11.21 | mse 11.21 | mre  0.00 |
scGPT - INFO - | epoch  77 | 3100/3602 batches | lr 0.0000 | ms/batch 91.85 | loss  9.89 | mse  9.89 | mre  0.00 |
scGPT - INFO - | epoch  77 | 3200/3602 batches | lr 0.0000 | ms/batch 91.68 | loss 10.82 | mse 10.82 | mre  0.00 |
scGPT - INFO - | epoch  77 | 3300/3602 batches | lr 0.0000 | ms/batch 91.79 | loss  9.76 | mse  9.76 | mre  0.00 |
scGPT - INFO - | epoch  77 | 3400/3602 batches | lr 0.0000 | ms/batch 91.37 | loss 10.82 | mse 10.82 | mre  0.00 |
scGPT - INFO - | epoch  77 | 3500/3602 batches | lr 0.0000 | ms/batch 91.60 | loss 10.55 | mse 10.55 | mre  0.00 |
scGPT - INFO - | epoch  77 | 3600/3602 batches | lr 0.0000 | ms/batch 91.33 | loss 11.47 | mse 11.47 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  77 | time: 342.32s | valid loss/mse 12.5533 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.552931918187088, min_delta 0.0001, val_loss 12.55327172716905
Loss error: -0.00033980898196261933
scGPT - INFO - INFO: Early stopping counter 2 of 10
epoch:  77
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.836 3.568 3.285 ... 3.453 3.648 3.879]
 [6.996 3.633 3.34  ... 3.46  3.69  3.912]
 [4.684 3.7   3.822 ... 4.094 3.713 3.549]
 ...
 [4.812 3.879 3.9   ... 4.066 3.79  3.578]
 [4.92  3.92  3.977 ... 4.12  3.865 3.617]
 [6.914 3.666 3.256 ... 3.47  3.611 3.889]]
(801, 11) (801, 11)
By feature:  MSE:  0.5706¬±0.0 MAE:  0.4926¬±0.0 R2:  0.2428¬±0.0 PCC:  0.8042¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5706¬±0.0 MAE:  0.4926¬±0.0 R2:  0.2869¬±0.0 PCC:  0.5228¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5705999732017517¬±0.0 MAE: 0.4925999939441681¬±0.0 R2: 0.2428¬±0.0 PCC: 0.8042¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5705999732017517¬±0.0 MAE: 0.4925999939441681¬±0.0 R2: 0.2869¬±0.0 PCC: 0.5228¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  78
Training model
scGPT - INFO - | epoch  78 | 100/3602 batches | lr 0.0000 | ms/batch 92.47 | loss 10.61 | mse 10.61 | mre  0.00 |
scGPT - INFO - | epoch  78 | 200/3602 batches | lr 0.0000 | ms/batch 91.16 | loss 12.03 | mse 12.03 | mre  0.00 |
scGPT - INFO - | epoch  78 | 300/3602 batches | lr 0.0000 | ms/batch 91.09 | loss  9.76 | mse  9.76 | mre  0.00 |
scGPT - INFO - | epoch  78 | 400/3602 batches | lr 0.0000 | ms/batch 91.17 | loss 10.93 | mse 10.93 | mre  0.00 |
scGPT - INFO - | epoch  78 | 500/3602 batches | lr 0.0000 | ms/batch 91.12 | loss  9.41 | mse  9.41 | mre  0.00 |
scGPT - INFO - | epoch  78 | 600/3602 batches | lr 0.0000 | ms/batch 91.17 | loss 11.00 | mse 11.00 | mre  0.00 |
scGPT - INFO - | epoch  78 | 700/3602 batches | lr 0.0000 | ms/batch 91.81 | loss 11.33 | mse 11.33 | mre  0.00 |
scGPT - INFO - | epoch  78 | 800/3602 batches | lr 0.0000 | ms/batch 91.56 | loss 13.90 | mse 13.90 | mre  0.00 |
scGPT - INFO - | epoch  78 | 900/3602 batches | lr 0.0000 | ms/batch 91.52 | loss 11.00 | mse 11.00 | mre  0.00 |
scGPT - INFO - | epoch  78 | 1000/3602 batches | lr 0.0000 | ms/batch 91.59 | loss 12.67 | mse 12.67 | mre  0.00 |
scGPT - INFO - | epoch  78 | 1100/3602 batches | lr 0.0000 | ms/batch 91.49 | loss 10.49 | mse 10.49 | mre  0.00 |
scGPT - INFO - | epoch  78 | 1200/3602 batches | lr 0.0000 | ms/batch 91.61 | loss 11.00 | mse 11.00 | mre  0.00 |
scGPT - INFO - | epoch  78 | 1300/3602 batches | lr 0.0000 | ms/batch 91.45 | loss 10.53 | mse 10.53 | mre  0.00 |
scGPT - INFO - | epoch  78 | 1400/3602 batches | lr 0.0000 | ms/batch 91.37 | loss 13.13 | mse 13.13 | mre  0.00 |
scGPT - INFO - | epoch  78 | 1500/3602 batches | lr 0.0000 | ms/batch 91.45 | loss 13.91 | mse 13.91 | mre  0.00 |
scGPT - INFO - | epoch  78 | 1600/3602 batches | lr 0.0000 | ms/batch 91.37 | loss  9.65 | mse  9.65 | mre  0.00 |
scGPT - INFO - | epoch  78 | 1700/3602 batches | lr 0.0000 | ms/batch 92.02 | loss 10.43 | mse 10.43 | mre  0.00 |
scGPT - INFO - | epoch  78 | 1800/3602 batches | lr 0.0000 | ms/batch 91.56 | loss 10.42 | mse 10.42 | mre  0.00 |
scGPT - INFO - | epoch  78 | 1900/3602 batches | lr 0.0000 | ms/batch 91.28 | loss 13.05 | mse 13.05 | mre  0.00 |
scGPT - INFO - | epoch  78 | 2000/3602 batches | lr 0.0000 | ms/batch 91.46 | loss 11.35 | mse 11.35 | mre  0.00 |
scGPT - INFO - | epoch  78 | 2100/3602 batches | lr 0.0000 | ms/batch 91.66 | loss 12.67 | mse 12.67 | mre  0.00 |
scGPT - INFO - | epoch  78 | 2200/3602 batches | lr 0.0000 | ms/batch 91.71 | loss 10.90 | mse 10.90 | mre  0.00 |
scGPT - INFO - | epoch  78 | 2300/3602 batches | lr 0.0000 | ms/batch 91.80 | loss 10.92 | mse 10.92 | mre  0.00 |
scGPT - INFO - | epoch  78 | 2400/3602 batches | lr 0.0000 | ms/batch 93.05 | loss 11.80 | mse 11.80 | mre  0.00 |
scGPT - INFO - | epoch  78 | 2500/3602 batches | lr 0.0000 | ms/batch 92.75 | loss  9.97 | mse  9.97 | mre  0.00 |
scGPT - INFO - | epoch  78 | 2600/3602 batches | lr 0.0000 | ms/batch 92.20 | loss 13.74 | mse 13.74 | mre  0.00 |
scGPT - INFO - | epoch  78 | 2700/3602 batches | lr 0.0000 | ms/batch 91.80 | loss 10.67 | mse 10.67 | mre  0.00 |
scGPT - INFO - | epoch  78 | 2800/3602 batches | lr 0.0000 | ms/batch 91.35 | loss 11.53 | mse 11.53 | mre  0.00 |
scGPT - INFO - | epoch  78 | 2900/3602 batches | lr 0.0000 | ms/batch 91.84 | loss 11.70 | mse 11.70 | mre  0.00 |
scGPT - INFO - | epoch  78 | 3000/3602 batches | lr 0.0000 | ms/batch 93.30 | loss 11.28 | mse 11.28 | mre  0.00 |
scGPT - INFO - | epoch  78 | 3100/3602 batches | lr 0.0000 | ms/batch 93.28 | loss 10.55 | mse 10.55 | mre  0.00 |
scGPT - INFO - | epoch  78 | 3200/3602 batches | lr 0.0000 | ms/batch 93.30 | loss 10.84 | mse 10.84 | mre  0.00 |
scGPT - INFO - | epoch  78 | 3300/3602 batches | lr 0.0000 | ms/batch 92.09 | loss  9.27 | mse  9.27 | mre  0.00 |
scGPT - INFO - | epoch  78 | 3400/3602 batches | lr 0.0000 | ms/batch 91.21 | loss 10.62 | mse 10.62 | mre  0.00 |
scGPT - INFO - | epoch  78 | 3500/3602 batches | lr 0.0000 | ms/batch 91.29 | loss  9.89 | mse  9.89 | mre  0.00 |
scGPT - INFO - | epoch  78 | 3600/3602 batches | lr 0.0000 | ms/batch 91.09 | loss 11.49 | mse 11.49 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  78 | time: 342.89s | valid loss/mse 12.5537 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.552931918187088, min_delta 0.0001, val_loss 12.55366353967812
Loss error: -0.0007316214910311203
scGPT - INFO - INFO: Early stopping counter 3 of 10
epoch:  78
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.836 3.568 3.285 ... 3.453 3.648 3.879]
 [6.996 3.633 3.34  ... 3.46  3.69  3.912]
 [4.684 3.7   3.822 ... 4.094 3.715 3.55 ]
 ...
 [4.812 3.879 3.9   ... 4.066 3.79  3.578]
 [4.92  3.92  3.977 ... 4.12  3.865 3.615]
 [6.918 3.666 3.256 ... 3.469 3.613 3.889]]
(801, 11) (801, 11)
By feature:  MSE:  0.5707¬±0.0 MAE:  0.4926¬±0.0 R2:  0.2428¬±0.0 PCC:  0.8042¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5707¬±0.0 MAE:  0.4926¬±0.0 R2:  0.2869¬±0.0 PCC:  0.5228¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5706999897956848¬±0.0 MAE: 0.4925999939441681¬±0.0 R2: 0.2428¬±0.0 PCC: 0.8042¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5706999897956848¬±0.0 MAE: 0.4925999939441681¬±0.0 R2: 0.2869¬±0.0 PCC: 0.5228¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  79
Training model
scGPT - INFO - | epoch  79 | 100/3602 batches | lr 0.0000 | ms/batch 92.95 | loss 10.56 | mse 10.56 | mre  0.00 |
scGPT - INFO - | epoch  79 | 200/3602 batches | lr 0.0000 | ms/batch 91.23 | loss 12.25 | mse 12.25 | mre  0.00 |
scGPT - INFO - | epoch  79 | 300/3602 batches | lr 0.0000 | ms/batch 91.18 | loss  9.71 | mse  9.71 | mre  0.00 |
scGPT - INFO - | epoch  79 | 400/3602 batches | lr 0.0000 | ms/batch 91.27 | loss 10.43 | mse 10.43 | mre  0.00 |
scGPT - INFO - | epoch  79 | 500/3602 batches | lr 0.0000 | ms/batch 91.19 | loss  9.27 | mse  9.27 | mre  0.00 |
scGPT - INFO - | epoch  79 | 600/3602 batches | lr 0.0000 | ms/batch 91.26 | loss 10.58 | mse 10.58 | mre  0.00 |
scGPT - INFO - | epoch  79 | 700/3602 batches | lr 0.0000 | ms/batch 92.39 | loss 11.20 | mse 11.20 | mre  0.00 |
scGPT - INFO - | epoch  79 | 800/3602 batches | lr 0.0000 | ms/batch 93.27 | loss 13.88 | mse 13.88 | mre  0.00 |
scGPT - INFO - | epoch  79 | 900/3602 batches | lr 0.0000 | ms/batch 93.24 | loss 11.21 | mse 11.21 | mre  0.00 |
scGPT - INFO - | epoch  79 | 1000/3602 batches | lr 0.0000 | ms/batch 93.19 | loss 12.90 | mse 12.90 | mre  0.00 |
scGPT - INFO - | epoch  79 | 1100/3602 batches | lr 0.0000 | ms/batch 91.86 | loss 10.57 | mse 10.57 | mre  0.00 |
scGPT - INFO - | epoch  79 | 1200/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 11.00 | mse 11.00 | mre  0.00 |
scGPT - INFO - | epoch  79 | 1300/3602 batches | lr 0.0000 | ms/batch 91.23 | loss 10.61 | mse 10.61 | mre  0.00 |
scGPT - INFO - | epoch  79 | 1400/3602 batches | lr 0.0000 | ms/batch 91.02 | loss 13.44 | mse 13.44 | mre  0.00 |
scGPT - INFO - | epoch  79 | 1500/3602 batches | lr 0.0000 | ms/batch 91.23 | loss 13.46 | mse 13.46 | mre  0.00 |
scGPT - INFO - | epoch  79 | 1600/3602 batches | lr 0.0000 | ms/batch 91.29 | loss  9.94 | mse  9.94 | mre  0.00 |
scGPT - INFO - | epoch  79 | 1700/3602 batches | lr 0.0000 | ms/batch 91.20 | loss 10.34 | mse 10.34 | mre  0.00 |
scGPT - INFO - | epoch  79 | 1800/3602 batches | lr 0.0000 | ms/batch 91.11 | loss 10.44 | mse 10.44 | mre  0.00 |
scGPT - INFO - | epoch  79 | 1900/3602 batches | lr 0.0000 | ms/batch 91.26 | loss 12.74 | mse 12.74 | mre  0.00 |
scGPT - INFO - | epoch  79 | 2000/3602 batches | lr 0.0000 | ms/batch 91.44 | loss 11.18 | mse 11.18 | mre  0.00 |
scGPT - INFO - | epoch  79 | 2100/3602 batches | lr 0.0000 | ms/batch 91.98 | loss 12.84 | mse 12.84 | mre  0.00 |
scGPT - INFO - | epoch  79 | 2200/3602 batches | lr 0.0000 | ms/batch 91.24 | loss 11.06 | mse 11.06 | mre  0.00 |
scGPT - INFO - | epoch  79 | 2300/3602 batches | lr 0.0000 | ms/batch 91.19 | loss 10.49 | mse 10.49 | mre  0.00 |
scGPT - INFO - | epoch  79 | 2400/3602 batches | lr 0.0000 | ms/batch 91.15 | loss 11.90 | mse 11.90 | mre  0.00 |
scGPT - INFO - | epoch  79 | 2500/3602 batches | lr 0.0000 | ms/batch 91.13 | loss 10.50 | mse 10.50 | mre  0.00 |
scGPT - INFO - | epoch  79 | 2600/3602 batches | lr 0.0000 | ms/batch 91.20 | loss 14.26 | mse 14.26 | mre  0.00 |
scGPT - INFO - | epoch  79 | 2700/3602 batches | lr 0.0000 | ms/batch 91.15 | loss 10.92 | mse 10.92 | mre  0.00 |
scGPT - INFO - | epoch  79 | 2800/3602 batches | lr 0.0000 | ms/batch 91.28 | loss 11.60 | mse 11.60 | mre  0.00 |
scGPT - INFO - | epoch  79 | 2900/3602 batches | lr 0.0000 | ms/batch 91.33 | loss 11.60 | mse 11.60 | mre  0.00 |
scGPT - INFO - | epoch  79 | 3000/3602 batches | lr 0.0000 | ms/batch 91.35 | loss 11.30 | mse 11.30 | mre  0.00 |
scGPT - INFO - | epoch  79 | 3100/3602 batches | lr 0.0000 | ms/batch 91.84 | loss 10.46 | mse 10.46 | mre  0.00 |
scGPT - INFO - | epoch  79 | 3200/3602 batches | lr 0.0000 | ms/batch 91.11 | loss 10.74 | mse 10.74 | mre  0.00 |
scGPT - INFO - | epoch  79 | 3300/3602 batches | lr 0.0000 | ms/batch 91.12 | loss  9.32 | mse  9.32 | mre  0.00 |
scGPT - INFO - | epoch  79 | 3400/3602 batches | lr 0.0000 | ms/batch 91.20 | loss 10.88 | mse 10.88 | mre  0.00 |
scGPT - INFO - | epoch  79 | 3500/3602 batches | lr 0.0000 | ms/batch 91.16 | loss 10.42 | mse 10.42 | mre  0.00 |
scGPT - INFO - | epoch  79 | 3600/3602 batches | lr 0.0000 | ms/batch 91.22 | loss 11.81 | mse 11.81 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  79 | time: 342.19s | valid loss/mse 12.5537 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.552931918187088, min_delta 0.0001, val_loss 12.553735352931696
Loss error: -0.0008034347446077561
scGPT - INFO - INFO: Early stopping counter 4 of 10
epoch:  79
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.836 3.568 3.285 ... 3.453 3.648 3.879]
 [6.996 3.63  3.338 ... 3.459 3.688 3.912]
 [4.688 3.7   3.822 ... 4.094 3.715 3.55 ]
 ...
 [4.816 3.879 3.9   ... 4.07  3.79  3.578]
 [4.92  3.92  3.977 ... 4.125 3.865 3.615]
 [6.918 3.666 3.256 ... 3.47  3.613 3.889]]
(801, 11) (801, 11)
By feature:  MSE:  0.5707¬±0.0 MAE:  0.4926¬±0.0 R2:  0.2429¬±0.0 PCC:  0.8042¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5707¬±0.0 MAE:  0.4926¬±0.0 R2:  0.287¬±0.0 PCC:  0.5229¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5706999897956848¬±0.0 MAE: 0.4925999939441681¬±0.0 R2: 0.2429¬±0.0 PCC: 0.8042¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5706999897956848¬±0.0 MAE: 0.4925999939441681¬±0.0 R2: 0.287¬±0.0 PCC: 0.5229¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  80
Training model
scGPT - INFO - | epoch  80 | 100/3602 batches | lr 0.0000 | ms/batch 93.17 | loss 10.81 | mse 10.81 | mre  0.00 |
scGPT - INFO - | epoch  80 | 200/3602 batches | lr 0.0000 | ms/batch 91.88 | loss 12.37 | mse 12.37 | mre  0.00 |
scGPT - INFO - | epoch  80 | 300/3602 batches | lr 0.0000 | ms/batch 91.57 | loss 10.20 | mse 10.20 | mre  0.00 |
scGPT - INFO - | epoch  80 | 400/3602 batches | lr 0.0000 | ms/batch 91.84 | loss 10.95 | mse 10.95 | mre  0.00 |
scGPT - INFO - | epoch  80 | 500/3602 batches | lr 0.0000 | ms/batch 91.90 | loss  9.21 | mse  9.21 | mre  0.00 |
scGPT - INFO - | epoch  80 | 600/3602 batches | lr 0.0000 | ms/batch 91.16 | loss 11.03 | mse 11.03 | mre  0.00 |
scGPT - INFO - | epoch  80 | 700/3602 batches | lr 0.0000 | ms/batch 91.27 | loss 10.85 | mse 10.85 | mre  0.00 |
scGPT - INFO - | epoch  80 | 800/3602 batches | lr 0.0000 | ms/batch 91.22 | loss 14.36 | mse 14.36 | mre  0.00 |
scGPT - INFO - | epoch  80 | 900/3602 batches | lr 0.0000 | ms/batch 91.18 | loss 11.10 | mse 11.10 | mre  0.00 |
scGPT - INFO - | epoch  80 | 1000/3602 batches | lr 0.0000 | ms/batch 91.21 | loss 12.80 | mse 12.80 | mre  0.00 |
scGPT - INFO - | epoch  80 | 1100/3602 batches | lr 0.0000 | ms/batch 91.20 | loss 10.63 | mse 10.63 | mre  0.00 |
scGPT - INFO - | epoch  80 | 1200/3602 batches | lr 0.0000 | ms/batch 91.17 | loss 10.78 | mse 10.78 | mre  0.00 |
scGPT - INFO - | epoch  80 | 1300/3602 batches | lr 0.0000 | ms/batch 91.36 | loss 11.12 | mse 11.12 | mre  0.00 |
scGPT - INFO - | epoch  80 | 1400/3602 batches | lr 0.0000 | ms/batch 91.11 | loss 13.27 | mse 13.27 | mre  0.00 |
scGPT - INFO - | epoch  80 | 1500/3602 batches | lr 0.0000 | ms/batch 91.65 | loss 13.54 | mse 13.54 | mre  0.00 |
scGPT - INFO - | epoch  80 | 1600/3602 batches | lr 0.0000 | ms/batch 91.17 | loss  9.47 | mse  9.47 | mre  0.00 |
scGPT - INFO - | epoch  80 | 1700/3602 batches | lr 0.0000 | ms/batch 91.23 | loss 10.37 | mse 10.37 | mre  0.00 |
scGPT - INFO - | epoch  80 | 1800/3602 batches | lr 0.0000 | ms/batch 91.15 | loss 10.49 | mse 10.49 | mre  0.00 |
scGPT - INFO - | epoch  80 | 1900/3602 batches | lr 0.0000 | ms/batch 91.16 | loss 12.81 | mse 12.81 | mre  0.00 |
scGPT - INFO - | epoch  80 | 2000/3602 batches | lr 0.0000 | ms/batch 91.90 | loss 11.13 | mse 11.13 | mre  0.00 |
scGPT - INFO - | epoch  80 | 2100/3602 batches | lr 0.0000 | ms/batch 93.22 | loss 12.89 | mse 12.89 | mre  0.00 |
scGPT - INFO - | epoch  80 | 2200/3602 batches | lr 0.0000 | ms/batch 93.29 | loss 10.88 | mse 10.88 | mre  0.00 |
scGPT - INFO - | epoch  80 | 2300/3602 batches | lr 0.0000 | ms/batch 92.23 | loss 10.39 | mse 10.39 | mre  0.00 |
scGPT - INFO - | epoch  80 | 2400/3602 batches | lr 0.0000 | ms/batch 91.06 | loss 11.97 | mse 11.97 | mre  0.00 |
scGPT - INFO - | epoch  80 | 2500/3602 batches | lr 0.0000 | ms/batch 91.61 | loss 10.01 | mse 10.01 | mre  0.00 |
scGPT - INFO - | epoch  80 | 2600/3602 batches | lr 0.0000 | ms/batch 91.09 | loss 13.90 | mse 13.90 | mre  0.00 |
scGPT - INFO - | epoch  80 | 2700/3602 batches | lr 0.0000 | ms/batch 91.08 | loss 10.45 | mse 10.45 | mre  0.00 |
scGPT - INFO - | epoch  80 | 2800/3602 batches | lr 0.0000 | ms/batch 91.07 | loss 11.83 | mse 11.83 | mre  0.00 |
scGPT - INFO - | epoch  80 | 2900/3602 batches | lr 0.0000 | ms/batch 91.00 | loss 11.38 | mse 11.38 | mre  0.00 |
scGPT - INFO - | epoch  80 | 3000/3602 batches | lr 0.0000 | ms/batch 91.11 | loss 11.59 | mse 11.59 | mre  0.00 |
scGPT - INFO - | epoch  80 | 3100/3602 batches | lr 0.0000 | ms/batch 91.11 | loss 10.19 | mse 10.19 | mre  0.00 |
scGPT - INFO - | epoch  80 | 3200/3602 batches | lr 0.0000 | ms/batch 91.11 | loss 10.92 | mse 10.92 | mre  0.00 |
scGPT - INFO - | epoch  80 | 3300/3602 batches | lr 0.0000 | ms/batch 91.18 | loss  9.66 | mse  9.66 | mre  0.00 |
scGPT - INFO - | epoch  80 | 3400/3602 batches | lr 0.0000 | ms/batch 91.29 | loss 10.49 | mse 10.49 | mre  0.00 |
scGPT - INFO - | epoch  80 | 3500/3602 batches | lr 0.0000 | ms/batch 91.88 | loss 10.60 | mse 10.60 | mre  0.00 |
scGPT - INFO - | epoch  80 | 3600/3602 batches | lr 0.0000 | ms/batch 91.46 | loss 11.43 | mse 11.43 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  80 | time: 342.00s | valid loss/mse 12.5532 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.552931918187088, min_delta 0.0001, val_loss 12.553206947114733
Loss error: -0.00027502892764452724
scGPT - INFO - INFO: Early stopping counter 5 of 10
epoch:  80
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.836 3.568 3.285 ... 3.453 3.648 3.879]
 [6.996 3.63  3.338 ... 3.459 3.688 3.912]
 [4.684 3.7   3.822 ... 4.094 3.715 3.549]
 ...
 [4.812 3.879 3.9   ... 4.066 3.79  3.58 ]
 [4.926 3.918 3.977 ... 4.12  3.865 3.615]
 [6.914 3.666 3.256 ... 3.469 3.611 3.889]]
(801, 11) (801, 11)
By feature:  MSE:  0.5706¬±0.0 MAE:  0.4926¬±0.0 R2:  0.2428¬±0.0 PCC:  0.8043¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5706¬±0.0 MAE:  0.4926¬±0.0 R2:  0.287¬±0.0 PCC:  0.523¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5705999732017517¬±0.0 MAE: 0.4925999939441681¬±0.0 R2: 0.2428¬±0.0 PCC: 0.8043¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5705999732017517¬±0.0 MAE: 0.4925999939441681¬±0.0 R2: 0.287¬±0.0 PCC: 0.523¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  81
Training model
scGPT - INFO - | epoch  81 | 100/3602 batches | lr 0.0000 | ms/batch 92.50 | loss 10.49 | mse 10.49 | mre  0.00 |
scGPT - INFO - | epoch  81 | 200/3602 batches | lr 0.0000 | ms/batch 91.51 | loss 12.40 | mse 12.40 | mre  0.00 |
scGPT - INFO - | epoch  81 | 300/3602 batches | lr 0.0000 | ms/batch 91.48 | loss  9.74 | mse  9.74 | mre  0.00 |
scGPT - INFO - | epoch  81 | 400/3602 batches | lr 0.0000 | ms/batch 91.50 | loss 10.60 | mse 10.60 | mre  0.00 |
scGPT - INFO - | epoch  81 | 500/3602 batches | lr 0.0000 | ms/batch 91.53 | loss  9.42 | mse  9.42 | mre  0.00 |
scGPT - INFO - | epoch  81 | 600/3602 batches | lr 0.0000 | ms/batch 91.47 | loss 10.95 | mse 10.95 | mre  0.00 |
scGPT - INFO - | epoch  81 | 700/3602 batches | lr 0.0000 | ms/batch 91.50 | loss 11.24 | mse 11.24 | mre  0.00 |
scGPT - INFO - | epoch  81 | 800/3602 batches | lr 0.0000 | ms/batch 91.27 | loss 14.08 | mse 14.08 | mre  0.00 |
scGPT - INFO - | epoch  81 | 900/3602 batches | lr 0.0000 | ms/batch 91.84 | loss 11.16 | mse 11.16 | mre  0.00 |
scGPT - INFO - | epoch  81 | 1000/3602 batches | lr 0.0000 | ms/batch 91.43 | loss 12.62 | mse 12.62 | mre  0.00 |
scGPT - INFO - | epoch  81 | 1100/3602 batches | lr 0.0000 | ms/batch 91.34 | loss 10.33 | mse 10.33 | mre  0.00 |
scGPT - INFO - | epoch  81 | 1200/3602 batches | lr 0.0000 | ms/batch 91.49 | loss 11.04 | mse 11.04 | mre  0.00 |
scGPT - INFO - | epoch  81 | 1300/3602 batches | lr 0.0000 | ms/batch 91.65 | loss 11.16 | mse 11.16 | mre  0.00 |
scGPT - INFO - | epoch  81 | 1400/3602 batches | lr 0.0000 | ms/batch 91.31 | loss 13.45 | mse 13.45 | mre  0.00 |
scGPT - INFO - | epoch  81 | 1500/3602 batches | lr 0.0000 | ms/batch 91.29 | loss 13.50 | mse 13.50 | mre  0.00 |
scGPT - INFO - | epoch  81 | 1600/3602 batches | lr 0.0000 | ms/batch 91.24 | loss  9.78 | mse  9.78 | mre  0.00 |
scGPT - INFO - | epoch  81 | 1700/3602 batches | lr 0.0000 | ms/batch 91.19 | loss 10.37 | mse 10.37 | mre  0.00 |
scGPT - INFO - | epoch  81 | 1800/3602 batches | lr 0.0000 | ms/batch 91.26 | loss 10.50 | mse 10.50 | mre  0.00 |
scGPT - INFO - | epoch  81 | 1900/3602 batches | lr 0.0000 | ms/batch 91.90 | loss 13.44 | mse 13.44 | mre  0.00 |
scGPT - INFO - | epoch  81 | 2000/3602 batches | lr 0.0000 | ms/batch 91.30 | loss 10.86 | mse 10.86 | mre  0.00 |
scGPT - INFO - | epoch  81 | 2100/3602 batches | lr 0.0000 | ms/batch 91.56 | loss 12.47 | mse 12.47 | mre  0.00 |
scGPT - INFO - | epoch  81 | 2200/3602 batches | lr 0.0000 | ms/batch 92.20 | loss 10.96 | mse 10.96 | mre  0.00 |
scGPT - INFO - | epoch  81 | 2300/3602 batches | lr 0.0000 | ms/batch 91.52 | loss 10.95 | mse 10.95 | mre  0.00 |
scGPT - INFO - | epoch  81 | 2400/3602 batches | lr 0.0000 | ms/batch 91.29 | loss 11.67 | mse 11.67 | mre  0.00 |
scGPT - INFO - | epoch  81 | 2500/3602 batches | lr 0.0000 | ms/batch 92.53 | loss 10.43 | mse 10.43 | mre  0.00 |
scGPT - INFO - | epoch  81 | 2600/3602 batches | lr 0.0000 | ms/batch 92.71 | loss 14.61 | mse 14.61 | mre  0.00 |
scGPT - INFO - | epoch  81 | 2700/3602 batches | lr 0.0000 | ms/batch 92.18 | loss 10.69 | mse 10.69 | mre  0.00 |
scGPT - INFO - | epoch  81 | 2800/3602 batches | lr 0.0000 | ms/batch 91.91 | loss 11.78 | mse 11.78 | mre  0.00 |
scGPT - INFO - | epoch  81 | 2900/3602 batches | lr 0.0000 | ms/batch 92.34 | loss 11.74 | mse 11.74 | mre  0.00 |
scGPT - INFO - | epoch  81 | 3000/3602 batches | lr 0.0000 | ms/batch 91.63 | loss 11.34 | mse 11.34 | mre  0.00 |
scGPT - INFO - | epoch  81 | 3100/3602 batches | lr 0.0000 | ms/batch 91.28 | loss 10.56 | mse 10.56 | mre  0.00 |
scGPT - INFO - | epoch  81 | 3200/3602 batches | lr 0.0000 | ms/batch 91.31 | loss 10.81 | mse 10.81 | mre  0.00 |
scGPT - INFO - | epoch  81 | 3300/3602 batches | lr 0.0000 | ms/batch 91.30 | loss  9.53 | mse  9.53 | mre  0.00 |
scGPT - INFO - | epoch  81 | 3400/3602 batches | lr 0.0000 | ms/batch 91.19 | loss 10.62 | mse 10.62 | mre  0.00 |
scGPT - INFO - | epoch  81 | 3500/3602 batches | lr 0.0000 | ms/batch 91.32 | loss 10.43 | mse 10.43 | mre  0.00 |
scGPT - INFO - | epoch  81 | 3600/3602 batches | lr 0.0000 | ms/batch 91.62 | loss 11.58 | mse 11.58 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  81 | time: 342.42s | valid loss/mse 12.5534 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.552931918187088, min_delta 0.0001, val_loss 12.553428994210323
Loss error: -0.0004970760232350102
scGPT - INFO - INFO: Early stopping counter 6 of 10
epoch:  81
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.836 3.568 3.285 ... 3.453 3.648 3.879]
 [6.996 3.63  3.338 ... 3.459 3.688 3.912]
 [4.684 3.7   3.822 ... 4.094 3.715 3.55 ]
 ...
 [4.812 3.879 3.9   ... 4.066 3.79  3.58 ]
 [4.92  3.918 3.977 ... 4.12  3.865 3.615]
 [6.914 3.664 3.256 ... 3.469 3.611 3.889]]
(801, 11) (801, 11)
By feature:  MSE:  0.5707¬±0.0 MAE:  0.4925¬±0.0 R2:  0.2429¬±0.0 PCC:  0.8043¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5707¬±0.0 MAE:  0.4925¬±0.0 R2:  0.287¬±0.0 PCC:  0.5229¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5706999897956848¬±0.0 MAE: 0.4925000071525574¬±0.0 R2: 0.2429¬±0.0 PCC: 0.8043¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5706999897956848¬±0.0 MAE: 0.4925000071525574¬±0.0 R2: 0.287¬±0.0 PCC: 0.5229¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  82
Training model
scGPT - INFO - | epoch  82 | 100/3602 batches | lr 0.0000 | ms/batch 93.12 | loss 10.40 | mse 10.40 | mre  0.00 |
scGPT - INFO - | epoch  82 | 200/3602 batches | lr 0.0000 | ms/batch 91.87 | loss 12.36 | mse 12.36 | mre  0.00 |
scGPT - INFO - | epoch  82 | 300/3602 batches | lr 0.0000 | ms/batch 91.83 | loss 10.48 | mse 10.48 | mre  0.00 |
scGPT - INFO - | epoch  82 | 400/3602 batches | lr 0.0000 | ms/batch 91.77 | loss 10.32 | mse 10.32 | mre  0.00 |
scGPT - INFO - | epoch  82 | 500/3602 batches | lr 0.0000 | ms/batch 91.59 | loss  9.20 | mse  9.20 | mre  0.00 |
scGPT - INFO - | epoch  82 | 600/3602 batches | lr 0.0000 | ms/batch 91.15 | loss 10.80 | mse 10.80 | mre  0.00 |
scGPT - INFO - | epoch  82 | 700/3602 batches | lr 0.0000 | ms/batch 91.15 | loss 11.61 | mse 11.61 | mre  0.00 |
scGPT - INFO - | epoch  82 | 800/3602 batches | lr 0.0000 | ms/batch 91.27 | loss 13.79 | mse 13.79 | mre  0.00 |
scGPT - INFO - | epoch  82 | 900/3602 batches | lr 0.0000 | ms/batch 92.55 | loss 11.08 | mse 11.08 | mre  0.00 |
scGPT - INFO - | epoch  82 | 1000/3602 batches | lr 0.0000 | ms/batch 91.69 | loss 12.27 | mse 12.27 | mre  0.00 |
scGPT - INFO - | epoch  82 | 1100/3602 batches | lr 0.0000 | ms/batch 91.88 | loss 10.50 | mse 10.50 | mre  0.00 |
scGPT - INFO - | epoch  82 | 1200/3602 batches | lr 0.0000 | ms/batch 91.95 | loss 10.97 | mse 10.97 | mre  0.00 |
scGPT - INFO - | epoch  82 | 1300/3602 batches | lr 0.0000 | ms/batch 92.19 | loss 10.38 | mse 10.38 | mre  0.00 |
scGPT - INFO - | epoch  82 | 1400/3602 batches | lr 0.0000 | ms/batch 91.32 | loss 13.82 | mse 13.82 | mre  0.00 |
scGPT - INFO - | epoch  82 | 1500/3602 batches | lr 0.0000 | ms/batch 91.33 | loss 13.70 | mse 13.70 | mre  0.00 |
scGPT - INFO - | epoch  82 | 1600/3602 batches | lr 0.0000 | ms/batch 91.13 | loss  9.73 | mse  9.73 | mre  0.00 |
scGPT - INFO - | epoch  82 | 1700/3602 batches | lr 0.0000 | ms/batch 91.90 | loss 10.18 | mse 10.18 | mre  0.00 |
scGPT - INFO - | epoch  82 | 1800/3602 batches | lr 0.0000 | ms/batch 91.98 | loss 10.76 | mse 10.76 | mre  0.00 |
scGPT - INFO - | epoch  82 | 1900/3602 batches | lr 0.0000 | ms/batch 91.97 | loss 13.20 | mse 13.20 | mre  0.00 |
scGPT - INFO - | epoch  82 | 2000/3602 batches | lr 0.0000 | ms/batch 92.52 | loss 11.32 | mse 11.32 | mre  0.00 |
scGPT - INFO - | epoch  82 | 2100/3602 batches | lr 0.0000 | ms/batch 92.26 | loss 12.35 | mse 12.35 | mre  0.00 |
scGPT - INFO - | epoch  82 | 2200/3602 batches | lr 0.0000 | ms/batch 93.33 | loss 11.20 | mse 11.20 | mre  0.00 |
scGPT - INFO - | epoch  82 | 2300/3602 batches | lr 0.0000 | ms/batch 93.96 | loss 10.92 | mse 10.92 | mre  0.00 |
scGPT - INFO - | epoch  82 | 2400/3602 batches | lr 0.0000 | ms/batch 92.36 | loss 11.55 | mse 11.55 | mre  0.00 |
scGPT - INFO - | epoch  82 | 2500/3602 batches | lr 0.0000 | ms/batch 92.26 | loss 10.26 | mse 10.26 | mre  0.00 |
scGPT - INFO - | epoch  82 | 2600/3602 batches | lr 0.0000 | ms/batch 92.42 | loss 14.47 | mse 14.47 | mre  0.00 |
scGPT - INFO - | epoch  82 | 2700/3602 batches | lr 0.0000 | ms/batch 92.82 | loss 10.42 | mse 10.42 | mre  0.00 |
scGPT - INFO - | epoch  82 | 2800/3602 batches | lr 0.0000 | ms/batch 92.59 | loss 11.97 | mse 11.97 | mre  0.00 |
scGPT - INFO - | epoch  82 | 2900/3602 batches | lr 0.0000 | ms/batch 92.23 | loss 11.77 | mse 11.77 | mre  0.00 |
scGPT - INFO - | epoch  82 | 3000/3602 batches | lr 0.0000 | ms/batch 91.97 | loss 11.64 | mse 11.64 | mre  0.00 |
scGPT - INFO - | epoch  82 | 3100/3602 batches | lr 0.0000 | ms/batch 91.90 | loss 10.57 | mse 10.57 | mre  0.00 |
scGPT - INFO - | epoch  82 | 3200/3602 batches | lr 0.0000 | ms/batch 91.98 | loss 11.32 | mse 11.32 | mre  0.00 |
scGPT - INFO - | epoch  82 | 3300/3602 batches | lr 0.0000 | ms/batch 91.89 | loss  9.56 | mse  9.56 | mre  0.00 |
scGPT - INFO - | epoch  82 | 3400/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 10.19 | mse 10.19 | mre  0.00 |
scGPT - INFO - | epoch  82 | 3500/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 10.03 | mse 10.03 | mre  0.00 |
scGPT - INFO - | epoch  82 | 3600/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 11.82 | mse 11.82 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  82 | time: 343.60s | valid loss/mse 12.5532 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.552931918187088, min_delta 0.0001, val_loss 12.553154177284718
Loss error: -0.000222259097629518
scGPT - INFO - INFO: Early stopping counter 7 of 10
epoch:  82
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.836 3.568 3.285 ... 3.451 3.648 3.879]
 [6.996 3.63  3.338 ... 3.459 3.688 3.912]
 [4.684 3.7   3.822 ... 4.094 3.715 3.55 ]
 ...
 [4.812 3.879 3.9   ... 4.066 3.79  3.578]
 [4.92  3.918 3.977 ... 4.12  3.865 3.617]
 [6.914 3.666 3.256 ... 3.469 3.611 3.889]]
(801, 11) (801, 11)
By feature:  MSE:  0.5706¬±0.0 MAE:  0.4925¬±0.0 R2:  0.2429¬±0.0 PCC:  0.8043¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5706¬±0.0 MAE:  0.4925¬±0.0 R2:  0.2871¬±0.0 PCC:  0.523¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5705999732017517¬±0.0 MAE: 0.4925000071525574¬±0.0 R2: 0.2429¬±0.0 PCC: 0.8043¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5705999732017517¬±0.0 MAE: 0.4925000071525574¬±0.0 R2: 0.2871¬±0.0 PCC: 0.523¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  83
Training model
scGPT - INFO - | epoch  83 | 100/3602 batches | lr 0.0000 | ms/batch 92.95 | loss 10.51 | mse 10.51 | mre  0.00 |
scGPT - INFO - | epoch  83 | 200/3602 batches | lr 0.0000 | ms/batch 91.79 | loss 12.53 | mse 12.53 | mre  0.00 |
scGPT - INFO - | epoch  83 | 300/3602 batches | lr 0.0000 | ms/batch 91.56 | loss 10.03 | mse 10.03 | mre  0.00 |
scGPT - INFO - | epoch  83 | 400/3602 batches | lr 0.0000 | ms/batch 91.83 | loss 10.72 | mse 10.72 | mre  0.00 |
scGPT - INFO - | epoch  83 | 500/3602 batches | lr 0.0000 | ms/batch 92.28 | loss  9.52 | mse  9.52 | mre  0.00 |
scGPT - INFO - | epoch  83 | 600/3602 batches | lr 0.0000 | ms/batch 91.77 | loss 10.89 | mse 10.89 | mre  0.00 |
scGPT - INFO - | epoch  83 | 700/3602 batches | lr 0.0000 | ms/batch 92.75 | loss 11.28 | mse 11.28 | mre  0.00 |
scGPT - INFO - | epoch  83 | 800/3602 batches | lr 0.0000 | ms/batch 91.84 | loss 13.83 | mse 13.83 | mre  0.00 |
scGPT - INFO - | epoch  83 | 900/3602 batches | lr 0.0000 | ms/batch 91.89 | loss 10.98 | mse 10.98 | mre  0.00 |
scGPT - INFO - | epoch  83 | 1000/3602 batches | lr 0.0000 | ms/batch 92.14 | loss 12.43 | mse 12.43 | mre  0.00 |
scGPT - INFO - | epoch  83 | 1100/3602 batches | lr 0.0000 | ms/batch 92.08 | loss 10.64 | mse 10.64 | mre  0.00 |
scGPT - INFO - | epoch  83 | 1200/3602 batches | lr 0.0000 | ms/batch 92.16 | loss 11.06 | mse 11.06 | mre  0.00 |
scGPT - INFO - | epoch  83 | 1300/3602 batches | lr 0.0000 | ms/batch 91.87 | loss 10.53 | mse 10.53 | mre  0.00 |
scGPT - INFO - | epoch  83 | 1400/3602 batches | lr 0.0000 | ms/batch 91.04 | loss 13.32 | mse 13.32 | mre  0.00 |
scGPT - INFO - | epoch  83 | 1500/3602 batches | lr 0.0000 | ms/batch 91.58 | loss 13.65 | mse 13.65 | mre  0.00 |
scGPT - INFO - | epoch  83 | 1600/3602 batches | lr 0.0000 | ms/batch 91.94 | loss  9.81 | mse  9.81 | mre  0.00 |
scGPT - INFO - | epoch  83 | 1700/3602 batches | lr 0.0000 | ms/batch 92.29 | loss 10.47 | mse 10.47 | mre  0.00 |
scGPT - INFO - | epoch  83 | 1800/3602 batches | lr 0.0000 | ms/batch 91.03 | loss 10.42 | mse 10.42 | mre  0.00 |
scGPT - INFO - | epoch  83 | 1900/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 12.91 | mse 12.91 | mre  0.00 |
scGPT - INFO - | epoch  83 | 2000/3602 batches | lr 0.0000 | ms/batch 91.20 | loss 11.13 | mse 11.13 | mre  0.00 |
scGPT - INFO - | epoch  83 | 2100/3602 batches | lr 0.0000 | ms/batch 91.25 | loss 12.61 | mse 12.61 | mre  0.00 |
scGPT - INFO - | epoch  83 | 2200/3602 batches | lr 0.0000 | ms/batch 92.21 | loss 11.03 | mse 11.03 | mre  0.00 |
scGPT - INFO - | epoch  83 | 2300/3602 batches | lr 0.0000 | ms/batch 96.43 | loss 10.78 | mse 10.78 | mre  0.00 |
scGPT - INFO - | epoch  83 | 2400/3602 batches | lr 0.0000 | ms/batch 96.50 | loss 11.56 | mse 11.56 | mre  0.00 |
scGPT - INFO - | epoch  83 | 2500/3602 batches | lr 0.0000 | ms/batch 96.91 | loss 10.75 | mse 10.75 | mre  0.00 |
scGPT - INFO - | epoch  83 | 2600/3602 batches | lr 0.0000 | ms/batch 96.15 | loss 14.30 | mse 14.30 | mre  0.00 |
scGPT - INFO - | epoch  83 | 2700/3602 batches | lr 0.0000 | ms/batch 97.10 | loss 10.41 | mse 10.41 | mre  0.00 |
scGPT - INFO - | epoch  83 | 2800/3602 batches | lr 0.0000 | ms/batch 97.18 | loss 11.64 | mse 11.64 | mre  0.00 |
scGPT - INFO - | epoch  83 | 2900/3602 batches | lr 0.0000 | ms/batch 96.98 | loss 11.49 | mse 11.49 | mre  0.00 |
scGPT - INFO - | epoch  83 | 3000/3602 batches | lr 0.0000 | ms/batch 96.81 | loss 11.43 | mse 11.43 | mre  0.00 |
scGPT - INFO - | epoch  83 | 3100/3602 batches | lr 0.0000 | ms/batch 96.82 | loss 10.37 | mse 10.37 | mre  0.00 |
scGPT - INFO - | epoch  83 | 3200/3602 batches | lr 0.0000 | ms/batch 96.63 | loss 10.92 | mse 10.92 | mre  0.00 |
scGPT - INFO - | epoch  83 | 3300/3602 batches | lr 0.0000 | ms/batch 96.38 | loss  9.47 | mse  9.47 | mre  0.00 |
scGPT - INFO - | epoch  83 | 3400/3602 batches | lr 0.0000 | ms/batch 96.37 | loss 10.29 | mse 10.29 | mre  0.00 |
scGPT - INFO - | epoch  83 | 3500/3602 batches | lr 0.0000 | ms/batch 96.53 | loss 10.07 | mse 10.07 | mre  0.00 |
scGPT - INFO - | epoch  83 | 3600/3602 batches | lr 0.0000 | ms/batch 96.33 | loss 11.88 | mse 11.88 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  83 | time: 352.22s | valid loss/mse 12.5539 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.552931918187088, min_delta 0.0001, val_loss 12.553918952947848
Loss error: -0.0009870347607598973
scGPT - INFO - INFO: Early stopping counter 8 of 10
epoch:  83
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.836 3.568 3.285 ... 3.451 3.648 3.879]
 [6.996 3.63  3.338 ... 3.459 3.688 3.912]
 [4.684 3.7   3.822 ... 4.094 3.715 3.55 ]
 ...
 [4.812 3.879 3.9   ... 4.066 3.79  3.578]
 [4.92  3.918 3.977 ... 4.12  3.865 3.615]
 [6.914 3.666 3.256 ... 3.469 3.613 3.889]]
(801, 11) (801, 11)
By feature:  MSE:  0.5707¬±0.0 MAE:  0.4926¬±0.0 R2:  0.2428¬±0.0 PCC:  0.8042¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5707¬±0.0 MAE:  0.4926¬±0.0 R2:  0.287¬±0.0 PCC:  0.5229¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5706999897956848¬±0.0 MAE: 0.4925999939441681¬±0.0 R2: 0.2428¬±0.0 PCC: 0.8042¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5706999897956848¬±0.0 MAE: 0.4925999939441681¬±0.0 R2: 0.287¬±0.0 PCC: 0.5229¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  84
Training model
scGPT - INFO - | epoch  84 | 100/3602 batches | lr 0.0000 | ms/batch 98.70 | loss 10.67 | mse 10.67 | mre  0.00 |
scGPT - INFO - | epoch  84 | 200/3602 batches | lr 0.0000 | ms/batch 96.23 | loss 12.13 | mse 12.13 | mre  0.00 |
scGPT - INFO - | epoch  84 | 300/3602 batches | lr 0.0000 | ms/batch 96.45 | loss 10.09 | mse 10.09 | mre  0.00 |
scGPT - INFO - | epoch  84 | 400/3602 batches | lr 0.0000 | ms/batch 96.19 | loss 10.64 | mse 10.64 | mre  0.00 |
scGPT - INFO - | epoch  84 | 500/3602 batches | lr 0.0000 | ms/batch 96.60 | loss  9.30 | mse  9.30 | mre  0.00 |
scGPT - INFO - | epoch  84 | 600/3602 batches | lr 0.0000 | ms/batch 96.47 | loss 10.61 | mse 10.61 | mre  0.00 |
scGPT - INFO - | epoch  84 | 700/3602 batches | lr 0.0000 | ms/batch 96.37 | loss 11.40 | mse 11.40 | mre  0.00 |
scGPT - INFO - | epoch  84 | 800/3602 batches | lr 0.0000 | ms/batch 96.38 | loss 13.79 | mse 13.79 | mre  0.00 |
scGPT - INFO - | epoch  84 | 900/3602 batches | lr 0.0000 | ms/batch 96.47 | loss 11.29 | mse 11.29 | mre  0.00 |
scGPT - INFO - | epoch  84 | 1000/3602 batches | lr 0.0000 | ms/batch 96.31 | loss 12.50 | mse 12.50 | mre  0.00 |
scGPT - INFO - | epoch  84 | 1100/3602 batches | lr 0.0000 | ms/batch 97.02 | loss 10.49 | mse 10.49 | mre  0.00 |
scGPT - INFO - | epoch  84 | 1200/3602 batches | lr 0.0000 | ms/batch 96.40 | loss 10.54 | mse 10.54 | mre  0.00 |
scGPT - INFO - | epoch  84 | 1300/3602 batches | lr 0.0000 | ms/batch 96.27 | loss 10.50 | mse 10.50 | mre  0.00 |
scGPT - INFO - | epoch  84 | 1400/3602 batches | lr 0.0000 | ms/batch 96.37 | loss 13.46 | mse 13.46 | mre  0.00 |
scGPT - INFO - | epoch  84 | 1500/3602 batches | lr 0.0000 | ms/batch 96.34 | loss 13.53 | mse 13.53 | mre  0.00 |
scGPT - INFO - | epoch  84 | 1600/3602 batches | lr 0.0000 | ms/batch 96.33 | loss  9.86 | mse  9.86 | mre  0.00 |
scGPT - INFO - | epoch  84 | 1700/3602 batches | lr 0.0000 | ms/batch 95.15 | loss 10.18 | mse 10.18 | mre  0.00 |
scGPT - INFO - | epoch  84 | 1800/3602 batches | lr 0.0000 | ms/batch 91.18 | loss 10.17 | mse 10.17 | mre  0.00 |
scGPT - INFO - | epoch  84 | 1900/3602 batches | lr 0.0000 | ms/batch 91.15 | loss 13.09 | mse 13.09 | mre  0.00 |
scGPT - INFO - | epoch  84 | 2000/3602 batches | lr 0.0000 | ms/batch 91.56 | loss 11.13 | mse 11.13 | mre  0.00 |
scGPT - INFO - | epoch  84 | 2100/3602 batches | lr 0.0000 | ms/batch 91.69 | loss 12.65 | mse 12.65 | mre  0.00 |
scGPT - INFO - | epoch  84 | 2200/3602 batches | lr 0.0000 | ms/batch 93.91 | loss 10.39 | mse 10.39 | mre  0.00 |
scGPT - INFO - | epoch  84 | 2300/3602 batches | lr 0.0000 | ms/batch 96.46 | loss 10.70 | mse 10.70 | mre  0.00 |
scGPT - INFO - | epoch  84 | 2400/3602 batches | lr 0.0000 | ms/batch 96.44 | loss 12.08 | mse 12.08 | mre  0.00 |
scGPT - INFO - | epoch  84 | 2500/3602 batches | lr 0.0000 | ms/batch 96.37 | loss 10.05 | mse 10.05 | mre  0.00 |
scGPT - INFO - | epoch  84 | 2600/3602 batches | lr 0.0000 | ms/batch 96.37 | loss 14.08 | mse 14.08 | mre  0.00 |
scGPT - INFO - | epoch  84 | 2700/3602 batches | lr 0.0000 | ms/batch 94.16 | loss 10.70 | mse 10.70 | mre  0.00 |
scGPT - INFO - | epoch  84 | 2800/3602 batches | lr 0.0000 | ms/batch 91.14 | loss 11.61 | mse 11.61 | mre  0.00 |
scGPT - INFO - | epoch  84 | 2900/3602 batches | lr 0.0000 | ms/batch 91.80 | loss 11.78 | mse 11.78 | mre  0.00 |
scGPT - INFO - | epoch  84 | 3000/3602 batches | lr 0.0000 | ms/batch 92.00 | loss 11.35 | mse 11.35 | mre  0.00 |
scGPT - INFO - | epoch  84 | 3100/3602 batches | lr 0.0000 | ms/batch 93.60 | loss 10.62 | mse 10.62 | mre  0.00 |
scGPT - INFO - | epoch  84 | 3200/3602 batches | lr 0.0000 | ms/batch 92.96 | loss 10.78 | mse 10.78 | mre  0.00 |
scGPT - INFO - | epoch  84 | 3300/3602 batches | lr 0.0000 | ms/batch 93.02 | loss  9.96 | mse  9.96 | mre  0.00 |
scGPT - INFO - | epoch  84 | 3400/3602 batches | lr 0.0000 | ms/batch 93.05 | loss 10.43 | mse 10.43 | mre  0.00 |
scGPT - INFO - | epoch  84 | 3500/3602 batches | lr 0.0000 | ms/batch 93.00 | loss 10.43 | mse 10.43 | mre  0.00 |
scGPT - INFO - | epoch  84 | 3600/3602 batches | lr 0.0000 | ms/batch 92.94 | loss 12.07 | mse 12.07 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  84 | time: 353.83s | valid loss/mse 12.5538 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.552931918187088, min_delta 0.0001, val_loss 12.553808830799383
Loss error: -0.0008769126122949444
scGPT - INFO - INFO: Early stopping counter 9 of 10
epoch:  84
[[6.6200733 3.4339871 3.5553482 ... 3.4011974 3.7841897 3.8501477]
 [3.6635616 7.916443  3.0445225 ... 3.713572  3.988984  3.5553482]
 [4.6634393 3.7376697 3.988984  ... 4.406719  3.8918204 3.6635616]
 ...
 [4.9767337 3.8066626 3.9702919 ... 3.9512436 3.713572  3.465736 ]
 [4.983607  3.8501477 3.8066626 ... 3.637586  3.2580965 3.3672957]
 [6.514713  3.3672957 3.1780539 ... 3.5263605 3.9318256 3.6109178]]
[[6.836 3.568 3.285 ... 3.451 3.648 3.879]
 [6.996 3.63  3.34  ... 3.459 3.688 3.912]
 [4.684 3.7   3.822 ... 4.094 3.713 3.55 ]
 ...
 [4.812 3.879 3.9   ... 4.066 3.79  3.578]
 [4.92  3.918 3.977 ... 4.12  3.865 3.615]
 [6.914 3.664 3.256 ... 3.469 3.611 3.889]]
(801, 11) (801, 11)
By feature:  MSE:  0.5707¬±0.0 MAE:  0.4925¬±0.0 R2:  0.2429¬±0.0 PCC:  0.8042¬±0.0 Cosine Similarity:  0.9899¬±0.0
By sample:  MSE:  0.5707¬±0.0 MAE:  0.4925¬±0.0 R2:  0.287¬±0.0 PCC:  0.5229¬±0.0 Cosine Similarity:  0.9855¬±0.0
scGPT - INFO - By feature: MSE: 0.5706999897956848¬±0.0 MAE: 0.4925000071525574¬±0.0 R2: 0.2429¬±0.0 PCC: 0.8042¬±0.0 Cosine Similarity: 0.9899¬±0.0
scGPT - INFO - By sample: MSE: 0.5706999897956848¬±0.0 MAE: 0.4925000071525574¬±0.0 R2: 0.287¬±0.0 PCC: 0.5229¬±0.0 Cosine Similarity: 0.9855¬±0.0
{'seed': 42, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/home/wsl20/Projects/RNA2prot/scPEFT_R2P/R2P_Efficient/scgpt_PEFT/scGPT_human', 'mask_ratio': 0.0, 'epochs': 100, 'n_bins': 51, 'n_hvg': False, 'n_hvp': 4000, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 5e-05, 'batch_size': 2, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'include_zero_gene': True, 'freeze': False, 'DSBN': False, 'data_path': '/home/wsl20/Projects/RNA2prot/scGPT/data/', 'use_prompt': True, 'prompt_type': 'Gene_encoder_prompt', 'num_tokens': 64, 'n_layers_conf': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'mlp_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'space_adapter_conf': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'use_mod': True}
Epoch:  85
Training model
scGPT - INFO - | epoch  85 | 100/3602 batches | lr 0.0000 | ms/batch 92.39 | loss 10.59 | mse 10.59 | mre  0.00 |
scGPT - INFO - | epoch  85 | 200/3602 batches | lr 0.0000 | ms/batch 91.21 | loss 12.83 | mse 12.83 | mre  0.00 |
scGPT - INFO - | epoch  85 | 300/3602 batches | lr 0.0000 | ms/batch 91.42 | loss  9.79 | mse  9.79 | mre  0.00 |
scGPT - INFO - | epoch  85 | 400/3602 batches | lr 0.0000 | ms/batch 93.60 | loss 10.67 | mse 10.67 | mre  0.00 |
scGPT - INFO - | epoch  85 | 500/3602 batches | lr 0.0000 | ms/batch 93.80 | loss  9.31 | mse  9.31 | mre  0.00 |
scGPT - INFO - | epoch  85 | 600/3602 batches | lr 0.0000 | ms/batch 92.14 | loss 10.68 | mse 10.68 | mre  0.00 |
scGPT - INFO - | epoch  85 | 700/3602 batches | lr 0.0000 | ms/batch 91.50 | loss 11.41 | mse 11.41 | mre  0.00 |
scGPT - INFO - | epoch  85 | 800/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 13.96 | mse 13.96 | mre  0.00 |
scGPT - INFO - | epoch  85 | 900/3602 batches | lr 0.0000 | ms/batch 90.91 | loss 11.03 | mse 11.03 | mre  0.00 |
scGPT - INFO - | epoch  85 | 1000/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 12.37 | mse 12.37 | mre  0.00 |
scGPT - INFO - | epoch  85 | 1100/3602 batches | lr 0.0000 | ms/batch 90.96 | loss 10.54 | mse 10.54 | mre  0.00 |
scGPT - INFO - | epoch  85 | 1200/3602 batches | lr 0.0000 | ms/batch 90.90 | loss 11.06 | mse 11.06 | mre  0.00 |
scGPT - INFO - | epoch  85 | 1300/3602 batches | lr 0.0000 | ms/batch 90.92 | loss 10.65 | mse 10.65 | mre  0.00 |
scGPT - INFO - | epoch  85 | 1400/3602 batches | lr 0.0000 | ms/batch 91.07 | loss 13.29 | mse 13.29 | mre  0.00 |
scGPT - INFO - | epoch  85 | 1500/3602 batches | lr 0.0000 | ms/batch 91.44 | loss 13.67 | mse 13.67 | mre  0.00 |
scGPT - INFO - | epoch  85 | 1600/3602 batches | lr 0.0000 | ms/batch 90.94 | loss  9.58 | mse  9.58 | mre  0.00 |
scGPT - INFO - | epoch  85 | 1700/3602 batches | lr 0.0000 | ms/batch 90.88 | loss 10.66 | mse 10.66 | mre  0.00 |
scGPT - INFO - | epoch  85 | 1800/3602 batches | lr 0.0000 | ms/batch 90.89 | loss 10.86 | mse 10.86 | mre  0.00 |
scGPT - INFO - | epoch  85 | 1900/3602 batches | lr 0.0000 | ms/batch 90.86 | loss 13.02 | mse 13.02 | mre  0.00 |
scGPT - INFO - | epoch  85 | 2000/3602 batches | lr 0.0000 | ms/batch 90.93 | loss 11.28 | mse 11.28 | mre  0.00 |
scGPT - INFO - | epoch  85 | 2100/3602 batches | lr 0.0000 | ms/batch 90.97 | loss 12.58 | mse 12.58 | mre  0.00 |
scGPT - INFO - | epoch  85 | 2200/3602 batches | lr 0.0000 | ms/batch 91.06 | loss 10.95 | mse 10.95 | mre  0.00 |
scGPT - INFO - | epoch  85 | 2300/3602 batches | lr 0.0000 | ms/batch 90.94 | loss 10.43 | mse 10.43 | mre  0.00 |
scGPT - INFO - | epoch  85 | 2400/3602 batches | lr 0.0000 | ms/batch 91.10 | loss 11.59 | mse 11.59 | mre  0.00 |
scGPT - INFO - | epoch  85 | 2500/3602 batches | lr 0.0000 | ms/batch 92.68 | loss 10.23 | mse 10.23 | mre  0.00 |
scGPT - INFO - | epoch  85 | 2600/3602 batches | lr 0.0000 | ms/batch 93.01 | loss 14.35 | mse 14.35 | mre  0.00 |
scGPT - INFO - | epoch  85 | 2700/3602 batches | lr 0.0000 | ms/batch 93.02 | loss 10.63 | mse 10.63 | mre  0.00 |
scGPT - INFO - | epoch  85 | 2800/3602 batches | lr 0.0000 | ms/batch 93.08 | loss 11.74 | mse 11.74 | mre  0.00 |
scGPT - INFO - | epoch  85 | 2900/3602 batches | lr 0.0000 | ms/batch 93.05 | loss 11.40 | mse 11.40 | mre  0.00 |
scGPT - INFO - | epoch  85 | 3000/3602 batches | lr 0.0000 | ms/batch 93.11 | loss 11.40 | mse 11.40 | mre  0.00 |
scGPT - INFO - | epoch  85 | 3100/3602 batches | lr 0.0000 | ms/batch 92.99 | loss 10.95 | mse 10.95 | mre  0.00 |
scGPT - INFO - | epoch  85 | 3200/3602 batches | lr 0.0000 | ms/batch 93.01 | loss 11.29 | mse 11.29 | mre  0.00 |
scGPT - INFO - | epoch  85 | 3300/3602 batches | lr 0.0000 | ms/batch 93.18 | loss  9.41 | mse  9.41 | mre  0.00 |
scGPT - INFO - | epoch  85 | 3400/3602 batches | lr 0.0000 | ms/batch 93.12 | loss 10.41 | mse 10.41 | mre  0.00 |
scGPT - INFO - | epoch  85 | 3500/3602 batches | lr 0.0000 | ms/batch 93.59 | loss 10.30 | mse 10.30 | mre  0.00 |
scGPT - INFO - | epoch  85 | 3600/3602 batches | lr 0.0000 | ms/batch 93.06 | loss 11.72 | mse 11.72 | mre  0.00 |
Evaluating model
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  85 | time: 344.04s | valid loss/mse 12.5531 |
scGPT - INFO - -----------------------------------------------------------------------------------------
best_loss: 12.552931918187088, min_delta 0.0001, val_loss 12.553149996476524
Loss error: -0.00021807828943565255
scGPT - INFO - INFO: Early stopping counter 10 of 10
scGPT - INFO - INFO: Early stopping
scGPT - INFO - Best model saved successfully!
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | End test time: 344.16s | 
scGPT - INFO - -----------------------------------------------------------------------------------------
wandb: - 0.003 MB of 0.003 MB uploadedwandb: \ 0.003 MB of 0.003 MB uploadedwandb: | 0.003 MB of 0.003 MB uploadedwandb: / 0.580 MB of 0.663 MB uploadedwandb: - 0.660 MB of 0.663 MB uploadedwandb: \ 0.663 MB of 0.663 MB uploadedwandb: 
wandb: Run history:
wandb:                        epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb: info/post_freeze_param_count ‚ñÅ
wandb:  info/pre_freeze_param_count ‚ñÅ
wandb:                    train/mse ‚ñÇ‚ñÉ‚ñÉ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÑ‚ñÇ‚ñÅ‚ñÉ‚ñÅ
wandb:                    valid/dab ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                    valid/err ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                    valid/mse ‚ñà‚ñà‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:            valid/sum_mse_dab ‚ñà‚ñà‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                        epoch 85
wandb: info/post_freeze_param_count 2639372
wandb:  info/pre_freeze_param_count 29650444
wandb:                    train/mse 7.37686
wandb:                    valid/err 0.0
wandb: 
wandb: üöÄ View run zesty-tree-351 at: https://wandb.ai/wang_wandb/scGPT/runs/e6cgo4q7
wandb: Ô∏è‚ö° View job at https://wandb.ai/wang_wandb/scGPT/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE1NTQ0MTQ2Nw==/version_details/v33
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240423_213439-e6cgo4q7/logs
